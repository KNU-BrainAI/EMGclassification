{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":2051.05586,"end_time":"2023-05-21T13:12:12.053262","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-05-21T12:38:00.997402","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.038029,"end_time":"2023-05-21T12:38:11.801112","exception":false,"start_time":"2023-05-21T12:38:11.763083","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T14:27:38.686884Z","iopub.execute_input":"2023-07-19T14:27:38.687318Z","iopub.status.idle":"2023-07-19T14:27:38.707285Z","shell.execute_reply.started":"2023-07-19T14:27:38.687282Z","shell.execute_reply":"2023-07-19T14:27:38.706235Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"/kaggle/input/datasetfornina/ninaprodb1train.pkl\n/kaggle/input/datasetfornina/ninaprodb1test.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\n#from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport math","metadata":{"papermill":{"duration":6.758124,"end_time":"2023-05-21T12:38:18.563919","exception":false,"start_time":"2023-05-21T12:38:11.805795","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T14:27:38.709137Z","iopub.execute_input":"2023-07-19T14:27:38.710090Z","iopub.status.idle":"2023-07-19T14:27:38.716442Z","shell.execute_reply.started":"2023-07-19T14:27:38.710051Z","shell.execute_reply":"2023-07-19T14:27:38.714977Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import pickle\n# Load the dataset from pickle file\nwith open('/kaggle/input/datasetfornina/ninaprodb1test.pkl', 'rb') as f:\n    test_data = pickle.load(f)\nwith open('/kaggle/input/datasetfornina/ninaprodb1train.pkl', 'rb') as f:\n    train_data = pickle.load(f)","metadata":{"papermill":{"duration":4.616483,"end_time":"2023-05-21T12:38:23.184515","exception":false,"start_time":"2023-05-21T12:38:18.568032","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T14:27:38.717981Z","iopub.execute_input":"2023-07-19T14:27:38.718428Z","iopub.status.idle":"2023-07-19T14:27:39.239071Z","shell.execute_reply.started":"2023-07-19T14:27:38.718391Z","shell.execute_reply":"2023-07-19T14:27:39.237982Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"papermill":{"duration":0.649738,"end_time":"2023-05-21T12:38:23.838778","exception":false,"start_time":"2023-05-21T12:38:23.189040","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T14:27:39.241563Z","iopub.execute_input":"2023-07-19T14:27:39.241899Z","iopub.status.idle":"2023-07-19T14:27:39.248234Z","shell.execute_reply.started":"2023-07-19T14:27:39.241870Z","shell.execute_reply":"2023-07-19T14:27:39.246764Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers, Sequential, optimizers, Input, Model","metadata":{"papermill":{"duration":0.0117,"end_time":"2023-05-21T12:38:23.854540","exception":false,"start_time":"2023-05-21T12:38:23.842840","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T14:27:39.249874Z","iopub.execute_input":"2023-07-19T14:27:39.250815Z","iopub.status.idle":"2023-07-19T14:27:39.259693Z","shell.execute_reply.started":"2023-07-19T14:27:39.250777Z","shell.execute_reply":"2023-07-19T14:27:39.258564Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"**WORKED CODE HERE**","metadata":{"papermill":{"duration":0.003685,"end_time":"2023-05-21T12:38:23.862030","exception":false,"start_time":"2023-05-21T12:38:23.858345","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\nimport gc\nimport random\nimport os\n\nimport matplotlib.pyplot as plt\nimport json","metadata":{"papermill":{"duration":0.013837,"end_time":"2023-05-21T12:38:23.879571","exception":false,"start_time":"2023-05-21T12:38:23.865734","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T14:27:39.261392Z","iopub.execute_input":"2023-07-19T14:27:39.261833Z","iopub.status.idle":"2023-07-19T14:27:39.275078Z","shell.execute_reply.started":"2023-07-19T14:27:39.261797Z","shell.execute_reply":"2023-07-19T14:27:39.273914Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"epochs = 64\nlearning_rate = 1e-3\nbatch_size = 16\nmethod = \"default\"\ndataset_type = 1","metadata":{"papermill":{"duration":0.011418,"end_time":"2023-05-21T12:38:23.894788","exception":false,"start_time":"2023-05-21T12:38:23.883370","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T14:27:39.276595Z","iopub.execute_input":"2023-07-19T14:27:39.277046Z","iopub.status.idle":"2023-07-19T14:27:39.287707Z","shell.execute_reply.started":"2023-07-19T14:27:39.277012Z","shell.execute_reply":"2023-07-19T14:27:39.286331Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"### import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.utils import to_categorical\n\n\nclass Nina1Dataset(tf.keras.utils.Sequence):\n    def __init__(self, dataframe, batch_size):\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.scaler = StandardScaler()\n        self.scaler.fit(np.concatenate(self.dataframe['emg'].tolist()))\n        \n    def __len__(self):\n        return int(np.ceil(len(self.dataframe) / self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_data = self.dataframe[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        batch_input_data = []\n        batch_labels = []\n\n        for i, target_row in batch_data.iterrows():\n            data = target_row['emg'][:500]\n\n            # Zero-Padding\n            if len(data) < 500:\n                data = np.concatenate((data, np.zeros((500 - len(data), 10))), axis=0)\n            data = self.scaler.transform(data)\n\n            # Division data by time-segment\n            #input_data = np.transpose(data.reshape((25, 20, 10)), (0, 2, 1))\n            input_data = data.reshape((25, 20, 10))\n            label = target_row['stimulus']\n            #label = to_categorical(label)\n            batch_input_data.append(input_data)\n            batch_labels.append(label)\n\n        # Check if the batch size is smaller than the desired batch_size\n        if len(batch_data) < self.batch_size:\n            # Create a dummy batch with all elements set to zero\n            dummy_input_data = np.zeros((self.batch_size,) + input_data.shape, dtype=np.float32)\n            dummy_labels = np.zeros((self.batch_size,), dtype=np.int32)\n            dummy_input_data[:len(batch_input_data)] = np.array(batch_input_data)\n            dummy_labels[:len(batch_labels)] = np.array(batch_labels)\n            dummy_labels = to_categorical(dummy_labels,num_classes=52)  # Convert labels to one-hot encoding\n            \n            return dummy_input_data, dummy_labels\n        \n        batch_labels = to_categorical(batch_labels,num_classes=52)  # Convert labels to one-hot encoding\n        \n        return np.array(batch_input_data), np.array(batch_labels)\n\n\n\n\n\n# Parameters\nbatch_size = 16\ntrain_dir = '/kaggle/input/datasetfornina/ninaprodb1train.pkl'\ntest_dir = '/kaggle/input/datasetfornina/ninaprodb1test.pkl'\n\n# Set up dataset\n\ntrain = pd.read_pickle(train_dir)\neval_data = pd.read_pickle(test_dir)\n\n# Load train data\ntrain_data = pd.read_pickle(train_dir)\n\n# Split data into train and validation sets\ntrain_data, val_data = train_test_split(train_data, test_size=0.3, random_state=21)\n\ntrain_data = train_data.sample(frac=1).reset_index(drop=True)\nval_data = val_data.sample(frac=1).reset_index(drop=True)\neval_data = eval_data.sample(frac=1).reset_index(drop=True)\n\n# Create train, test and validation datasets\ntrain_dataset = Nina1Dataset(train_data, batch_size=batch_size)\nval_dataset= Nina1Dataset(val_data, batch_size=batch_size)\ntest_dataset= Nina1Dataset(eval_data, batch_size=batch_size)\n\n\nprint(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T14:27:39.289483Z","iopub.execute_input":"2023-07-19T14:27:39.289881Z","iopub.status.idle":"2023-07-19T14:27:41.784243Z","shell.execute_reply.started":"2023-07-19T14:27:39.289849Z","shell.execute_reply":"2023-07-19T14:27:41.782335Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"<__main__.Nina1Dataset object at 0x7f10be4f7010>\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\n# Define the CNN-BiLSTM model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.layers import TimeDistributed\n\ndef cnn(x):\n    #print(x.shape)\n    #x = Reshape((25, 20,10))(x)  \n    print(x.shape)\n    x = TimeDistributed((Conv1D(64, kernel_size=9, strides=2, padding='same', activation=tanh)))(x)\n    print(x.shape)\n    #(Batch, 10, 64)\n\n    x = TimeDistributed(MaxPooling1D(pool_size=8, strides=2))(x)\n    print(x.shape)\n    #(Batch, 2, 64)\n    \n    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n    #(Batch, 1, 64)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    \n    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n    print(x.shape)\n    #(Batch, 1, 64)\n    #x = TimeDistributed(Dropout(0.2093))(x)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    \n    #x = TimeDistributed(ReLU())(Conv1D(64, kernel_size=3, strides=2, padding='same')(x))\n    x = TimeDistributed((Conv1D(64, kernel_size=3, strides=2, padding='same', activation=tanh)))(x)\n    #print(x.shape)\n    #(Batch, 1, 64)\n    #x = TimeDistributed(Dropout(0.2093))(x)\n    #x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(Flatten())(x)\n    print(x.shape)\n    # (Batch, 64)\n\n    return x\n\ndef Bi_LSTMModel(input_shape,x):\n    #model = Sequential()\n    # Hidden dimensions\n    hidden_dim = 200\n    print(x.shape)\n    print(11)\n    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.2093), input_shape=input_shape)(x)\n    #x = Dropout(0.2093)(x)\n    print(x.shape)\n    print(11)\n    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.2093))(x)\n\n    print(x.shape)\n    print(11)\n    #x = Dropout(0.2093)(x)\n    x = Flatten()(x)\n    # ( ,10000)\n\n    return x\ndef EMGHandNet(input_shape, num_classes):\n    # Define the input layer\n    x = Input(shape=input_shape)\n    inputs = x\n    #print(x.shape)\n    #(batch, 25, 20, 10)\n    #temp = [cnn(x[:, t, :, :]) for t in range(x.shape[1])]\n    #x = tf.stack(temp, axis=1)\n    #print(x.shape)\n    x = cnn(x)\n   \n    #print(x.shape)\n    x = Bi_LSTMModel(x.shape[1:],x)\n    #print(x.shape)\n    #x = Dropout(0.2093)(x)\n    \n    x = Dense(512, activation='tanh')(x)\n    #print(x.shape)\n    x = Dropout(0.2093)(x)\n\n    # Add the output layer\n    output_layer = Dense(52, activation='softmax')(x)\n    # Create the model\n    model = Model(inputs=inputs, outputs=output_layer)\n\n    return model\n\n\nnum_classes = 52  # Adjust based on the number of hand activity classes\n\n\n# Create an instance of the EMGHandNet model\nmodel = EMGHandNet((25,20,10), 52)\n\n\n# Compile the model\n\n#dtype = tf.float32\n\ninitial_learning_rate = 0.001\ndecay_steps = 1000\ndecay_rate = 0.9\nbatch_size = 16\n# Define your model and its optimizer\n# Define learning rate schedule\nlr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n\n    # Compile the model with learning rate schedule\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999),\n                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n                metrics=['accuracy'])\nhistory = model.fit(train_dataset,\n                    epochs=200,\n                    validation_data=val_dataset,\n                    batch_size=16\n                    )","metadata":{"papermill":{"duration":1994.814887,"end_time":"2023-05-21T13:11:39.782204","exception":false,"start_time":"2023-05-21T12:38:24.967317","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T14:27:41.786003Z","iopub.execute_input":"2023-07-19T14:27:41.786820Z","iopub.status.idle":"2023-07-19T15:13:00.034816Z","shell.execute_reply.started":"2023-07-19T14:27:41.786781Z","shell.execute_reply":"2023-07-19T15:13:00.033632Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"(None, 25, 20, 10)\n(None, 25, 10, 64)\n(None, 25, 2, 64)\n(None, 25, 1, 64)\n(None, 25, 64)\n(None, 25, 64)\n11\n(None, 25, 400)\n11\n(None, 25, 400)\n11\nEpoch 1/200\n430/430 [==============================] - 25s 34ms/step - loss: 3.1164 - accuracy: 0.2096 - val_loss: 2.4540 - val_accuracy: 0.3135\nEpoch 2/200\n430/430 [==============================] - 12s 29ms/step - loss: 2.2118 - accuracy: 0.3842 - val_loss: 1.9764 - val_accuracy: 0.4311\nEpoch 3/200\n430/430 [==============================] - 13s 31ms/step - loss: 1.7517 - accuracy: 0.5004 - val_loss: 1.7334 - val_accuracy: 0.4885\nEpoch 4/200\n430/430 [==============================] - 12s 29ms/step - loss: 1.3140 - accuracy: 0.6096 - val_loss: 1.4746 - val_accuracy: 0.5699\nEpoch 5/200\n430/430 [==============================] - 13s 30ms/step - loss: 0.9110 - accuracy: 0.7183 - val_loss: 1.2516 - val_accuracy: 0.6291\nEpoch 6/200\n430/430 [==============================] - 13s 30ms/step - loss: 0.6276 - accuracy: 0.8000 - val_loss: 1.1325 - val_accuracy: 0.6649\nEpoch 7/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.3820 - accuracy: 0.8808 - val_loss: 1.0506 - val_accuracy: 0.6909\nEpoch 8/200\n430/430 [==============================] - 14s 32ms/step - loss: 0.2268 - accuracy: 0.9344 - val_loss: 1.0065 - val_accuracy: 0.7122\nEpoch 9/200\n430/430 [==============================] - 13s 31ms/step - loss: 0.1513 - accuracy: 0.9590 - val_loss: 0.9244 - val_accuracy: 0.7368\nEpoch 10/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.1249 - accuracy: 0.9660 - val_loss: 0.9587 - val_accuracy: 0.7429\nEpoch 11/200\n430/430 [==============================] - 13s 29ms/step - loss: 0.0828 - accuracy: 0.9807 - val_loss: 0.9289 - val_accuracy: 0.7473\nEpoch 12/200\n430/430 [==============================] - 12s 29ms/step - loss: 0.0991 - accuracy: 0.9733 - val_loss: 0.9442 - val_accuracy: 0.7507\nEpoch 13/200\n430/430 [==============================] - 13s 29ms/step - loss: 0.0653 - accuracy: 0.9842 - val_loss: 0.9622 - val_accuracy: 0.7524\nEpoch 14/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0394 - accuracy: 0.9923 - val_loss: 0.9296 - val_accuracy: 0.7611\nEpoch 15/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0535 - accuracy: 0.9859 - val_loss: 1.0499 - val_accuracy: 0.7267\nEpoch 16/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0460 - accuracy: 0.9882 - val_loss: 1.0636 - val_accuracy: 0.7274\nEpoch 17/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.1085 - accuracy: 0.9686 - val_loss: 1.0081 - val_accuracy: 0.7446\nEpoch 18/200\n430/430 [==============================] - 12s 29ms/step - loss: 0.0530 - accuracy: 0.9853 - val_loss: 0.9786 - val_accuracy: 0.7534\nEpoch 19/200\n430/430 [==============================] - 13s 29ms/step - loss: 0.0201 - accuracy: 0.9961 - val_loss: 0.8959 - val_accuracy: 0.7811\nEpoch 20/200\n430/430 [==============================] - 12s 27ms/step - loss: 0.0082 - accuracy: 0.9991 - val_loss: 0.8657 - val_accuracy: 0.7868\nEpoch 21/200\n430/430 [==============================] - 12s 29ms/step - loss: 0.0048 - accuracy: 0.9997 - val_loss: 0.8390 - val_accuracy: 0.7980\nEpoch 22/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.8302 - val_accuracy: 0.8027\nEpoch 23/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 1.0003 - val_accuracy: 0.7709\nEpoch 24/200\n430/430 [==============================] - 12s 27ms/step - loss: 0.0639 - accuracy: 0.9807 - val_loss: 1.0555 - val_accuracy: 0.7486\nEpoch 25/200\n430/430 [==============================] - 12s 29ms/step - loss: 0.0281 - accuracy: 0.9930 - val_loss: 0.9506 - val_accuracy: 0.7767\nEpoch 26/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0218 - accuracy: 0.9948 - val_loss: 0.9656 - val_accuracy: 0.7726\nEpoch 27/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0078 - accuracy: 0.9987 - val_loss: 0.9052 - val_accuracy: 0.7902\nEpoch 28/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.8822 - val_accuracy: 0.7949\nEpoch 29/200\n430/430 [==============================] - 11s 27ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.8601 - val_accuracy: 0.8010\nEpoch 30/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8586 - val_accuracy: 0.8041\nEpoch 31/200\n430/430 [==============================] - 12s 28ms/step - loss: 8.7970e-04 - accuracy: 1.0000 - val_loss: 0.8598 - val_accuracy: 0.8007\nEpoch 32/200\n430/430 [==============================] - 12s 27ms/step - loss: 0.0277 - accuracy: 0.9919 - val_loss: 1.0018 - val_accuracy: 0.7709\nEpoch 33/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0088 - accuracy: 0.9980 - val_loss: 0.9135 - val_accuracy: 0.7899\nEpoch 34/200\n430/430 [==============================] - 12s 29ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.8941 - val_accuracy: 0.7949\nEpoch 35/200\n430/430 [==============================] - 12s 27ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.9111 - val_accuracy: 0.7909\nEpoch 36/200\n430/430 [==============================] - 13s 30ms/step - loss: 0.0013 - accuracy: 0.9999 - val_loss: 0.8873 - val_accuracy: 0.8051\nEpoch 37/200\n430/430 [==============================] - 13s 30ms/step - loss: 7.2548e-04 - accuracy: 1.0000 - val_loss: 0.8773 - val_accuracy: 0.8081\nEpoch 38/200\n430/430 [==============================] - 13s 30ms/step - loss: 5.5961e-04 - accuracy: 1.0000 - val_loss: 0.8769 - val_accuracy: 0.8111\nEpoch 39/200\n430/430 [==============================] - 12s 29ms/step - loss: 5.2113e-04 - accuracy: 1.0000 - val_loss: 0.8829 - val_accuracy: 0.8054\nEpoch 40/200\n430/430 [==============================] - 12s 29ms/step - loss: 4.4581e-04 - accuracy: 1.0000 - val_loss: 0.8718 - val_accuracy: 0.8115\nEpoch 41/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.9379e-04 - accuracy: 1.0000 - val_loss: 0.8826 - val_accuracy: 0.8115\nEpoch 42/200\n430/430 [==============================] - 12s 28ms/step - loss: 3.5867e-04 - accuracy: 1.0000 - val_loss: 0.8668 - val_accuracy: 0.8176\nEpoch 43/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.1784e-04 - accuracy: 1.0000 - val_loss: 0.8770 - val_accuracy: 0.8118\nEpoch 44/200\n430/430 [==============================] - 12s 29ms/step - loss: 2.8529e-04 - accuracy: 1.0000 - val_loss: 0.8914 - val_accuracy: 0.8139\nEpoch 45/200\n430/430 [==============================] - 12s 27ms/step - loss: 2.5521e-04 - accuracy: 1.0000 - val_loss: 0.8914 - val_accuracy: 0.8132\nEpoch 46/200\n430/430 [==============================] - 12s 29ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 1.0346 - val_accuracy: 0.7905\nEpoch 47/200\n430/430 [==============================] - 12s 27ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 0.9642 - val_accuracy: 0.7956\nEpoch 48/200\n430/430 [==============================] - 12s 28ms/step - loss: 7.8399e-04 - accuracy: 1.0000 - val_loss: 0.9224 - val_accuracy: 0.8041\nEpoch 49/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.3081e-04 - accuracy: 1.0000 - val_loss: 0.9114 - val_accuracy: 0.8088\nEpoch 50/200\n430/430 [==============================] - 12s 28ms/step - loss: 3.5593e-04 - accuracy: 1.0000 - val_loss: 0.9061 - val_accuracy: 0.8125\nEpoch 51/200\n430/430 [==============================] - 14s 32ms/step - loss: 6.6534e-04 - accuracy: 0.9999 - val_loss: 0.9295 - val_accuracy: 0.8068\nEpoch 52/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.7202e-04 - accuracy: 1.0000 - val_loss: 0.9103 - val_accuracy: 0.8125\nEpoch 53/200\n430/430 [==============================] - 16s 37ms/step - loss: 2.7242e-04 - accuracy: 1.0000 - val_loss: 0.9036 - val_accuracy: 0.8155\nEpoch 54/200\n430/430 [==============================] - 12s 29ms/step - loss: 2.4501e-04 - accuracy: 1.0000 - val_loss: 0.9031 - val_accuracy: 0.8118\nEpoch 55/200\n430/430 [==============================] - 13s 30ms/step - loss: 1.8209e-04 - accuracy: 1.0000 - val_loss: 0.9045 - val_accuracy: 0.8145\nEpoch 56/200\n430/430 [==============================] - 13s 31ms/step - loss: 1.5148e-04 - accuracy: 1.0000 - val_loss: 0.9029 - val_accuracy: 0.8149\nEpoch 57/200\n430/430 [==============================] - 12s 29ms/step - loss: 1.3020e-04 - accuracy: 1.0000 - val_loss: 0.9065 - val_accuracy: 0.8149\nEpoch 58/200\n430/430 [==============================] - 13s 30ms/step - loss: 1.5059e-04 - accuracy: 1.0000 - val_loss: 0.9044 - val_accuracy: 0.8169\nEpoch 59/200\n430/430 [==============================] - 12s 29ms/step - loss: 1.1278e-04 - accuracy: 1.0000 - val_loss: 0.9072 - val_accuracy: 0.8159\nEpoch 60/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.0421e-04 - accuracy: 1.0000 - val_loss: 0.9034 - val_accuracy: 0.8145\nEpoch 61/200\n430/430 [==============================] - 13s 29ms/step - loss: 1.1048e-04 - accuracy: 1.0000 - val_loss: 0.9127 - val_accuracy: 0.8152\nEpoch 62/200\n430/430 [==============================] - 12s 29ms/step - loss: 9.9619e-05 - accuracy: 1.0000 - val_loss: 0.9060 - val_accuracy: 0.8193\nEpoch 63/200\n430/430 [==============================] - 13s 31ms/step - loss: 7.8699e-05 - accuracy: 1.0000 - val_loss: 0.9154 - val_accuracy: 0.8145\nEpoch 64/200\n430/430 [==============================] - 12s 28ms/step - loss: 7.3195e-05 - accuracy: 1.0000 - val_loss: 0.9043 - val_accuracy: 0.8220\nEpoch 65/200\n430/430 [==============================] - 13s 31ms/step - loss: 7.0336e-05 - accuracy: 1.0000 - val_loss: 0.9058 - val_accuracy: 0.8196\nEpoch 66/200\n430/430 [==============================] - 13s 30ms/step - loss: 6.6542e-05 - accuracy: 1.0000 - val_loss: 0.9167 - val_accuracy: 0.8149\nEpoch 67/200\n430/430 [==============================] - 13s 31ms/step - loss: 5.5378e-05 - accuracy: 1.0000 - val_loss: 0.9182 - val_accuracy: 0.8199\nEpoch 68/200\n430/430 [==============================] - 14s 32ms/step - loss: 4.7742e-05 - accuracy: 1.0000 - val_loss: 0.9143 - val_accuracy: 0.8176\nEpoch 69/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.9279e-05 - accuracy: 1.0000 - val_loss: 0.9109 - val_accuracy: 0.8220\nEpoch 70/200\n430/430 [==============================] - 14s 33ms/step - loss: 4.2948e-05 - accuracy: 1.0000 - val_loss: 0.9117 - val_accuracy: 0.8209\nEpoch 71/200\n430/430 [==============================] - 14s 32ms/step - loss: 3.7109e-05 - accuracy: 1.0000 - val_loss: 0.9218 - val_accuracy: 0.8220\nEpoch 72/200\n430/430 [==============================] - 15s 35ms/step - loss: 3.5436e-05 - accuracy: 1.0000 - val_loss: 0.9267 - val_accuracy: 0.8199\nEpoch 73/200\n430/430 [==============================] - 12s 28ms/step - loss: 3.2403e-05 - accuracy: 1.0000 - val_loss: 0.9337 - val_accuracy: 0.8149\nEpoch 74/200\n430/430 [==============================] - 12s 29ms/step - loss: 3.2542e-05 - accuracy: 1.0000 - val_loss: 0.9253 - val_accuracy: 0.8206\nEpoch 75/200\n430/430 [==============================] - 13s 29ms/step - loss: 2.3714e-05 - accuracy: 1.0000 - val_loss: 0.9279 - val_accuracy: 0.8189\nEpoch 76/200\n430/430 [==============================] - 12s 28ms/step - loss: 2.4805e-05 - accuracy: 1.0000 - val_loss: 0.9281 - val_accuracy: 0.8223\nEpoch 77/200\n430/430 [==============================] - 15s 35ms/step - loss: 2.3898e-05 - accuracy: 1.0000 - val_loss: 0.9210 - val_accuracy: 0.8220\nEpoch 78/200\n430/430 [==============================] - 12s 28ms/step - loss: 2.0091e-05 - accuracy: 1.0000 - val_loss: 0.9313 - val_accuracy: 0.8260\nEpoch 79/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.8323e-05 - accuracy: 1.0000 - val_loss: 0.9295 - val_accuracy: 0.8209\nEpoch 80/200\n430/430 [==============================] - 13s 29ms/step - loss: 2.0382e-05 - accuracy: 1.0000 - val_loss: 0.9375 - val_accuracy: 0.8220\nEpoch 81/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.9543e-05 - accuracy: 1.0000 - val_loss: 0.9415 - val_accuracy: 0.8226\nEpoch 82/200\n430/430 [==============================] - 15s 35ms/step - loss: 1.6093e-05 - accuracy: 1.0000 - val_loss: 0.9366 - val_accuracy: 0.8216\nEpoch 83/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.6444e-05 - accuracy: 1.0000 - val_loss: 0.9411 - val_accuracy: 0.8257\nEpoch 84/200\n430/430 [==============================] - 13s 29ms/step - loss: 1.4862e-05 - accuracy: 1.0000 - val_loss: 0.9364 - val_accuracy: 0.8270\nEpoch 85/200\n430/430 [==============================] - 13s 31ms/step - loss: 1.3337e-05 - accuracy: 1.0000 - val_loss: 0.9420 - val_accuracy: 0.8253\nEpoch 86/200\n430/430 [==============================] - 13s 30ms/step - loss: 1.3498e-05 - accuracy: 1.0000 - val_loss: 0.9456 - val_accuracy: 0.8247\nEpoch 87/200\n430/430 [==============================] - 14s 32ms/step - loss: 1.1264e-05 - accuracy: 1.0000 - val_loss: 0.9436 - val_accuracy: 0.8223\nEpoch 88/200\n430/430 [==============================] - 13s 31ms/step - loss: 1.1419e-05 - accuracy: 1.0000 - val_loss: 0.9445 - val_accuracy: 0.8233\nEpoch 89/200\n430/430 [==============================] - 14s 32ms/step - loss: 1.1922e-05 - accuracy: 1.0000 - val_loss: 0.9466 - val_accuracy: 0.8209\nEpoch 90/200\n430/430 [==============================] - 14s 32ms/step - loss: 1.1885e-05 - accuracy: 1.0000 - val_loss: 0.9432 - val_accuracy: 0.8250\nEpoch 91/200\n430/430 [==============================] - 15s 34ms/step - loss: 1.1910e-05 - accuracy: 1.0000 - val_loss: 0.9461 - val_accuracy: 0.8264\nEpoch 92/200\n430/430 [==============================] - 13s 31ms/step - loss: 1.1145e-05 - accuracy: 1.0000 - val_loss: 0.9455 - val_accuracy: 0.8291\nEpoch 93/200\n430/430 [==============================] - 13s 31ms/step - loss: 8.7891e-06 - accuracy: 1.0000 - val_loss: 0.9469 - val_accuracy: 0.8291\nEpoch 94/200\n430/430 [==============================] - 13s 31ms/step - loss: 9.0112e-06 - accuracy: 1.0000 - val_loss: 0.9476 - val_accuracy: 0.8284\nEpoch 95/200\n430/430 [==============================] - 13s 30ms/step - loss: 8.6518e-06 - accuracy: 1.0000 - val_loss: 0.9545 - val_accuracy: 0.8240\nEpoch 96/200\n430/430 [==============================] - 13s 30ms/step - loss: 8.8328e-06 - accuracy: 1.0000 - val_loss: 0.9576 - val_accuracy: 0.8280\nEpoch 97/200\n430/430 [==============================] - 13s 29ms/step - loss: 7.6633e-06 - accuracy: 1.0000 - val_loss: 0.9511 - val_accuracy: 0.8270\nEpoch 98/200\n430/430 [==============================] - 14s 32ms/step - loss: 7.7173e-06 - accuracy: 1.0000 - val_loss: 0.9492 - val_accuracy: 0.8284\nEpoch 99/200\n430/430 [==============================] - 13s 31ms/step - loss: 7.8234e-06 - accuracy: 1.0000 - val_loss: 0.9498 - val_accuracy: 0.8304\nEpoch 100/200\n430/430 [==============================] - 13s 31ms/step - loss: 7.2634e-06 - accuracy: 1.0000 - val_loss: 0.9469 - val_accuracy: 0.8274\nEpoch 101/200\n430/430 [==============================] - 13s 30ms/step - loss: 6.4530e-06 - accuracy: 1.0000 - val_loss: 0.9588 - val_accuracy: 0.8270\nEpoch 102/200\n430/430 [==============================] - 13s 30ms/step - loss: 6.7663e-06 - accuracy: 1.0000 - val_loss: 0.9622 - val_accuracy: 0.8247\nEpoch 103/200\n430/430 [==============================] - 14s 33ms/step - loss: 6.6844e-06 - accuracy: 1.0000 - val_loss: 0.9592 - val_accuracy: 0.8243\nEpoch 104/200\n430/430 [==============================] - 13s 31ms/step - loss: 6.9608e-06 - accuracy: 1.0000 - val_loss: 0.9523 - val_accuracy: 0.8280\nEpoch 105/200\n430/430 [==============================] - 14s 33ms/step - loss: 5.7888e-06 - accuracy: 1.0000 - val_loss: 0.9590 - val_accuracy: 0.8274\nEpoch 106/200\n430/430 [==============================] - 13s 30ms/step - loss: 6.4715e-06 - accuracy: 1.0000 - val_loss: 0.9557 - val_accuracy: 0.8280\nEpoch 107/200\n430/430 [==============================] - 14s 33ms/step - loss: 7.0472e-06 - accuracy: 1.0000 - val_loss: 0.9573 - val_accuracy: 0.8274\nEpoch 108/200\n430/430 [==============================] - 13s 30ms/step - loss: 6.1926e-06 - accuracy: 1.0000 - val_loss: 0.9477 - val_accuracy: 0.8287\nEpoch 109/200\n430/430 [==============================] - 16s 37ms/step - loss: 5.5730e-06 - accuracy: 1.0000 - val_loss: 0.9571 - val_accuracy: 0.8314\nEpoch 110/200\n430/430 [==============================] - 13s 29ms/step - loss: 5.6847e-06 - accuracy: 1.0000 - val_loss: 0.9544 - val_accuracy: 0.8287\nEpoch 111/200\n430/430 [==============================] - 13s 30ms/step - loss: 5.7324e-06 - accuracy: 1.0000 - val_loss: 0.9577 - val_accuracy: 0.8284\nEpoch 112/200\n430/430 [==============================] - 13s 31ms/step - loss: 6.0971e-06 - accuracy: 1.0000 - val_loss: 0.9665 - val_accuracy: 0.8264\nEpoch 113/200\n430/430 [==============================] - 13s 31ms/step - loss: 5.1247e-06 - accuracy: 1.0000 - val_loss: 0.9506 - val_accuracy: 0.8280\nEpoch 114/200\n430/430 [==============================] - 14s 32ms/step - loss: 5.4336e-06 - accuracy: 1.0000 - val_loss: 0.9616 - val_accuracy: 0.8297\nEpoch 115/200\n430/430 [==============================] - 13s 31ms/step - loss: 5.3614e-06 - accuracy: 1.0000 - val_loss: 0.9579 - val_accuracy: 0.8287\nEpoch 116/200\n430/430 [==============================] - 16s 38ms/step - loss: 4.9445e-06 - accuracy: 1.0000 - val_loss: 0.9558 - val_accuracy: 0.8270\nEpoch 117/200\n430/430 [==============================] - 12s 29ms/step - loss: 5.7061e-06 - accuracy: 1.0000 - val_loss: 0.9615 - val_accuracy: 0.8304\nEpoch 118/200\n430/430 [==============================] - 13s 30ms/step - loss: 5.0852e-06 - accuracy: 1.0000 - val_loss: 0.9598 - val_accuracy: 0.8294\nEpoch 119/200\n430/430 [==============================] - 14s 32ms/step - loss: 5.2517e-06 - accuracy: 1.0000 - val_loss: 0.9606 - val_accuracy: 0.8260\nEpoch 120/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.9762e-06 - accuracy: 1.0000 - val_loss: 0.9626 - val_accuracy: 0.8291\nEpoch 121/200\n430/430 [==============================] - 13s 31ms/step - loss: 4.9762e-06 - accuracy: 1.0000 - val_loss: 0.9632 - val_accuracy: 0.8284\nEpoch 122/200\n430/430 [==============================] - 13s 30ms/step - loss: 5.2820e-06 - accuracy: 1.0000 - val_loss: 0.9624 - val_accuracy: 0.8284\nEpoch 123/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.6906e-06 - accuracy: 1.0000 - val_loss: 0.9603 - val_accuracy: 0.8287\nEpoch 124/200\n430/430 [==============================] - 14s 32ms/step - loss: 4.5891e-06 - accuracy: 1.0000 - val_loss: 0.9645 - val_accuracy: 0.8267\nEpoch 125/200\n430/430 [==============================] - 13s 31ms/step - loss: 4.6975e-06 - accuracy: 1.0000 - val_loss: 0.9667 - val_accuracy: 0.8270\nEpoch 126/200\n430/430 [==============================] - 13s 30ms/step - loss: 5.0304e-06 - accuracy: 1.0000 - val_loss: 0.9572 - val_accuracy: 0.8294\nEpoch 127/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.5604e-06 - accuracy: 1.0000 - val_loss: 0.9612 - val_accuracy: 0.8284\nEpoch 128/200\n430/430 [==============================] - 14s 32ms/step - loss: 4.6474e-06 - accuracy: 1.0000 - val_loss: 0.9592 - val_accuracy: 0.8267\nEpoch 129/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.1283e-06 - accuracy: 1.0000 - val_loss: 0.9626 - val_accuracy: 0.8294\nEpoch 130/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.5484e-06 - accuracy: 1.0000 - val_loss: 0.9666 - val_accuracy: 0.8284\nEpoch 131/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.2506e-06 - accuracy: 1.0000 - val_loss: 0.9676 - val_accuracy: 0.8294\nEpoch 132/200\n430/430 [==============================] - 14s 32ms/step - loss: 5.1443e-06 - accuracy: 1.0000 - val_loss: 0.9613 - val_accuracy: 0.8280\nEpoch 133/200\n430/430 [==============================] - 13s 31ms/step - loss: 4.3844e-06 - accuracy: 1.0000 - val_loss: 0.9624 - val_accuracy: 0.8270\nEpoch 134/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.7104e-06 - accuracy: 1.0000 - val_loss: 0.9700 - val_accuracy: 0.8297\nEpoch 135/200\n430/430 [==============================] - 14s 32ms/step - loss: 4.3481e-06 - accuracy: 1.0000 - val_loss: 0.9648 - val_accuracy: 0.8277\nEpoch 136/200\n430/430 [==============================] - 13s 29ms/step - loss: 4.4548e-06 - accuracy: 1.0000 - val_loss: 0.9706 - val_accuracy: 0.8257\nEpoch 137/200\n430/430 [==============================] - 14s 32ms/step - loss: 4.1470e-06 - accuracy: 1.0000 - val_loss: 0.9704 - val_accuracy: 0.8284\nEpoch 138/200\n430/430 [==============================] - 13s 31ms/step - loss: 4.2580e-06 - accuracy: 1.0000 - val_loss: 0.9677 - val_accuracy: 0.8287\nEpoch 139/200\n430/430 [==============================] - 15s 36ms/step - loss: 4.2131e-06 - accuracy: 1.0000 - val_loss: 0.9620 - val_accuracy: 0.8321\nEpoch 140/200\n430/430 [==============================] - 13s 29ms/step - loss: 4.5256e-06 - accuracy: 1.0000 - val_loss: 0.9707 - val_accuracy: 0.8277\nEpoch 141/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.2387e-06 - accuracy: 1.0000 - val_loss: 0.9643 - val_accuracy: 0.8284\nEpoch 142/200\n430/430 [==============================] - 13s 29ms/step - loss: 3.7145e-06 - accuracy: 1.0000 - val_loss: 0.9664 - val_accuracy: 0.8297\nEpoch 143/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.1981e-06 - accuracy: 1.0000 - val_loss: 0.9660 - val_accuracy: 0.8280\nEpoch 144/200\n430/430 [==============================] - 13s 31ms/step - loss: 4.1779e-06 - accuracy: 1.0000 - val_loss: 0.9653 - val_accuracy: 0.8270\nEpoch 145/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.1551e-06 - accuracy: 1.0000 - val_loss: 0.9692 - val_accuracy: 0.8304\nEpoch 146/200\n430/430 [==============================] - 14s 32ms/step - loss: 3.8474e-06 - accuracy: 1.0000 - val_loss: 0.9694 - val_accuracy: 0.8277\nEpoch 147/200\n430/430 [==============================] - 12s 29ms/step - loss: 3.9014e-06 - accuracy: 1.0000 - val_loss: 0.9619 - val_accuracy: 0.8274\nEpoch 148/200\n430/430 [==============================] - 14s 32ms/step - loss: 3.9134e-06 - accuracy: 1.0000 - val_loss: 0.9716 - val_accuracy: 0.8270\nEpoch 149/200\n430/430 [==============================] - 14s 32ms/step - loss: 3.9366e-06 - accuracy: 1.0000 - val_loss: 0.9687 - val_accuracy: 0.8257\nEpoch 150/200\n430/430 [==============================] - 14s 33ms/step - loss: 4.1918e-06 - accuracy: 1.0000 - val_loss: 0.9734 - val_accuracy: 0.8287\nEpoch 151/200\n430/430 [==============================] - 12s 29ms/step - loss: 4.0942e-06 - accuracy: 1.0000 - val_loss: 0.9694 - val_accuracy: 0.8240\nEpoch 152/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.6007e-06 - accuracy: 1.0000 - val_loss: 0.9667 - val_accuracy: 0.8294\nEpoch 153/200\n430/430 [==============================] - 13s 31ms/step - loss: 3.9258e-06 - accuracy: 1.0000 - val_loss: 0.9705 - val_accuracy: 0.8324\nEpoch 154/200\n430/430 [==============================] - 13s 31ms/step - loss: 3.7596e-06 - accuracy: 1.0000 - val_loss: 0.9685 - val_accuracy: 0.8280\nEpoch 155/200\n430/430 [==============================] - 13s 31ms/step - loss: 4.0397e-06 - accuracy: 1.0000 - val_loss: 0.9653 - val_accuracy: 0.8318\nEpoch 156/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.1027e-06 - accuracy: 1.0000 - val_loss: 0.9612 - val_accuracy: 0.8260\nEpoch 157/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.8979e-06 - accuracy: 1.0000 - val_loss: 0.9720 - val_accuracy: 0.8260\nEpoch 158/200\n430/430 [==============================] - 13s 29ms/step - loss: 4.1394e-06 - accuracy: 1.0000 - val_loss: 0.9685 - val_accuracy: 0.8250\nEpoch 159/200\n430/430 [==============================] - 15s 36ms/step - loss: 4.5121e-06 - accuracy: 1.0000 - val_loss: 0.9675 - val_accuracy: 0.8297\nEpoch 160/200\n430/430 [==============================] - 12s 29ms/step - loss: 4.4148e-06 - accuracy: 1.0000 - val_loss: 0.9719 - val_accuracy: 0.8284\nEpoch 161/200\n430/430 [==============================] - 12s 29ms/step - loss: 3.9794e-06 - accuracy: 1.0000 - val_loss: 0.9717 - val_accuracy: 0.8277\nEpoch 162/200\n430/430 [==============================] - 12s 29ms/step - loss: 4.1581e-06 - accuracy: 1.0000 - val_loss: 0.9727 - val_accuracy: 0.8291\nEpoch 163/200\n430/430 [==============================] - 13s 29ms/step - loss: 3.9359e-06 - accuracy: 1.0000 - val_loss: 0.9725 - val_accuracy: 0.8291\nEpoch 164/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.5638e-06 - accuracy: 1.0000 - val_loss: 0.9716 - val_accuracy: 0.8291\nEpoch 165/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.6724e-06 - accuracy: 1.0000 - val_loss: 0.9811 - val_accuracy: 0.8280\nEpoch 166/200\n430/430 [==============================] - 13s 31ms/step - loss: 4.2020e-06 - accuracy: 1.0000 - val_loss: 0.9648 - val_accuracy: 0.8270\nEpoch 167/200\n430/430 [==============================] - 12s 29ms/step - loss: 5.0965e-06 - accuracy: 1.0000 - val_loss: 0.9769 - val_accuracy: 0.8243\nEpoch 168/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.8809e-06 - accuracy: 1.0000 - val_loss: 0.9697 - val_accuracy: 0.8270\nEpoch 169/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.3174e-06 - accuracy: 1.0000 - val_loss: 0.9642 - val_accuracy: 0.8260\nEpoch 170/200\n430/430 [==============================] - 16s 37ms/step - loss: 4.2288e-06 - accuracy: 1.0000 - val_loss: 0.9741 - val_accuracy: 0.8274\nEpoch 171/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.1263e-06 - accuracy: 1.0000 - val_loss: 0.9675 - val_accuracy: 0.8277\nEpoch 172/200\n430/430 [==============================] - 13s 29ms/step - loss: 4.6132e-06 - accuracy: 1.0000 - val_loss: 0.9695 - val_accuracy: 0.8294\nEpoch 173/200\n430/430 [==============================] - 14s 33ms/step - loss: 3.7752e-06 - accuracy: 1.0000 - val_loss: 0.9712 - val_accuracy: 0.8294\nEpoch 174/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.3888e-06 - accuracy: 1.0000 - val_loss: 0.9642 - val_accuracy: 0.8270\nEpoch 175/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.0670e-06 - accuracy: 1.0000 - val_loss: 0.9738 - val_accuracy: 0.8250\nEpoch 176/200\n430/430 [==============================] - 12s 28ms/step - loss: 3.7266e-06 - accuracy: 1.0000 - val_loss: 0.9697 - val_accuracy: 0.8277\nEpoch 177/200\n430/430 [==============================] - 13s 29ms/step - loss: 3.6586e-06 - accuracy: 1.0000 - val_loss: 0.9695 - val_accuracy: 0.8267\nEpoch 178/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.7945e-06 - accuracy: 1.0000 - val_loss: 0.9686 - val_accuracy: 0.8267\nEpoch 179/200\n430/430 [==============================] - 12s 29ms/step - loss: 3.9636e-06 - accuracy: 1.0000 - val_loss: 0.9763 - val_accuracy: 0.8291\nEpoch 180/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.7577e-06 - accuracy: 1.0000 - val_loss: 0.9656 - val_accuracy: 0.8287\nEpoch 181/200\n430/430 [==============================] - 12s 28ms/step - loss: 3.8091e-06 - accuracy: 1.0000 - val_loss: 0.9727 - val_accuracy: 0.8270\nEpoch 182/200\n430/430 [==============================] - 12s 28ms/step - loss: 4.6109e-06 - accuracy: 1.0000 - val_loss: 0.9697 - val_accuracy: 0.8264\nEpoch 183/200\n430/430 [==============================] - 13s 31ms/step - loss: 4.2366e-06 - accuracy: 1.0000 - val_loss: 0.9761 - val_accuracy: 0.8253\nEpoch 184/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.9117e-06 - accuracy: 1.0000 - val_loss: 0.9791 - val_accuracy: 0.8236\nEpoch 185/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.2713e-06 - accuracy: 1.0000 - val_loss: 0.9775 - val_accuracy: 0.8264\nEpoch 186/200\n430/430 [==============================] - 12s 28ms/step - loss: 4.0719e-06 - accuracy: 1.0000 - val_loss: 0.9737 - val_accuracy: 0.8284\nEpoch 187/200\n430/430 [==============================] - 13s 31ms/step - loss: 3.8040e-06 - accuracy: 1.0000 - val_loss: 0.9724 - val_accuracy: 0.8267\nEpoch 188/200\n430/430 [==============================] - 12s 29ms/step - loss: 3.7251e-06 - accuracy: 1.0000 - val_loss: 0.9778 - val_accuracy: 0.8267\nEpoch 189/200\n430/430 [==============================] - 13s 29ms/step - loss: 4.4784e-06 - accuracy: 1.0000 - val_loss: 0.9679 - val_accuracy: 0.8284\nEpoch 190/200\n430/430 [==============================] - 13s 29ms/step - loss: 3.8919e-06 - accuracy: 1.0000 - val_loss: 0.9735 - val_accuracy: 0.8277\nEpoch 191/200\n430/430 [==============================] - 12s 28ms/step - loss: 4.1955e-06 - accuracy: 1.0000 - val_loss: 0.9751 - val_accuracy: 0.8280\nEpoch 192/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.6891e-06 - accuracy: 1.0000 - val_loss: 0.9704 - val_accuracy: 0.8280\nEpoch 193/200\n430/430 [==============================] - 13s 29ms/step - loss: 3.8049e-06 - accuracy: 1.0000 - val_loss: 0.9718 - val_accuracy: 0.8287\nEpoch 194/200\n430/430 [==============================] - 12s 29ms/step - loss: 4.0444e-06 - accuracy: 1.0000 - val_loss: 0.9752 - val_accuracy: 0.8294\nEpoch 195/200\n430/430 [==============================] - 12s 28ms/step - loss: 3.9781e-06 - accuracy: 1.0000 - val_loss: 0.9658 - val_accuracy: 0.8297\nEpoch 196/200\n430/430 [==============================] - 13s 29ms/step - loss: 3.8021e-06 - accuracy: 1.0000 - val_loss: 0.9745 - val_accuracy: 0.8270\nEpoch 197/200\n430/430 [==============================] - 13s 31ms/step - loss: 3.7240e-06 - accuracy: 1.0000 - val_loss: 0.9856 - val_accuracy: 0.8267\nEpoch 198/200\n430/430 [==============================] - 13s 30ms/step - loss: 4.1556e-06 - accuracy: 1.0000 - val_loss: 0.9634 - val_accuracy: 0.8284\nEpoch 199/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.8024e-06 - accuracy: 1.0000 - val_loss: 0.9666 - val_accuracy: 0.8297\nEpoch 200/200\n430/430 [==============================] - 12s 29ms/step - loss: 3.9332e-06 - accuracy: 1.0000 - val_loss: 0.9705 - val_accuracy: 0.8294\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = []  # True labels for the test data\npredicted_labels = []  # Predicted labels for the test data\n\nfor batch_data, batch_labels in test_dataset:\n    batch_predictions = model.predict(batch_data)\n    batch_predicted_labels = np.argmax(batch_predictions, axis=1)\n    predicted_labels.extend(batch_predicted_labels)\n    true_labels.extend(np.argmax(batch_labels, axis=1))\n\ntrue_labels = np.array(true_labels)\npredicted_labels = np.array(predicted_labels)\n\naccuracy = np.mean(true_labels == predicted_labels)\n\nprint(accuracy)","metadata":{"papermill":{"duration":23.887179,"end_time":"2023-05-21T13:12:05.959505","exception":false,"start_time":"2023-05-21T13:11:42.072326","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T15:15:17.761219Z","iopub.execute_input":"2023-07-19T15:15:17.761637Z","iopub.status.idle":"2023-07-19T15:15:37.301991Z","shell.execute_reply.started":"2023-07-19T15:15:17.761583Z","shell.execute_reply":"2023-07-19T15:15:37.300953Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 48ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 39ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 42ms/step\n1/1 [==============================] - 0s 53ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 36ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 55ms/step\n1/1 [==============================] - 0s 38ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 52ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 58ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 48ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 40ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 29ms/step\n0.8948863636363636\n","output_type":"stream"}]}]}