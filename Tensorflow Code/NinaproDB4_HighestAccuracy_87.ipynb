{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.025217,"end_time":"2023-08-06T21:57:19.317113","exception":false,"start_time":"2023-08-06T21:57:19.291896","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T09:59:34.665635Z","iopub.execute_input":"2023-08-09T09:59:34.666052Z","iopub.status.idle":"2023-08-09T09:59:35.102789Z","shell.execute_reply.started":"2023-08-09T09:59:34.666017Z","shell.execute_reply":"2023-08-09T09:59:35.101982Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/newninaprodb4/ninaprodb44test.pkl\n/kaggle/input/newninaprodb4/ninaprodb44train.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom tensorflow.keras.layers import TimeDistributed, Conv1D\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import Activation\n\n# ... (other code) ...\n\n\nimport random\n#from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport math","metadata":{"papermill":{"duration":9.307141,"end_time":"2023-08-06T21:57:28.628582","exception":false,"start_time":"2023-08-06T21:57:19.321441","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T09:59:35.104266Z","iopub.execute_input":"2023-08-09T09:59:35.104612Z","iopub.status.idle":"2023-08-09T10:00:15.478739Z","shell.execute_reply.started":"2023-08-09T09:59:35.104586Z","shell.execute_reply":"2023-08-09T10:00:15.477827Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"D0809 10:00:06.905497169      15 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD0809 10:00:06.905519709      15 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD0809 10:00:06.905523079      15 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD0809 10:00:06.905525630      15 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD0809 10:00:06.905528072      15 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD0809 10:00:06.905530907      15 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD0809 10:00:06.905533334      15 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD0809 10:00:06.905536617      15 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD0809 10:00:06.905539038      15 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD0809 10:00:06.905541337      15 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD0809 10:00:06.905543702      15 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD0809 10:00:06.905545972      15 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD0809 10:00:06.905548380      15 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD0809 10:00:06.905550797      15 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI0809 10:00:06.905779319      15 ev_epoll1_linux.cc:122]               grpc epoll fd: 63\nD0809 10:00:06.911247197      15 ev_posix.cc:144]                      Using polling engine: epoll1\nD0809 10:00:06.911274085      15 dns_resolver_ares.cc:822]             Using ares dns resolver\nD0809 10:00:06.911727832      15 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0809 10:00:06.911739895      15 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0809 10:00:06.911743771      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0809 10:00:06.911747191      15 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0809 10:00:06.911750654      15 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0809 10:00:06.911753934      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD0809 10:00:06.911761171      15 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0809 10:00:06.911780222      15 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0809 10:00:06.911807508      15 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0809 10:00:06.911822462      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0809 10:00:06.911826917      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0809 10:00:06.911830557      15 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0809 10:00:06.911837377      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD0809 10:00:06.911841131      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0809 10:00:06.911844694      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0809 10:00:06.911849331      15 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI0809 10:00:06.914296618      15 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI0809 10:00:06.927445365     307 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0809 10:00:06.934870166     307 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2023-08-09T10:00:06.934854033+00:00\", grpc_status:2}\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"papermill":{"duration":1.004777,"end_time":"2023-08-06T21:57:29.646226","exception":false,"start_time":"2023-08-06T21:57:28.641449","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T10:00:15.479895Z","iopub.execute_input":"2023-08-09T10:00:15.480396Z","iopub.status.idle":"2023-08-09T10:00:16.103187Z","shell.execute_reply.started":"2023-08-09T10:00:15.480366Z","shell.execute_reply":"2023-08-09T10:00:16.102187Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers, Sequential, optimizers, Input, Model","metadata":{"papermill":{"duration":0.012163,"end_time":"2023-08-06T21:57:29.663303","exception":false,"start_time":"2023-08-06T21:57:29.651140","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T10:00:16.105516Z","iopub.execute_input":"2023-08-09T10:00:16.105981Z","iopub.status.idle":"2023-08-09T10:00:16.109922Z","shell.execute_reply.started":"2023-08-09T10:00:16.105953Z","shell.execute_reply":"2023-08-09T10:00:16.109179Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**WORKED CODE HERE**","metadata":{"papermill":{"duration":0.004019,"end_time":"2023-08-06T21:57:29.671448","exception":false,"start_time":"2023-08-06T21:57:29.667429","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n#import tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\nimport gc\nimport random\nimport os\n\nimport matplotlib.pyplot as plt\nimport json","metadata":{"papermill":{"duration":0.014276,"end_time":"2023-08-06T21:57:29.689823","exception":false,"start_time":"2023-08-06T21:57:29.675547","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T10:00:16.110838Z","iopub.execute_input":"2023-08-09T10:00:16.111087Z","iopub.status.idle":"2023-08-09T10:00:17.288820Z","shell.execute_reply.started":"2023-08-09T10:00:16.111063Z","shell.execute_reply":"2023-08-09T10:00:17.287833Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"epochs = 64\nlearning_rate = 1e-3\nbatch_size = 16\nmethod = \"default\"\ndataset_type = 1","metadata":{"papermill":{"duration":0.012297,"end_time":"2023-08-06T21:57:29.708339","exception":false,"start_time":"2023-08-06T21:57:29.696042","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T10:00:17.290211Z","iopub.execute_input":"2023-08-09T10:00:17.290540Z","iopub.status.idle":"2023-08-09T10:00:17.295184Z","shell.execute_reply.started":"2023-08-09T10:00:17.290507Z","shell.execute_reply":"2023-08-09T10:00:17.294360Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"pip install PyWavelets\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T10:00:17.296248Z","iopub.execute_input":"2023-08-09T10:00:17.296521Z","iopub.status.idle":"2023-08-09T10:00:23.572359Z","shell.execute_reply.started":"2023-08-09T10:00:17.296497Z","shell.execute_reply":"2023-08-09T10:00:23.571374Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting PyWavelets\n  Downloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/site-packages (from PyWavelets) (1.23.5)\nInstalling collected packages: PyWavelets\nSuccessfully installed PyWavelets-1.4.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pywt  # Added import for wavelet denoising\nimport scipy.signal as signal  # Added import for power line noise and low pass filtering\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nimport pywt\nimport scipy.signal as signal\nimport tensorflow as tf\nclass Nina1Dataset(tf.keras.utils.Sequence):\n    def __init__(self, dataframe, batch_size):\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.scaler = StandardScaler()\n        self.scaler.fit(np.concatenate(self.dataframe['emg'].tolist()))\n        \n        \n    def __len__(self):\n        return int(np.ceil(len(self.dataframe) / self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_data = self.dataframe[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        batch_input_data = []\n        batch_labels = []\n\n        for i, target_row in batch_data.iterrows():\n            data = target_row['emg'][:10000]\n\n            # Zero-Padding\n            if len(data) < 10000:\n                data = np.concatenate((data, np.zeros((10000 - len(data), 12))), axis=0)\n\n            \n            if(target_row['stimulus'] != 52):\n                fs = 2000  # Assuming sampling frequency of 2000 Hz\n                f0 = 50  # Power line frequency\n                Q = 30  # Quality factor\n                w0 = f0 / (fs / 2)\n                b, a = signal.iirnotch(w0, Q)\n                data = signal.lfilter(b, a, data, axis=0)\n\n                    # Low Pass Filtering with cutoff frequency fc = 500 Hz\n                fc = 500  # Cutoff frequency\n                b, a = signal.butter(4, fc / (fs / 2), 'low')\n                data = signal.lfilter(b, a, data, axis=0)\n\n\n                coeffs = pywt.wavedec(data, 'sym8')  \n                coeffs[1:] = (pywt.threshold(c, value=0.5) for c in coeffs[1:])\n                data = pywt.waverec(coeffs, 'sym8')\n\n\n                # Wavelet Denoising\n\n\n\n\n                    # Standardize the data\n                data = self.scaler.transform(data)\n                #data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n\n                    # Division data by time-segment (channel-wise)\n                input_data = data.reshape((25, 400, 12))\n                label = target_row['stimulus']\n                batch_input_data.append(input_data)\n                batch_labels.append(label)\n\n        # Check if the batch size is smaller than the desired batch_size\n        if len(batch_data) < self.batch_size:\n            # Create a dummy batch with all elements set to zero\n            dummy_input_data = np.zeros((self.batch_size,) + input_data.shape, dtype=np.float32)\n            dummy_labels = np.zeros((self.batch_size,), dtype=np.int32)\n            dummy_input_data[:len(batch_input_data)] = np.array(batch_input_data)\n            dummy_labels[:len(batch_labels)] = np.array(batch_labels)\n            dummy_labels = to_categorical(dummy_labels, num_classes=52)  # Convert labels to one-hot encoding\n\n            return dummy_input_data, dummy_labels\n\n        batch_labels = to_categorical(batch_labels, num_classes=52)  # Convert labels to one-hot encoding\n\n        return np.array(batch_input_data), np.array(batch_labels)\n\n# Parameters\n\nbatch_size = 16\ntrain_dir = '/kaggle/input/newninaprodb4/ninaprodb44train.pkl'\ntest_dir = '/kaggle/input/newninaprodb4/ninaprodb44test.pkl'\n\n# Set up dataset\ntrain = pd.read_pickle(train_dir)\neval_data = pd.read_pickle(test_dir)\n\n# Load train data\ntrain_data = pd.read_pickle(train_dir)\n\nimport pandas as pd\n\n# Assuming you have your DataFrame 'train_data' already defined\n\nselected_categories = [51, 28, 11]\n\nadditional_data = []  # Initialize an empty list to store the additional data\n\nfor category in selected_categories:\n    category_data = train_data[train_data['stimulus'] == category]\n    existing_count = len(category_data)\n    \n    if existing_count < 40:\n        additional_count = 40 - existing_count\n        if additional_count > 0:\n            sampled_data = category_data.sample(min(additional_count, len(category_data)), replace=True)\n            additional_data.append(sampled_data)\n\n# Concatenate the additional data into a new DataFrame\nadditional_data_df = pd.concat(additional_data, ignore_index=True)\n\n# Concatenate the original train_data and the additional_data_df\ntrain_data = pd.concat([train_data, additional_data_df], ignore_index=True)\n\nprint(train_data['stimulus'].value_counts())  # Display updated category counts\n\n\n# Shuffle data\ntrain_data = train_data.sample(frac=1).reset_index(drop=True)\neval_data = eval_data.sample(frac=1).reset_index(drop=True)\n#train_data, val_data = train_test_split(train_data, test_size=0.3, random_state=21)\n# Create train and test datasets\ntrain_dataset = Nina1Dataset(train_data, batch_size=batch_size)\n#val_dataset = Nina1Dataset(val_data, batch_size=batch_size)\ntest_dataset = Nina1Dataset(eval_data, batch_size=batch_size)\n\nprint(train_dataset)\n","metadata":{"papermill":{"duration":20.88191,"end_time":"2023-08-06T21:57:50.594783","exception":false,"start_time":"2023-08-06T21:57:29.712873","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T10:24:47.422648Z","iopub.execute_input":"2023-08-09T10:24:47.423130Z","iopub.status.idle":"2023-08-09T10:24:58.094951Z","shell.execute_reply.started":"2023-08-09T10:24:47.423093Z","shell.execute_reply":"2023-08-09T10:24:58.093705Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"stimulus\n0     40\n1     40\n28    40\n29    40\n30    40\n31    40\n32    40\n33    40\n34    40\n35    40\n36    40\n37    40\n38    40\n39    40\n40    40\n41    40\n42    40\n43    40\n44    40\n45    40\n46    40\n47    40\n48    40\n49    40\n50    40\n27    40\n26    40\n25    40\n12    40\n2     40\n3     40\n4     40\n5     40\n6     40\n7     40\n8     40\n9     40\n10    40\n11    40\n13    40\n24    40\n14    40\n15    40\n16    40\n17    40\n18    40\n19    40\n20    40\n21    40\n22    40\n23    40\n51    40\nName: count, dtype: int64\n<__main__.Nina1Dataset object at 0x7cf9c08410a0>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_data['stimulus'])","metadata":{"papermill":{"duration":0.017775,"end_time":"2023-08-06T21:57:50.617152","exception":false,"start_time":"2023-08-06T21:57:50.599377","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T10:02:01.474542Z","iopub.execute_input":"2023-08-09T10:02:01.474910Z","iopub.status.idle":"2023-08-09T10:02:01.482328Z","shell.execute_reply.started":"2023-08-09T10:02:01.474877Z","shell.execute_reply":"2023-08-09T10:02:01.481275Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"0        0\n1       12\n2       29\n3       35\n4       14\n        ..\n2075    47\n2076    28\n2077    26\n2078    40\n2079     6\nName: stimulus, Length: 2080, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"print(eval_data['stimulus'])","metadata":{"execution":{"iopub.status.busy":"2023-08-09T10:02:01.483647Z","iopub.execute_input":"2023-08-09T10:02:01.483976Z","iopub.status.idle":"2023-08-09T10:02:01.503719Z","shell.execute_reply.started":"2023-08-09T10:02:01.483947Z","shell.execute_reply":"2023-08-09T10:02:01.502678Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"0       41\n1       17\n2        8\n3       29\n4       15\n        ..\n1035     2\n1036    19\n1037    31\n1038     9\n1039    31\nName: stimulus, Length: 1040, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras import regularizers, initializers\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\n# Define the CNN-BiLSTM model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.layers import TimeDistributed\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\n# Define the CNN-BiLSTM model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras import regularizers, initializers\ndef cnn(x):\n    #print(x.shape)\n    #x = Reshape((25, 20,10))(x)  \n    print(x.shape)\n    x = TimeDistributed((Conv1D(64, kernel_size=9, strides=2, padding='same', activation=tanh)))(x)\n    print(x.shape)\n    #(Batch, 10, 64)\n    #x = TimeDistributed(Dropout(0.2093))(x)\n    #x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    \n    x = TimeDistributed(MaxPooling1D(pool_size=8, strides=2))(x)\n    print(x.shape)\n    #(Batch, 2, 64)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(Dropout(0.2093))(x)\n    \n    \n    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n    #(Batch, 1, 64)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(Dropout(0.2093))(x)\n    \n    \n    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n    print(x.shape)\n    #(Batch, 1, 64)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(Dropout(0.2093))(x)\n   \n    \n    x = TimeDistributed((Conv1D(64, kernel_size=3, strides=2, padding='same', activation=tanh)))(x)\n    #print(x.shape)\n    #(Batch, 1, 64)\n    x = TimeDistributed(Dropout(0.2093))(x)\n    x = TimeDistributed(Dropout(0.5))(x)\n    #x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(Flatten())(x)\n    print(x.shape)\n    # (Batch, 64)\n\n    return x\n\ndef Bi_LSTMModel(input_shape,x):\n    #model = Sequential()\n    # Hidden dimensions\n    hidden_dim = 200\n    print(x.shape)\n    print(11)\n    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.3), input_shape=input_shape)(x)\n    #x = Dropout(0.2093)(x)\n    print(x.shape)\n    print(11)\n    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.3))(x)\n\n    print(x.shape)\n    print(11)\n    #x = Dropout(0.2093)(x)\n    x = Flatten()(x)\n    # ( ,10000)\n\n    return x\ndef EMGHandNet(input_shape, num_classes):\n    # Define the input layer\n    x = Input(shape=input_shape)\n    inputs = x\n    #print(x.shape)\n    #(batch, 25, 20, 10)\n    #temp = [cnn(x[:, t, :, :]) for t in range(x.shape[1])]\n    #x = tf.stack(temp, axis=1)\n    #print(x.shape)\n    x = cnn(x)\n   \n    #print(x.shape)\n    x = Bi_LSTMModel(x.shape[1:],x)\n    #print(x.shape)\n    #x = Dropout(0.2093)(x)\n    \n    x = Dense(512, activation='tanh')(x)\n    #print(x.shape)\n    x = Dropout(0.2093)(x)\n    #x = BatchNormalization(epsilon=1e-6, momentum=0.95)(x)\n\n    # Add the output layer\n    output_layer = Dense(52, activation='softmax')(x)\n    # Create the model\n    model = Model(inputs=inputs, outputs=output_layer)\n\n    return model\n\nnum_classes = 52\nmodel = EMGHandNet((25, 400, 12), num_classes)\n\n\ninitial_learning_rate = 0.001\ndecay_steps = 1000\ndecay_rate = 0.9\nbatch_size = 16\n# Define your model and its optimizer\n# Define learning rate schedule\nlr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n\n    # Compile the model with learning rate schedule\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999),\n                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n                metrics=['accuracy'])\n\n\n# Define your model and its optimizer\n# Define learning rate schedule\n\n\nhistory = model.fit(train_dataset,\n                    epochs=200,\n                    batch_size=16,\n                    validation_data=test_dataset)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T10:29:37.730145Z","iopub.execute_input":"2023-08-09T10:29:37.730622Z","iopub.status.idle":"2023-08-09T11:45:48.778561Z","shell.execute_reply.started":"2023-08-09T10:29:37.730584Z","shell.execute_reply":"2023-08-09T11:45:48.777285Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"(None, 25, 400, 12)\n(None, 25, 200, 64)\n(None, 25, 97, 64)\n(None, 25, 25, 64)\n(None, 25, 832)\n(None, 25, 832)\n11\n(None, 25, 400)\n11\n(None, 25, 400)\n11\nEpoch 1/200\n130/130 [==============================] - 36s 191ms/step - loss: 3.9178 - accuracy: 0.0755 - val_loss: 2.9570 - val_accuracy: 0.2115\nEpoch 2/200\n130/130 [==============================] - 23s 174ms/step - loss: 3.0015 - accuracy: 0.2014 - val_loss: 2.5069 - val_accuracy: 0.2788\nEpoch 3/200\n130/130 [==============================] - 23s 174ms/step - loss: 2.3931 - accuracy: 0.3149 - val_loss: 2.1492 - val_accuracy: 0.3500\nEpoch 4/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.9580 - accuracy: 0.4202 - val_loss: 1.6926 - val_accuracy: 0.4548\nEpoch 5/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.6848 - accuracy: 0.4923 - val_loss: 1.5732 - val_accuracy: 0.5010\nEpoch 6/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.4006 - accuracy: 0.5611 - val_loss: 1.3106 - val_accuracy: 0.5798\nEpoch 7/200\n130/130 [==============================] - 23s 173ms/step - loss: 1.2424 - accuracy: 0.6106 - val_loss: 1.5000 - val_accuracy: 0.5279\nEpoch 8/200\n130/130 [==============================] - 23s 176ms/step - loss: 1.0966 - accuracy: 0.6428 - val_loss: 1.1652 - val_accuracy: 0.6279\nEpoch 9/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.9451 - accuracy: 0.6899 - val_loss: 1.0777 - val_accuracy: 0.6596\nEpoch 10/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.7844 - accuracy: 0.7370 - val_loss: 0.9866 - val_accuracy: 0.6817\nEpoch 11/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.6045 - accuracy: 0.8014 - val_loss: 0.8495 - val_accuracy: 0.7212\nEpoch 12/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.4984 - accuracy: 0.8438 - val_loss: 0.8117 - val_accuracy: 0.7298\nEpoch 13/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.4621 - accuracy: 0.8529 - val_loss: 0.8473 - val_accuracy: 0.7183\nEpoch 14/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.3598 - accuracy: 0.8899 - val_loss: 0.7537 - val_accuracy: 0.7567\nEpoch 15/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.3007 - accuracy: 0.9077 - val_loss: 0.7019 - val_accuracy: 0.7779\nEpoch 16/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.2637 - accuracy: 0.9221 - val_loss: 0.7098 - val_accuracy: 0.7731\nEpoch 17/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.2389 - accuracy: 0.9260 - val_loss: 0.8017 - val_accuracy: 0.7385\nEpoch 18/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.1959 - accuracy: 0.9413 - val_loss: 0.6392 - val_accuracy: 0.8019\nEpoch 19/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.1535 - accuracy: 0.9587 - val_loss: 0.7144 - val_accuracy: 0.7721\nEpoch 20/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.1642 - accuracy: 0.9466 - val_loss: 0.6955 - val_accuracy: 0.7894\nEpoch 21/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.1715 - accuracy: 0.9466 - val_loss: 0.7749 - val_accuracy: 0.7558\nEpoch 22/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.1484 - accuracy: 0.9543 - val_loss: 0.6687 - val_accuracy: 0.7856\nEpoch 23/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.1325 - accuracy: 0.9611 - val_loss: 0.6656 - val_accuracy: 0.7827\nEpoch 24/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.1078 - accuracy: 0.9688 - val_loss: 0.7169 - val_accuracy: 0.7760\nEpoch 25/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.1210 - accuracy: 0.9683 - val_loss: 0.6699 - val_accuracy: 0.7981\nEpoch 26/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0769 - accuracy: 0.9793 - val_loss: 0.6334 - val_accuracy: 0.8183\nEpoch 27/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0659 - accuracy: 0.9870 - val_loss: 0.6005 - val_accuracy: 0.8183\nEpoch 28/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0579 - accuracy: 0.9856 - val_loss: 0.6702 - val_accuracy: 0.8106\nEpoch 29/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0545 - accuracy: 0.9865 - val_loss: 0.6129 - val_accuracy: 0.8135\nEpoch 30/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0520 - accuracy: 0.9899 - val_loss: 0.6734 - val_accuracy: 0.8058\nEpoch 31/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0312 - accuracy: 0.9952 - val_loss: 0.5768 - val_accuracy: 0.8327\nEpoch 32/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0288 - accuracy: 0.9942 - val_loss: 0.5655 - val_accuracy: 0.8317\nEpoch 33/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0311 - accuracy: 0.9942 - val_loss: 0.6914 - val_accuracy: 0.8019\nEpoch 34/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0250 - accuracy: 0.9962 - val_loss: 0.5664 - val_accuracy: 0.8423\nEpoch 35/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0284 - accuracy: 0.9942 - val_loss: 0.6141 - val_accuracy: 0.8260\nEpoch 36/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0404 - accuracy: 0.9889 - val_loss: 0.6458 - val_accuracy: 0.8212\nEpoch 37/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.2349 - accuracy: 0.9288 - val_loss: 0.8375 - val_accuracy: 0.7683\nEpoch 38/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.2000 - accuracy: 0.9337 - val_loss: 0.8015 - val_accuracy: 0.7683\nEpoch 39/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.1845 - accuracy: 0.9413 - val_loss: 0.7447 - val_accuracy: 0.7798\nEpoch 40/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.1346 - accuracy: 0.9601 - val_loss: 0.6931 - val_accuracy: 0.8010\nEpoch 41/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0858 - accuracy: 0.9736 - val_loss: 0.6200 - val_accuracy: 0.8260\nEpoch 42/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0666 - accuracy: 0.9827 - val_loss: 0.6509 - val_accuracy: 0.8096\nEpoch 43/200\n130/130 [==============================] - 22s 172ms/step - loss: 0.0421 - accuracy: 0.9875 - val_loss: 0.6280 - val_accuracy: 0.8298\nEpoch 44/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0331 - accuracy: 0.9942 - val_loss: 0.6624 - val_accuracy: 0.8183\nEpoch 45/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0347 - accuracy: 0.9918 - val_loss: 0.6478 - val_accuracy: 0.8144\nEpoch 46/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0230 - accuracy: 0.9957 - val_loss: 0.5811 - val_accuracy: 0.8375\nEpoch 47/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0234 - accuracy: 0.9952 - val_loss: 0.6542 - val_accuracy: 0.8317\nEpoch 48/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0238 - accuracy: 0.9947 - val_loss: 0.6569 - val_accuracy: 0.8154\nEpoch 49/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0217 - accuracy: 0.9962 - val_loss: 0.6775 - val_accuracy: 0.8279\nEpoch 50/200\n130/130 [==============================] - 22s 173ms/step - loss: 0.0164 - accuracy: 0.9957 - val_loss: 0.6650 - val_accuracy: 0.8298\nEpoch 51/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0163 - accuracy: 0.9981 - val_loss: 0.6806 - val_accuracy: 0.8115\nEpoch 52/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0174 - accuracy: 0.9957 - val_loss: 0.6900 - val_accuracy: 0.8192\nEpoch 53/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0198 - accuracy: 0.9952 - val_loss: 0.6330 - val_accuracy: 0.8337\nEpoch 54/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0247 - accuracy: 0.9933 - val_loss: 0.6590 - val_accuracy: 0.8288\nEpoch 55/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0197 - accuracy: 0.9962 - val_loss: 0.6699 - val_accuracy: 0.8308\nEpoch 56/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0138 - accuracy: 0.9971 - val_loss: 0.6826 - val_accuracy: 0.8183\nEpoch 57/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0337 - accuracy: 0.9913 - val_loss: 0.6554 - val_accuracy: 0.8231\nEpoch 58/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0295 - accuracy: 0.9909 - val_loss: 0.7300 - val_accuracy: 0.8125\nEpoch 59/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0282 - accuracy: 0.9937 - val_loss: 0.6166 - val_accuracy: 0.8365\nEpoch 60/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0477 - accuracy: 0.9846 - val_loss: 0.8229 - val_accuracy: 0.7894\nEpoch 61/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0550 - accuracy: 0.9798 - val_loss: 0.6911 - val_accuracy: 0.8115\nEpoch 62/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0973 - accuracy: 0.9707 - val_loss: 0.7085 - val_accuracy: 0.8087\nEpoch 63/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0411 - accuracy: 0.9880 - val_loss: 0.6490 - val_accuracy: 0.8260\nEpoch 64/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0329 - accuracy: 0.9923 - val_loss: 0.6954 - val_accuracy: 0.8192\nEpoch 65/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0232 - accuracy: 0.9937 - val_loss: 0.6668 - val_accuracy: 0.8106\nEpoch 66/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0135 - accuracy: 0.9976 - val_loss: 0.6279 - val_accuracy: 0.8337\nEpoch 67/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0180 - accuracy: 0.9962 - val_loss: 0.6391 - val_accuracy: 0.8240\nEpoch 68/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0115 - accuracy: 0.9986 - val_loss: 0.5914 - val_accuracy: 0.8490\nEpoch 69/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0157 - accuracy: 0.9971 - val_loss: 0.6284 - val_accuracy: 0.8337\nEpoch 70/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0106 - accuracy: 0.9990 - val_loss: 0.5601 - val_accuracy: 0.8548\nEpoch 71/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0055 - accuracy: 1.0000 - val_loss: 0.5708 - val_accuracy: 0.8519\nEpoch 72/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.5576 - val_accuracy: 0.8519\nEpoch 73/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.5835 - val_accuracy: 0.8510\nEpoch 74/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0052 - accuracy: 0.9995 - val_loss: 0.5597 - val_accuracy: 0.8587\nEpoch 75/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.5460 - val_accuracy: 0.8587\nEpoch 76/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.5357 - val_accuracy: 0.8702\nEpoch 77/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.5470 - val_accuracy: 0.8654\nEpoch 78/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.5354 - val_accuracy: 0.8654\nEpoch 79/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5508 - val_accuracy: 0.8625\nEpoch 80/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.5758 - val_accuracy: 0.8625\nEpoch 81/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0034 - accuracy: 0.9995 - val_loss: 0.5855 - val_accuracy: 0.8538\nEpoch 82/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.5768 - val_accuracy: 0.8606\nEpoch 83/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.5509 - val_accuracy: 0.8663\nEpoch 84/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.5450 - val_accuracy: 0.8721\nEpoch 85/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.5963 - val_accuracy: 0.8606\nEpoch 86/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0086 - accuracy: 0.9971 - val_loss: 0.8416 - val_accuracy: 0.8096\nEpoch 87/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0382 - accuracy: 0.9899 - val_loss: 0.8716 - val_accuracy: 0.7962\nEpoch 88/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0490 - accuracy: 0.9856 - val_loss: 0.8784 - val_accuracy: 0.8010\nEpoch 89/200\n130/130 [==============================] - 22s 172ms/step - loss: 0.0329 - accuracy: 0.9918 - val_loss: 0.6748 - val_accuracy: 0.8375\nEpoch 90/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0151 - accuracy: 0.9957 - val_loss: 0.6593 - val_accuracy: 0.8423\nEpoch 91/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0098 - accuracy: 0.9976 - val_loss: 0.6273 - val_accuracy: 0.8462\nEpoch 92/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.6406 - val_accuracy: 0.8500\nEpoch 93/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0141 - accuracy: 0.9957 - val_loss: 0.6698 - val_accuracy: 0.8365\nEpoch 94/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0067 - accuracy: 0.9981 - val_loss: 0.6513 - val_accuracy: 0.8500\nEpoch 95/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 0.6426 - val_accuracy: 0.8500\nEpoch 96/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0037 - accuracy: 0.9995 - val_loss: 0.6308 - val_accuracy: 0.8490\nEpoch 97/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.6022 - val_accuracy: 0.8644\nEpoch 98/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.5939 - val_accuracy: 0.8654\nEpoch 99/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.5778 - val_accuracy: 0.8663\nEpoch 100/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.5837 - val_accuracy: 0.8683\nEpoch 101/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.5615 - val_accuracy: 0.8731\nEpoch 102/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0035 - accuracy: 0.9990 - val_loss: 0.6720 - val_accuracy: 0.8346\nEpoch 103/200\n130/130 [==============================] - 22s 173ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.6038 - val_accuracy: 0.8625\nEpoch 104/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.5859 - val_accuracy: 0.8702\nEpoch 105/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.6934 - val_accuracy: 0.8452\nEpoch 106/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0206 - accuracy: 0.9933 - val_loss: 0.6583 - val_accuracy: 0.8452\nEpoch 107/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0082 - accuracy: 0.9990 - val_loss: 0.6182 - val_accuracy: 0.8481\nEpoch 108/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0122 - accuracy: 0.9962 - val_loss: 0.6080 - val_accuracy: 0.8471\nEpoch 109/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0064 - accuracy: 0.9986 - val_loss: 0.6448 - val_accuracy: 0.8452\nEpoch 110/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0061 - accuracy: 0.9981 - val_loss: 0.6190 - val_accuracy: 0.8529\nEpoch 111/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.6108 - val_accuracy: 0.8558\nEpoch 112/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.6063 - val_accuracy: 0.8596\nEpoch 113/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5798 - val_accuracy: 0.8635\nEpoch 114/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5536 - val_accuracy: 0.8750\nEpoch 115/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.5850 - val_accuracy: 0.8558\nEpoch 116/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5956 - val_accuracy: 0.8663\nEpoch 117/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.5970 - val_accuracy: 0.8606\nEpoch 118/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5910 - val_accuracy: 0.8596\nEpoch 119/200\n130/130 [==============================] - 23s 175ms/step - loss: 9.0794e-04 - accuracy: 1.0000 - val_loss: 0.5754 - val_accuracy: 0.8548\nEpoch 120/200\n130/130 [==============================] - 23s 175ms/step - loss: 7.7714e-04 - accuracy: 1.0000 - val_loss: 0.5516 - val_accuracy: 0.8625\nEpoch 121/200\n130/130 [==============================] - 23s 178ms/step - loss: 8.9308e-04 - accuracy: 1.0000 - val_loss: 0.5662 - val_accuracy: 0.8606\nEpoch 122/200\n130/130 [==============================] - 23s 175ms/step - loss: 5.6954e-04 - accuracy: 1.0000 - val_loss: 0.5700 - val_accuracy: 0.8644\nEpoch 123/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0052 - accuracy: 0.9976 - val_loss: 0.6570 - val_accuracy: 0.8413\nEpoch 124/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0048 - accuracy: 0.9990 - val_loss: 0.6030 - val_accuracy: 0.8548\nEpoch 125/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5958 - val_accuracy: 0.8558\nEpoch 126/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0019 - accuracy: 0.9995 - val_loss: 0.5915 - val_accuracy: 0.8644\nEpoch 127/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.5881 - val_accuracy: 0.8635\nEpoch 128/200\n130/130 [==============================] - 23s 174ms/step - loss: 8.3459e-04 - accuracy: 1.0000 - val_loss: 0.5986 - val_accuracy: 0.8673\nEpoch 129/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0037 - accuracy: 0.9986 - val_loss: 0.6365 - val_accuracy: 0.8558\nEpoch 130/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0025 - accuracy: 0.9995 - val_loss: 0.6557 - val_accuracy: 0.8558\nEpoch 131/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6327 - val_accuracy: 0.8567\nEpoch 132/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5989 - val_accuracy: 0.8635\nEpoch 133/200\n130/130 [==============================] - 23s 174ms/step - loss: 6.0965e-04 - accuracy: 1.0000 - val_loss: 0.6004 - val_accuracy: 0.8663\nEpoch 134/200\n130/130 [==============================] - 23s 175ms/step - loss: 9.7076e-04 - accuracy: 1.0000 - val_loss: 0.6420 - val_accuracy: 0.8577\nEpoch 135/200\n130/130 [==============================] - 23s 175ms/step - loss: 5.7583e-04 - accuracy: 1.0000 - val_loss: 0.6237 - val_accuracy: 0.8567\nEpoch 136/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.6323 - val_accuracy: 0.8567\nEpoch 137/200\n130/130 [==============================] - 23s 174ms/step - loss: 5.1833e-04 - accuracy: 1.0000 - val_loss: 0.6113 - val_accuracy: 0.8635\nEpoch 138/200\n130/130 [==============================] - 23s 175ms/step - loss: 5.5682e-04 - accuracy: 1.0000 - val_loss: 0.6014 - val_accuracy: 0.8625\nEpoch 139/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0100 - accuracy: 0.9962 - val_loss: 0.7007 - val_accuracy: 0.8404\nEpoch 140/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0058 - accuracy: 0.9986 - val_loss: 0.6324 - val_accuracy: 0.8596\nEpoch 141/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0020 - accuracy: 0.9995 - val_loss: 0.6531 - val_accuracy: 0.8423\nEpoch 142/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6781 - val_accuracy: 0.8423\nEpoch 143/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0022 - accuracy: 0.9990 - val_loss: 0.6483 - val_accuracy: 0.8529\nEpoch 144/200\n130/130 [==============================] - 23s 175ms/step - loss: 5.0211e-04 - accuracy: 1.0000 - val_loss: 0.6435 - val_accuracy: 0.8548\nEpoch 145/200\n130/130 [==============================] - 23s 175ms/step - loss: 8.7991e-04 - accuracy: 1.0000 - val_loss: 0.6337 - val_accuracy: 0.8577\nEpoch 146/200\n130/130 [==============================] - 23s 173ms/step - loss: 9.4027e-04 - accuracy: 1.0000 - val_loss: 0.6249 - val_accuracy: 0.8635\nEpoch 147/200\n130/130 [==============================] - 23s 176ms/step - loss: 4.8578e-04 - accuracy: 1.0000 - val_loss: 0.6075 - val_accuracy: 0.8683\nEpoch 148/200\n130/130 [==============================] - 23s 177ms/step - loss: 7.1499e-04 - accuracy: 1.0000 - val_loss: 0.6249 - val_accuracy: 0.8625\nEpoch 149/200\n130/130 [==============================] - 23s 173ms/step - loss: 4.2856e-04 - accuracy: 1.0000 - val_loss: 0.5972 - val_accuracy: 0.8673\nEpoch 150/200\n130/130 [==============================] - 23s 175ms/step - loss: 4.2911e-04 - accuracy: 1.0000 - val_loss: 0.5860 - val_accuracy: 0.8683\nEpoch 151/200\n130/130 [==============================] - 23s 175ms/step - loss: 4.2290e-04 - accuracy: 1.0000 - val_loss: 0.5929 - val_accuracy: 0.8692\nEpoch 152/200\n130/130 [==============================] - 23s 175ms/step - loss: 5.0773e-04 - accuracy: 1.0000 - val_loss: 0.6005 - val_accuracy: 0.8702\nEpoch 153/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.5997 - val_accuracy: 0.8654\nEpoch 154/200\n130/130 [==============================] - 23s 176ms/step - loss: 5.1974e-04 - accuracy: 1.0000 - val_loss: 0.5990 - val_accuracy: 0.8644\nEpoch 155/200\n130/130 [==============================] - 23s 176ms/step - loss: 9.7590e-04 - accuracy: 0.9995 - val_loss: 0.7002 - val_accuracy: 0.8365\nEpoch 156/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.6568 - val_accuracy: 0.8538\nEpoch 157/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6599 - val_accuracy: 0.8510\nEpoch 158/200\n130/130 [==============================] - 23s 175ms/step - loss: 4.9665e-04 - accuracy: 1.0000 - val_loss: 0.6313 - val_accuracy: 0.8606\nEpoch 159/200\n130/130 [==============================] - 23s 174ms/step - loss: 6.1705e-04 - accuracy: 1.0000 - val_loss: 0.6382 - val_accuracy: 0.8510\nEpoch 160/200\n130/130 [==============================] - 23s 173ms/step - loss: 2.8095e-04 - accuracy: 1.0000 - val_loss: 0.6181 - val_accuracy: 0.8567\nEpoch 161/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.6992 - val_accuracy: 0.8394\nEpoch 162/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.6353 - val_accuracy: 0.8471\nEpoch 163/200\n130/130 [==============================] - 23s 176ms/step - loss: 3.3357e-04 - accuracy: 1.0000 - val_loss: 0.6226 - val_accuracy: 0.8567\nEpoch 164/200\n130/130 [==============================] - 23s 175ms/step - loss: 3.4752e-04 - accuracy: 1.0000 - val_loss: 0.6127 - val_accuracy: 0.8596\nEpoch 165/200\n130/130 [==============================] - 23s 174ms/step - loss: 3.6566e-04 - accuracy: 1.0000 - val_loss: 0.6043 - val_accuracy: 0.8625\nEpoch 166/200\n130/130 [==============================] - 23s 175ms/step - loss: 8.1005e-04 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 0.8606\nEpoch 167/200\n130/130 [==============================] - 23s 174ms/step - loss: 3.1428e-04 - accuracy: 1.0000 - val_loss: 0.6005 - val_accuracy: 0.8606\nEpoch 168/200\n130/130 [==============================] - 23s 176ms/step - loss: 3.5004e-04 - accuracy: 1.0000 - val_loss: 0.6093 - val_accuracy: 0.8635\nEpoch 169/200\n130/130 [==============================] - 23s 174ms/step - loss: 2.0746e-04 - accuracy: 1.0000 - val_loss: 0.5954 - val_accuracy: 0.8721\nEpoch 170/200\n130/130 [==============================] - 23s 175ms/step - loss: 3.2059e-04 - accuracy: 1.0000 - val_loss: 0.6004 - val_accuracy: 0.8654\nEpoch 171/200\n130/130 [==============================] - 23s 176ms/step - loss: 3.9375e-04 - accuracy: 1.0000 - val_loss: 0.5964 - val_accuracy: 0.8654\nEpoch 172/200\n130/130 [==============================] - 23s 174ms/step - loss: 3.4229e-04 - accuracy: 1.0000 - val_loss: 0.5902 - val_accuracy: 0.8702\nEpoch 173/200\n130/130 [==============================] - 23s 175ms/step - loss: 2.2676e-04 - accuracy: 1.0000 - val_loss: 0.6015 - val_accuracy: 0.8644\nEpoch 174/200\n130/130 [==============================] - 22s 173ms/step - loss: 4.3316e-04 - accuracy: 1.0000 - val_loss: 0.6295 - val_accuracy: 0.8577\nEpoch 175/200\n130/130 [==============================] - 23s 176ms/step - loss: 2.1938e-04 - accuracy: 1.0000 - val_loss: 0.6195 - val_accuracy: 0.8654\nEpoch 176/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.8881e-04 - accuracy: 1.0000 - val_loss: 0.6133 - val_accuracy: 0.8673\nEpoch 177/200\n130/130 [==============================] - 23s 174ms/step - loss: 2.1268e-04 - accuracy: 1.0000 - val_loss: 0.6170 - val_accuracy: 0.8654\nEpoch 178/200\n130/130 [==============================] - 23s 175ms/step - loss: 3.2289e-04 - accuracy: 1.0000 - val_loss: 0.6387 - val_accuracy: 0.8567\nEpoch 179/200\n130/130 [==============================] - 23s 176ms/step - loss: 4.0704e-04 - accuracy: 1.0000 - val_loss: 0.6217 - val_accuracy: 0.8644\nEpoch 180/200\n130/130 [==============================] - 23s 175ms/step - loss: 2.9299e-04 - accuracy: 1.0000 - val_loss: 0.6052 - val_accuracy: 0.8692\nEpoch 181/200\n130/130 [==============================] - 22s 173ms/step - loss: 3.0555e-04 - accuracy: 1.0000 - val_loss: 0.5867 - val_accuracy: 0.8721\nEpoch 182/200\n130/130 [==============================] - 23s 174ms/step - loss: 2.9885e-04 - accuracy: 1.0000 - val_loss: 0.6193 - val_accuracy: 0.8587\nEpoch 183/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.6241 - val_accuracy: 0.8644\nEpoch 184/200\n130/130 [==============================] - 23s 175ms/step - loss: 3.1053e-04 - accuracy: 1.0000 - val_loss: 0.6058 - val_accuracy: 0.8683\nEpoch 185/200\n130/130 [==============================] - 23s 174ms/step - loss: 2.3716e-04 - accuracy: 1.0000 - val_loss: 0.6225 - val_accuracy: 0.8663\nEpoch 186/200\n130/130 [==============================] - 23s 175ms/step - loss: 2.6715e-04 - accuracy: 1.0000 - val_loss: 0.6016 - val_accuracy: 0.8683\nEpoch 187/200\n130/130 [==============================] - 23s 176ms/step - loss: 2.5621e-04 - accuracy: 1.0000 - val_loss: 0.6141 - val_accuracy: 0.8663\nEpoch 188/200\n130/130 [==============================] - 23s 173ms/step - loss: 2.7374e-04 - accuracy: 1.0000 - val_loss: 0.6088 - val_accuracy: 0.8635\nEpoch 189/200\n130/130 [==============================] - 23s 178ms/step - loss: 2.4371e-04 - accuracy: 1.0000 - val_loss: 0.5955 - val_accuracy: 0.8712\nEpoch 190/200\n130/130 [==============================] - 23s 175ms/step - loss: 3.9890e-04 - accuracy: 1.0000 - val_loss: 0.6412 - val_accuracy: 0.8519\nEpoch 191/200\n130/130 [==============================] - 23s 176ms/step - loss: 4.2232e-04 - accuracy: 1.0000 - val_loss: 0.6325 - val_accuracy: 0.8644\nEpoch 192/200\n130/130 [==============================] - 23s 175ms/step - loss: 2.1301e-04 - accuracy: 1.0000 - val_loss: 0.6088 - val_accuracy: 0.8615\nEpoch 193/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.4552e-04 - accuracy: 1.0000 - val_loss: 0.6053 - val_accuracy: 0.8644\nEpoch 194/200\n130/130 [==============================] - 23s 176ms/step - loss: 4.2575e-04 - accuracy: 1.0000 - val_loss: 0.6361 - val_accuracy: 0.8519\nEpoch 195/200\n130/130 [==============================] - 22s 172ms/step - loss: 2.1344e-04 - accuracy: 1.0000 - val_loss: 0.6115 - val_accuracy: 0.8615\nEpoch 196/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.6353 - val_accuracy: 0.8596\nEpoch 197/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.6367 - val_accuracy: 0.8644\nEpoch 198/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.4608e-04 - accuracy: 1.0000 - val_loss: 0.6245 - val_accuracy: 0.8712\nEpoch 199/200\n130/130 [==============================] - 23s 173ms/step - loss: 2.5404e-04 - accuracy: 1.0000 - val_loss: 0.6129 - val_accuracy: 0.8635\nEpoch 200/200\n130/130 [==============================] - 23s 176ms/step - loss: 1.2410e-04 - accuracy: 1.0000 - val_loss: 0.6101 - val_accuracy: 0.8702\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = []  # True labels for the test data\npredicted_labels = []  # Predicted labels for the test data\n\nfor batch_data, batch_labels in test_dataset:\n    batch_predictions = model.predict(batch_data)\n    batch_predicted_labels = np.argmax(batch_predictions, axis=1)\n    predicted_labels.extend(batch_predicted_labels)\n    true_labels.extend(np.argmax(batch_labels, axis=1))\n\ntrue_labels = np.array(true_labels)\npredicted_labels = np.array(predicted_labels)\n\naccuracy = np.mean(true_labels == predicted_labels)\nprint(true_labels)\nprint(predicted_labels)\nprint(accuracy)","metadata":{"papermill":{"duration":12.813259,"end_time":"2023-08-06T22:53:46.633707","exception":false,"start_time":"2023-08-06T22:53:33.820448","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T11:45:48.780811Z","iopub.execute_input":"2023-08-09T11:45:48.781171Z","iopub.status.idle":"2023-08-09T11:46:01.886529Z","shell.execute_reply.started":"2023-08-09T11:45:48.781138Z","shell.execute_reply":"2023-08-09T11:46:01.885291Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 71ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 59ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 59ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 110ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 58ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 171ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 71ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 59ms/step\n1/1 [==============================] - 0s 59ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 58ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 74ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 59ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 144ms/step\n1/1 [==============================] - 0s 75ms/step\n[ 8 32 19 ... 28 12 48]\n[ 8 32 19 ... 28 12 48]\n0.8701923076923077\n","output_type":"stream"}]}]}