{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de807d7c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-22T22:17:24.705909Z",
     "iopub.status.busy": "2023-07-22T22:17:24.705496Z",
     "iopub.status.idle": "2023-07-22T22:17:24.723932Z",
     "shell.execute_reply": "2023-07-22T22:17:24.722583Z"
    },
    "papermill": {
     "duration": 0.026457,
     "end_time": "2023-07-22T22:17:24.726022",
     "exception": false,
     "start_time": "2023-07-22T22:17:24.699565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/datasetfornina/ninaprodb1train.pkl\n",
      "/kaggle/input/datasetfornina/ninaprodb1test.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4050f2d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T22:17:24.736104Z",
     "iopub.status.busy": "2023-07-22T22:17:24.735332Z",
     "iopub.status.idle": "2023-07-22T22:17:35.156613Z",
     "shell.execute_reply": "2023-07-22T22:17:35.155648Z"
    },
    "papermill": {
     "duration": 10.428868,
     "end_time": "2023-07-22T22:17:35.159145",
     "exception": false,
     "start_time": "2023-07-22T22:17:24.730277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217a8854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T22:17:35.170652Z",
     "iopub.status.busy": "2023-07-22T22:17:35.169494Z",
     "iopub.status.idle": "2023-07-22T22:17:39.716040Z",
     "shell.execute_reply": "2023-07-22T22:17:39.714996Z"
    },
    "papermill": {
     "duration": 4.555298,
     "end_time": "2023-07-22T22:17:39.718792",
     "exception": false,
     "start_time": "2023-07-22T22:17:35.163494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the dataset from pickle file\n",
    "with open('/kaggle/input/datasetfornina/ninaprodb1test.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "with open('/kaggle/input/datasetfornina/ninaprodb1train.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed955da1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T22:17:39.728361Z",
     "iopub.status.busy": "2023-07-22T22:17:39.728060Z",
     "iopub.status.idle": "2023-07-22T22:17:40.799940Z",
     "shell.execute_reply": "2023-07-22T22:17:40.798676Z"
    },
    "papermill": {
     "duration": 1.080364,
     "end_time": "2023-07-22T22:17:40.803452",
     "exception": false,
     "start_time": "2023-07-22T22:17:39.723088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e175f332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T22:17:40.813465Z",
     "iopub.status.busy": "2023-07-22T22:17:40.813167Z",
     "iopub.status.idle": "2023-07-22T22:17:40.817800Z",
     "shell.execute_reply": "2023-07-22T22:17:40.816614Z"
    },
    "papermill": {
     "duration": 0.012104,
     "end_time": "2023-07-22T22:17:40.820198",
     "exception": false,
     "start_time": "2023-07-22T22:17:40.808094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential, optimizers, Input, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dec734",
   "metadata": {
    "papermill": {
     "duration": 0.004006,
     "end_time": "2023-07-22T22:17:40.828286",
     "exception": false,
     "start_time": "2023-07-22T22:17:40.824280",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**WORKED CODE HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71bef045",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T22:17:40.837285Z",
     "iopub.status.busy": "2023-07-22T22:17:40.837018Z",
     "iopub.status.idle": "2023-07-22T22:17:40.843903Z",
     "shell.execute_reply": "2023-07-22T22:17:40.843041Z"
    },
    "papermill": {
     "duration": 0.013657,
     "end_time": "2023-07-22T22:17:40.845956",
     "exception": false,
     "start_time": "2023-07-22T22:17:40.832299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96fe01eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T22:17:40.854889Z",
     "iopub.status.busy": "2023-07-22T22:17:40.854596Z",
     "iopub.status.idle": "2023-07-22T22:17:40.859277Z",
     "shell.execute_reply": "2023-07-22T22:17:40.858262Z"
    },
    "papermill": {
     "duration": 0.011649,
     "end_time": "2023-07-22T22:17:40.861491",
     "exception": false,
     "start_time": "2023-07-22T22:17:40.849842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 64\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "method = \"default\"\n",
    "dataset_type = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4db2545",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T22:17:40.870650Z",
     "iopub.status.busy": "2023-07-22T22:17:40.870373Z",
     "iopub.status.idle": "2023-07-22T22:17:43.467198Z",
     "shell.execute_reply": "2023-07-22T22:17:43.466166Z"
    },
    "papermill": {
     "duration": 2.604508,
     "end_time": "2023-07-22T22:17:43.469830",
     "exception": false,
     "start_time": "2023-07-22T22:17:40.865322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Nina1Dataset object at 0x7a33e0b93f10>\n"
     ]
    }
   ],
   "source": [
    "### import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import pywt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "class Nina1Dataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataframe, batch_size):\n",
    "        self.dataframe = dataframe\n",
    "        self.batch_size = batch_size\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(np.concatenate(self.dataframe['emg'].tolist()))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataframe) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_data = self.dataframe[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        batch_input_data = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for i, target_row in batch_data.iterrows():\n",
    "            data = target_row['emg'][:500]\n",
    "\n",
    "            # Zero-Padding\n",
    "            if len(data) < 500:\n",
    "                data = np.concatenate((data, np.zeros((500 - len(data), 10))), axis=0)\n",
    "            coeffs = pywt.wavedec(data, 'sym8')  # Using 3 decomposition levels as an example\n",
    "            coeffs[1:] = (pywt.threshold(c, value=0.5, mode='soft') for c in coeffs[1:])\n",
    "            data = pywt.waverec(coeffs, 'sym8')\n",
    "            data = self.scaler.transform(data)\n",
    "            \n",
    "            # Division data by time-segment\n",
    "            #input_data = np.transpose(data.reshape((25, 20, 10)), (0, 2, 1))\n",
    "            input_data = data.reshape((25, 20, 10))\n",
    "            label = target_row['stimulus']\n",
    "            #label = to_categorical(label)\n",
    "            batch_input_data.append(input_data)\n",
    "            batch_labels.append(label)\n",
    "\n",
    "        # Check if the batch size is smaller than the desired batch_size\n",
    "        if len(batch_data) < self.batch_size:\n",
    "            # Create a dummy batch with all elements set to zero\n",
    "            dummy_input_data = np.zeros((self.batch_size,) + input_data.shape, dtype=np.float32)\n",
    "            dummy_labels = np.zeros((self.batch_size,), dtype=np.int32)\n",
    "            dummy_input_data[:len(batch_input_data)] = np.array(batch_input_data)\n",
    "            dummy_labels[:len(batch_labels)] = np.array(batch_labels)\n",
    "            dummy_labels = to_categorical(dummy_labels,num_classes=52)  # Convert labels to one-hot encoding\n",
    "            \n",
    "            return dummy_input_data, dummy_labels\n",
    "        \n",
    "        batch_labels = to_categorical(batch_labels,num_classes=52)  # Convert labels to one-hot encoding\n",
    "        \n",
    "        return np.array(batch_input_data), np.array(batch_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "batch_size = 16\n",
    "train_dir = '/kaggle/input/datasetfornina/ninaprodb1train.pkl'\n",
    "test_dir = '/kaggle/input/datasetfornina/ninaprodb1test.pkl'\n",
    "\n",
    "# Set up dataset\n",
    "\n",
    "train = pd.read_pickle(train_dir)\n",
    "eval_data = pd.read_pickle(test_dir)\n",
    "\n",
    "# Load train data\n",
    "train_data = pd.read_pickle(train_dir)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "#train_data, val_data = train_test_split(train_data, test_size=0.3, random_state=21)\n",
    "\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "#val_data = val_data.sample(frac=1).reset_index(drop=True)\n",
    "eval_data = eval_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Create train, test and validation datasets\n",
    "train_dataset = Nina1Dataset(train_data, batch_size=batch_size)\n",
    "#val_dataset= Nina1Dataset(val_data, batch_size=batch_size)\n",
    "test_dataset= Nina1Dataset(eval_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7badc5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T22:17:43.480134Z",
     "iopub.status.busy": "2023-07-22T22:17:43.479848Z",
     "iopub.status.idle": "2023-07-22T23:13:23.133860Z",
     "shell.execute_reply": "2023-07-22T23:13:23.132744Z"
    },
    "papermill": {
     "duration": 3339.661871,
     "end_time": "2023-07-22T23:13:23.136188",
     "exception": false,
     "start_time": "2023-07-22T22:17:43.474317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 25, 20, 10)\n",
      "(None, 25, 10, 64)\n",
      "(None, 25, 2, 64)\n",
      "(None, 25, 1, 64)\n",
      "(None, 25, 64)\n",
      "(None, 25, 64)\n",
      "11\n",
      "(None, 25, 400)\n",
      "11\n",
      "(None, 25, 400)\n",
      "11\n",
      "Epoch 1/200\n",
      "615/615 [==============================] - 36s 23ms/step - loss: 2.9277 - accuracy: 0.2467\n",
      "Epoch 2/200\n",
      "615/615 [==============================] - 14s 24ms/step - loss: 1.9706 - accuracy: 0.4410\n",
      "Epoch 3/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.4091 - accuracy: 0.5762\n",
      "Epoch 4/200\n",
      "615/615 [==============================] - 15s 25ms/step - loss: 0.9528 - accuracy: 0.7052\n",
      "Epoch 5/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 0.5959 - accuracy: 0.8105\n",
      "Epoch 6/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.3657 - accuracy: 0.8821\n",
      "Epoch 7/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.2315 - accuracy: 0.9303\n",
      "Epoch 8/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.1498 - accuracy: 0.9567\n",
      "Epoch 9/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.1019 - accuracy: 0.9725\n",
      "Epoch 10/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 0.1019 - accuracy: 0.9709\n",
      "Epoch 11/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.0620 - accuracy: 0.9836\n",
      "Epoch 12/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.0603 - accuracy: 0.9846\n",
      "Epoch 13/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 0.0499 - accuracy: 0.9870\n",
      "Epoch 14/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.0452 - accuracy: 0.9862\n",
      "Epoch 15/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 0.0354 - accuracy: 0.9926\n",
      "Epoch 16/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.0286 - accuracy: 0.9925\n",
      "Epoch 17/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 0.0302 - accuracy: 0.9916\n",
      "Epoch 18/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 0.0227 - accuracy: 0.9950\n",
      "Epoch 19/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 0.0213 - accuracy: 0.9946\n",
      "Epoch 20/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 21/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 0.0071 - accuracy: 0.9982\n",
      "Epoch 22/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.0083 - accuracy: 0.9987\n",
      "Epoch 23/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.0055 - accuracy: 0.9992\n",
      "Epoch 24/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.0085 - accuracy: 0.9984\n",
      "Epoch 25/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 0.0065 - accuracy: 0.9989\n",
      "Epoch 26/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 0.0029 - accuracy: 0.9996\n",
      "Epoch 27/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 0.0094 - accuracy: 0.9971\n",
      "Epoch 28/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 0.0021 - accuracy: 0.9998\n",
      "Epoch 29/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 30/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 8.0341e-04 - accuracy: 1.0000\n",
      "Epoch 31/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 0.0035 - accuracy: 0.9994\n",
      "Epoch 32/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.0057 - accuracy: 0.9987\n",
      "Epoch 33/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 7.6424e-04 - accuracy: 0.9999\n",
      "Epoch 34/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 35/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 8.4901e-04 - accuracy: 0.9999\n",
      "Epoch 36/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 5.9841e-04 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 3.2244e-04 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 2.8804e-04 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 2.4646e-04 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 6.7494e-04 - accuracy: 0.9999\n",
      "Epoch 41/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 42/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 4.0340e-04 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.7365e-04 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.5061e-04 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.3028e-04 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.2237e-04 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0401e-04 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 9.7126e-05 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 9.3706e-05 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 8.4035e-05 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 8.2876e-05 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 7.8397e-05 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 6.2892e-05 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 5.2363e-05 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 5.4804e-05 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 5.2885e-05 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 4.7532e-05 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 4.9853e-05 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 3.6942e-05 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 3.9434e-05 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 3.4505e-05 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 3.0130e-05 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 3.1945e-05 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 2.4849e-05 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 2.6352e-05 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 2.3109e-05 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 2.4769e-05 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 2.0498e-05 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 2.2053e-05 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 2.0744e-05 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.8465e-05 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.8847e-05 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 2.2821e-05 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.7989e-05 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.7378e-05 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.5268e-05 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.6638e-05 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.5198e-05 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "615/615 [==============================] - 15s 25ms/step - loss: 1.7071e-05 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.6695e-05 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.3191e-05 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.4108e-05 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.3326e-05 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.3537e-05 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.4150e-05 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.3062e-05 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.3809e-05 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.3088e-05 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.2782e-05 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.3181e-05 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.3236e-05 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.1191e-05 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.1319e-05 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.2223e-05 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.1948e-05 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.3302e-05 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.1164e-05 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.2065e-05 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.3153e-05 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "615/615 [==============================] - 13s 21ms/step - loss: 1.2311e-05 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.1548e-05 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0081e-05 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.1659e-05 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.1201e-05 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0443e-05 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.1271e-05 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.1783e-05 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.0035e-05 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.2215e-05 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "615/615 [==============================] - 13s 21ms/step - loss: 1.0666e-05 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 9.7106e-06 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.1896e-05 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.1379e-05 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0216e-05 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.0888e-05 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 9.9561e-06 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0416e-05 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0636e-05 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.9664e-05 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.0725e-05 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.1809e-05 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0521e-05 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.1261e-05 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.1120e-05 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0491e-05 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.1214e-05 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "615/615 [==============================] - 15s 25ms/step - loss: 1.0746e-05 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.0023e-05 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.1641e-05 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.0730e-05 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 9.8318e-06 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0459e-05 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 9.8266e-06 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0444e-05 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 9.5972e-06 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0452e-05 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.2030e-05 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "615/615 [==============================] - 14s 24ms/step - loss: 1.0738e-05 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0243e-05 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0224e-05 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.0325e-05 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "615/615 [==============================] - 15s 25ms/step - loss: 1.0587e-05 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.1316e-05 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0782e-05 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 9.7453e-06 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0895e-05 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.1460e-05 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "615/615 [==============================] - 14s 24ms/step - loss: 1.0546e-05 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.0382e-05 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "615/615 [==============================] - 13s 21ms/step - loss: 1.0269e-05 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0809e-05 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.1410e-05 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "615/615 [==============================] - 15s 25ms/step - loss: 1.0451e-05 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "615/615 [==============================] - 13s 21ms/step - loss: 1.0723e-05 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.0557e-05 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "615/615 [==============================] - 15s 25ms/step - loss: 1.0379e-05 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.0383e-05 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "615/615 [==============================] - 15s 25ms/step - loss: 1.0791e-05 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 9.6879e-06 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0470e-05 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 9.8552e-06 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0349e-05 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0661e-05 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.0244e-05 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 9.7426e-06 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0924e-05 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0375e-05 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0052e-05 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "615/615 [==============================] - 13s 21ms/step - loss: 9.9407e-06 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "615/615 [==============================] - 15s 25ms/step - loss: 1.0574e-05 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 9.8845e-06 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0768e-05 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.1236e-05 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0091e-05 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0255e-05 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0904e-05 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.2484e-05 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.0010e-05 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.0469e-05 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.0036e-05 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "615/615 [==============================] - 16s 25ms/step - loss: 1.0302e-05 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.2684e-05 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.0398e-05 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "615/615 [==============================] - 13s 21ms/step - loss: 1.0374e-05 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "615/615 [==============================] - 15s 25ms/step - loss: 1.0359e-05 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.0302e-05 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.0618e-05 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 1.1277e-05 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.0564e-05 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.0266e-05 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 1.0436e-05 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.1241e-05 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.0548e-05 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "615/615 [==============================] - 13s 21ms/step - loss: 1.1629e-05 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "615/615 [==============================] - 13s 21ms/step - loss: 1.0483e-05 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.1457e-05 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "615/615 [==============================] - 15s 24ms/step - loss: 1.1182e-05 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "615/615 [==============================] - 14s 22ms/step - loss: 1.1335e-05 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "615/615 [==============================] - 13s 22ms/step - loss: 9.8731e-06 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "615/615 [==============================] - 14s 23ms/step - loss: 9.4424e-06 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Define the CNN-BiLSTM model\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\n",
    "from tensorflow.keras.activations import tanh\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "\n",
    "def cnn(x):\n",
    "    #print(x.shape)\n",
    "    #x = Reshape((25, 20,10))(x)  \n",
    "    print(x.shape)\n",
    "    x = TimeDistributed((Conv1D(64, kernel_size=9, strides=2, padding='same', activation=tanh)))(x)\n",
    "    print(x.shape)\n",
    "    #(Batch, 10, 64)\n",
    "\n",
    "    x = TimeDistributed(MaxPooling1D(pool_size=8, strides=2))(x)\n",
    "    print(x.shape)\n",
    "    #(Batch, 2, 64)\n",
    "    \n",
    "    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n",
    "    #(Batch, 1, 64)\n",
    "    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n",
    "    \n",
    "    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n",
    "    print(x.shape)\n",
    "    #(Batch, 1, 64)\n",
    "    #x = TimeDistributed(Dropout(0.2093))(x)\n",
    "    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n",
    "    \n",
    "    #x = TimeDistributed(ReLU())(Conv1D(64, kernel_size=3, strides=2, padding='same')(x))\n",
    "    x = TimeDistributed((Conv1D(64, kernel_size=3, strides=2, padding='same', activation=tanh)))(x)\n",
    "    #print(x.shape)\n",
    "    #(Batch, 1, 64)\n",
    "    #x = TimeDistributed(Dropout(0.2093))(x)\n",
    "    #x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n",
    "    x = TimeDistributed(Flatten())(x)\n",
    "    print(x.shape)\n",
    "    # (Batch, 64)\n",
    "\n",
    "    return x\n",
    "\n",
    "def Bi_LSTMModel(input_shape,x):\n",
    "    #model = Sequential()\n",
    "    # Hidden dimensions\n",
    "    hidden_dim = 200\n",
    "    print(x.shape)\n",
    "    print(11)\n",
    "    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.2093), input_shape=input_shape)(x)\n",
    "    #x = Dropout(0.2093)(x)\n",
    "    print(x.shape)\n",
    "    print(11)\n",
    "    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.2093))(x)\n",
    "\n",
    "    print(x.shape)\n",
    "    print(11)\n",
    "    #x = Dropout(0.2093)(x)\n",
    "    x = Flatten()(x)\n",
    "    # ( ,10000)\n",
    "\n",
    "    return x\n",
    "def EMGHandNet(input_shape, num_classes):\n",
    "    # Define the input layer\n",
    "    x = Input(shape=input_shape)\n",
    "    inputs = x\n",
    "    #print(x.shape)\n",
    "    #(batch, 25, 20, 10)\n",
    "    #temp = [cnn(x[:, t, :, :]) for t in range(x.shape[1])]\n",
    "    #x = tf.stack(temp, axis=1)\n",
    "    #print(x.shape)\n",
    "    x = cnn(x)\n",
    "   \n",
    "    #print(x.shape)\n",
    "    x = Bi_LSTMModel(x.shape[1:],x)\n",
    "    #print(x.shape)\n",
    "    #x = Dropout(0.2093)(x)\n",
    "    \n",
    "    x = Dense(512, activation='tanh')(x)\n",
    "    #print(x.shape)\n",
    "    x = Dropout(0.2093)(x)\n",
    "\n",
    "    # Add the output layer\n",
    "    output_layer = Dense(52, activation='softmax')(x)\n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "num_classes = 52  # Adjust based on the number of hand activity classes\n",
    "\n",
    "\n",
    "# Create an instance of the EMGHandNet model\n",
    "model = EMGHandNet((25,20,10), 52)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "#dtype = tf.float32\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 1000\n",
    "decay_rate = 0.9\n",
    "batch_size = 16\n",
    "# Define your model and its optimizer\n",
    "# Define learning rate schedule\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "    # Compile the model with learning rate schedule\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999),\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=200,\n",
    "                    batch_size=16\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc61c2a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-22T23:13:30.213063Z",
     "iopub.status.busy": "2023-07-22T23:13:30.212704Z",
     "iopub.status.idle": "2023-07-22T23:13:49.579543Z",
     "shell.execute_reply": "2023-07-22T23:13:49.578658Z"
    },
    "papermill": {
     "duration": 22.94182,
     "end_time": "2023-07-22T23:13:49.581594",
     "exception": false,
     "start_time": "2023-07-22T23:13:26.639774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "0.9353693181818182\n"
     ]
    }
   ],
   "source": [
    "true_labels = []  # True labels for the test data\n",
    "predicted_labels = []  # Predicted labels for the test data\n",
    "\n",
    "for batch_data, batch_labels in test_dataset:\n",
    "    batch_predictions = model.predict(batch_data)\n",
    "    batch_predicted_labels = np.argmax(batch_predictions, axis=1)\n",
    "    predicted_labels.extend(batch_predicted_labels)\n",
    "    true_labels.extend(np.argmax(batch_labels, axis=1))\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "accuracy = np.mean(true_labels == predicted_labels)\n",
    "\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3403.333726,
   "end_time": "2023-07-22T23:13:56.491040",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-22T22:17:13.157314",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
