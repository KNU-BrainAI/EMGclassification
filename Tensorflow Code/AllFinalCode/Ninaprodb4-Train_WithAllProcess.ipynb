{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":2720.239413,"end_time":"2023-07-24T13:02:44.064785","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-07-24T12:17:23.825372","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.032572,"end_time":"2023-07-24T12:17:38.078235","exception":false,"start_time":"2023-07-24T12:17:38.045663","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-29T17:32:30.542414Z","iopub.execute_input":"2023-07-29T17:32:30.542978Z","iopub.status.idle":"2023-07-29T17:32:30.560325Z","shell.execute_reply.started":"2023-07-29T17:32:30.542936Z","shell.execute_reply":"2023-07-29T17:32:30.559346Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/newninaprodb4/ninaprodb44test.pkl\n/kaggle/input/newninaprodb4/ninaprodb44train.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\n#from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport math","metadata":{"papermill":{"duration":16.610033,"end_time":"2023-07-24T12:17:54.692511","exception":false,"start_time":"2023-07-24T12:17:38.082478","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-29T17:32:30.562477Z","iopub.execute_input":"2023-07-29T17:32:30.562971Z","iopub.status.idle":"2023-07-29T17:32:39.862193Z","shell.execute_reply.started":"2023-07-29T17:32:30.562938Z","shell.execute_reply":"2023-07-29T17:32:39.861108Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"papermill":{"duration":1.569552,"end_time":"2023-07-24T12:17:56.266778","exception":false,"start_time":"2023-07-24T12:17:54.697226","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-29T17:32:39.863954Z","iopub.execute_input":"2023-07-29T17:32:39.864817Z","iopub.status.idle":"2023-07-29T17:32:40.903697Z","shell.execute_reply.started":"2023-07-29T17:32:39.864778Z","shell.execute_reply":"2023-07-29T17:32:40.902701Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers, Sequential, optimizers, Input, Model","metadata":{"papermill":{"duration":0.011879,"end_time":"2023-07-24T12:17:56.283281","exception":false,"start_time":"2023-07-24T12:17:56.271402","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-29T17:32:40.906298Z","iopub.execute_input":"2023-07-29T17:32:40.906665Z","iopub.status.idle":"2023-07-29T17:32:40.911577Z","shell.execute_reply.started":"2023-07-29T17:32:40.906613Z","shell.execute_reply":"2023-07-29T17:32:40.910664Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**WORKED CODE HERE**","metadata":{"papermill":{"duration":0.003681,"end_time":"2023-07-24T12:17:56.290873","exception":false,"start_time":"2023-07-24T12:17:56.287192","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\nimport gc\nimport random\nimport os\n\nimport matplotlib.pyplot as plt\nimport json","metadata":{"papermill":{"duration":0.014563,"end_time":"2023-07-24T12:17:56.309522","exception":false,"start_time":"2023-07-24T12:17:56.294959","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-29T17:32:40.913032Z","iopub.execute_input":"2023-07-29T17:32:40.913618Z","iopub.status.idle":"2023-07-29T17:32:40.924565Z","shell.execute_reply.started":"2023-07-29T17:32:40.913585Z","shell.execute_reply":"2023-07-29T17:32:40.923687Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"epochs = 64\nlearning_rate = 1e-3\nbatch_size = 16\nmethod = \"default\"\ndataset_type = 1","metadata":{"papermill":{"duration":0.011176,"end_time":"2023-07-24T12:17:56.324703","exception":false,"start_time":"2023-07-24T12:17:56.313527","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-29T17:32:40.927537Z","iopub.execute_input":"2023-07-29T17:32:40.927803Z","iopub.status.idle":"2023-07-29T17:32:40.936736Z","shell.execute_reply.started":"2023-07-29T17:32:40.927780Z","shell.execute_reply":"2023-07-29T17:32:40.935857Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pywt  # Added import for wavelet denoising\nimport scipy.signal as signal  # Added import for power line noise and low pass filtering\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nimport pywt\nimport scipy.signal as signal\nimport tensorflow as tf\nclass Nina1Dataset(tf.keras.utils.Sequence):\n    def __init__(self, dataframe, batch_size):\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.scaler = StandardScaler()\n        self.scaler.fit(np.concatenate(self.dataframe['emg'].tolist()))\n        \n        \n    def __len__(self):\n        return int(np.ceil(len(self.dataframe) / self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_data = self.dataframe[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        batch_input_data = []\n        batch_labels = []\n\n        for i, target_row in batch_data.iterrows():\n            data = target_row['emg'][:10000]\n\n            # Zero-Padding\n            if len(data) < 10000:\n                data = np.concatenate((data, np.zeros((10000 - len(data), 12))), axis=0)\n\n            \n            if(target_row['stimulus'] != 52):\n                fs = 2000  # Assuming sampling frequency of 2000 Hz\n                f0 = 50  # Power line frequency\n                Q = 30  # Quality factor\n                w0 = f0 / (fs / 2)\n                b, a = signal.iirnotch(w0, Q)\n                data = signal.lfilter(b, a, data, axis=0)\n\n                    # Low Pass Filtering with cutoff frequency fc = 500 Hz\n                fc = 500  # Cutoff frequency\n                b, a = signal.butter(4, fc / (fs / 2), 'low')\n                data = signal.lfilter(b, a, data, axis=0)\n\n\n                coeffs = pywt.wavedec(data, 'sym8')  \n                coeffs[1:] = (pywt.threshold(c, value=0.5, mode='soft') for c in coeffs[1:])\n                data = pywt.waverec(coeffs, 'sym8')\n\n\n                # Wavelet Denoising\n\n\n\n\n                    # Standardize the data\n                data = self.scaler.transform(data)\n\n                    # Division data by time-segment (channel-wise)\n                input_data = data.reshape((25, 400, 12))\n                label = target_row['stimulus']\n                batch_input_data.append(input_data)\n                batch_labels.append(label)\n\n        # Check if the batch size is smaller than the desired batch_size\n        if len(batch_data) < self.batch_size:\n            # Create a dummy batch with all elements set to zero\n            dummy_input_data = np.zeros((self.batch_size,) + input_data.shape, dtype=np.float32)\n            dummy_labels = np.zeros((self.batch_size,), dtype=np.int32)\n            dummy_input_data[:len(batch_input_data)] = np.array(batch_input_data)\n            dummy_labels[:len(batch_labels)] = np.array(batch_labels)\n            dummy_labels = to_categorical(dummy_labels, num_classes=52)  # Convert labels to one-hot encoding\n\n            return dummy_input_data, dummy_labels\n\n        batch_labels = to_categorical(batch_labels, num_classes=52)  # Convert labels to one-hot encoding\n\n        return np.array(batch_input_data), np.array(batch_labels)\n\n# Parameters\n\nbatch_size = 16\ntrain_dir = '/kaggle/input/newninaprodb4/ninaprodb44train.pkl'\ntest_dir = '/kaggle/input/newninaprodb4/ninaprodb44test.pkl'\n\n# Set up dataset\ntrain = pd.read_pickle(train_dir)\neval_data = pd.read_pickle(test_dir)\n\n# Load train data\ntrain_data = pd.read_pickle(train_dir)\n\n# Shuffle data\ntrain_data = train_data.sample(frac=1).reset_index(drop=True)\neval_data = eval_data.sample(frac=1).reset_index(drop=True)\n#train_data, val_data = train_test_split(train_data, test_size=0.3, random_state=21)\n# Create train and test datasets\ntrain_dataset = Nina1Dataset(train_data, batch_size=batch_size)\n#val_dataset = Nina1Dataset(val_data, batch_size=batch_size)\ntest_dataset = Nina1Dataset(eval_data, batch_size=batch_size)\n\nprint(train_dataset)\n","metadata":{"papermill":{"duration":24.125294,"end_time":"2023-07-24T12:18:20.453839","exception":false,"start_time":"2023-07-24T12:17:56.328545","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-29T17:32:40.937893Z","iopub.execute_input":"2023-07-29T17:32:40.938152Z","iopub.status.idle":"2023-07-29T17:33:02.255203Z","shell.execute_reply.started":"2023-07-29T17:32:40.938123Z","shell.execute_reply":"2023-07-29T17:33:02.254124Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<__main__.Nina1Dataset object at 0x7cb2079c01c0>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_data['stimulus'])","metadata":{"papermill":{"duration":0.017808,"end_time":"2023-07-24T12:18:20.475976","exception":false,"start_time":"2023-07-24T12:18:20.458168","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-29T17:33:02.256887Z","iopub.execute_input":"2023-07-29T17:33:02.257681Z","iopub.status.idle":"2023-07-29T17:33:02.266947Z","shell.execute_reply.started":"2023-07-29T17:33:02.257617Z","shell.execute_reply":"2023-07-29T17:33:02.265873Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"0        3\n1       37\n2        3\n3       40\n4       44\n        ..\n2045    41\n2046    30\n2047    44\n2048    31\n2049    38\nName: stimulus, Length: 2050, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras import regularizers, initializers\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\n# Define the CNN-BiLSTM model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.layers import TimeDistributed\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\n# Define the CNN-BiLSTM model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras import regularizers, initializers\ndef cnn(x):\n    #print(x.shape)\n    #x = Reshape((25, 20,10))(x)  \n    print(x.shape)\n    x = TimeDistributed((Conv1D(64, kernel_size=9, strides=2, padding='same', activation=tanh)))(x)\n    print(x.shape)\n    #(Batch, 10, 64)\n    #x = TimeDistributed(Dropout(0.2093))(x)\n    #x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    \n    x = TimeDistributed(MaxPooling1D(pool_size=8, strides=2))(x)\n    print(x.shape)\n    #(Batch, 2, 64)\n    x = TimeDistributed(Dropout(0.2093))(x)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    \n    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n    #(Batch, 1, 64)\n    x = TimeDistributed(Dropout(0.2093))(x)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    \n    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n    print(x.shape)\n    #(Batch, 1, 64)\n    x = TimeDistributed(Dropout(0.2093))(x)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    \n    x = TimeDistributed((Conv1D(64, kernel_size=3, strides=2, padding='same', activation=tanh)))(x)\n    #print(x.shape)\n    #(Batch, 1, 64)\n    x = TimeDistributed(Dropout(0.2093))(x)\n    #x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(Flatten())(x)\n    print(x.shape)\n    # (Batch, 64)\n\n    return x\n\ndef Bi_LSTMModel(input_shape,x):\n    #model = Sequential()\n    # Hidden dimensions\n    hidden_dim = 200\n    print(x.shape)\n    print(11)\n    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.2093), input_shape=input_shape)(x)\n    #x = Dropout(0.2093)(x)\n    print(x.shape)\n    print(11)\n    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.2093))(x)\n\n    print(x.shape)\n    print(11)\n    #x = Dropout(0.2093)(x)\n    x = Flatten()(x)\n    # ( ,10000)\n\n    return x\ndef EMGHandNet(input_shape, num_classes):\n    # Define the input layer\n    x = Input(shape=input_shape)\n    inputs = x\n    #print(x.shape)\n    #(batch, 25, 20, 10)\n    #temp = [cnn(x[:, t, :, :]) for t in range(x.shape[1])]\n    #x = tf.stack(temp, axis=1)\n    #print(x.shape)\n    x = cnn(x)\n   \n    #print(x.shape)\n    x = Bi_LSTMModel(x.shape[1:],x)\n    #print(x.shape)\n    #x = Dropout(0.2093)(x)\n    \n    x = Dense(512, activation='tanh')(x)\n    #print(x.shape)\n    x = Dropout(0.2093)(x)\n    #x = BatchNormalization(epsilon=1e-6, momentum=0.95)(x)\n\n    # Add the output layer\n    output_layer = Dense(52, activation='softmax')(x)\n    # Create the model\n    model = Model(inputs=inputs, outputs=output_layer)\n\n    return model\n\nnum_classes = 52\nmodel = EMGHandNet((25, 400, 12), num_classes)\n\n\ninitial_learning_rate = 0.001\ndecay_steps = 1000\ndecay_rate = 0.9\nbatch_size = 16\n# Define your model and its optimizer\n# Define learning rate schedule\nlr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n\n    # Compile the model with learning rate schedule\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999),\n                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n                metrics=['accuracy'])\n\n\n# Define your model and its optimizer\n# Define learning rate schedule\n\n\nhistory = model.fit(train_dataset,\n                    epochs=200,\n                    batch_size=16)\n\n\n","metadata":{"papermill":{"duration":2641.952916,"end_time":"2023-07-24T13:02:22.432907","exception":false,"start_time":"2023-07-24T12:18:20.479991","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-29T19:41:06.027142Z","iopub.execute_input":"2023-07-29T19:41:06.027504Z","iopub.status.idle":"2023-07-29T20:20:02.638818Z","shell.execute_reply.started":"2023-07-29T19:41:06.027471Z","shell.execute_reply":"2023-07-29T20:20:02.637766Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(None, 25, 400, 12)\n(None, 25, 200, 64)\n(None, 25, 97, 64)\n(None, 25, 25, 64)\n(None, 25, 832)\n(None, 25, 832)\n11\n(None, 25, 400)\n11\n(None, 25, 400)\n11\nEpoch 1/200\n129/129 [==============================] - 22s 83ms/step - loss: 3.9540 - accuracy: 0.0732\nEpoch 2/200\n129/129 [==============================] - 10s 79ms/step - loss: 3.1209 - accuracy: 0.1860\nEpoch 3/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.4591 - accuracy: 0.3028\nEpoch 4/200\n129/129 [==============================] - 11s 82ms/step - loss: 2.1159 - accuracy: 0.3784\nEpoch 5/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.7090 - accuracy: 0.4719\nEpoch 6/200\n129/129 [==============================] - 10s 80ms/step - loss: 1.4326 - accuracy: 0.5509\nEpoch 7/200\n129/129 [==============================] - 11s 83ms/step - loss: 1.2871 - accuracy: 0.5940\nEpoch 8/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.0331 - accuracy: 0.6730\nEpoch 9/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.9115 - accuracy: 0.7020\nEpoch 10/200\n129/129 [==============================] - 11s 84ms/step - loss: 0.6335 - accuracy: 0.7955\nEpoch 11/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.4592 - accuracy: 0.8648\nEpoch 12/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.3438 - accuracy: 0.8944\nEpoch 13/200\n129/129 [==============================] - 11s 83ms/step - loss: 0.2652 - accuracy: 0.9191\nEpoch 14/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.1863 - accuracy: 0.9482\nEpoch 15/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.1351 - accuracy: 0.9695\nEpoch 16/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0963 - accuracy: 0.9758\nEpoch 17/200\n129/129 [==============================] - 10s 77ms/step - loss: 0.0522 - accuracy: 0.9932\nEpoch 18/200\n129/129 [==============================] - 11s 84ms/step - loss: 0.0391 - accuracy: 0.9952\nEpoch 19/200\n129/129 [==============================] - 11s 82ms/step - loss: 0.0446 - accuracy: 0.9908\nEpoch 20/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0314 - accuracy: 0.9952\nEpoch 21/200\n129/129 [==============================] - 11s 83ms/step - loss: 0.0164 - accuracy: 0.9990\nEpoch 22/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0120 - accuracy: 0.9995\nEpoch 23/200\n129/129 [==============================] - 11s 81ms/step - loss: 0.0079 - accuracy: 1.0000\nEpoch 24/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0061 - accuracy: 1.0000\nEpoch 25/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0678 - accuracy: 0.9840\nEpoch 26/200\n129/129 [==============================] - 11s 83ms/step - loss: 0.1793 - accuracy: 0.9477\nEpoch 27/200\n129/129 [==============================] - 10s 81ms/step - loss: 0.4072 - accuracy: 0.8721\nEpoch 28/200\n129/129 [==============================] - 11s 83ms/step - loss: 0.2332 - accuracy: 0.9264\nEpoch 29/200\n129/129 [==============================] - 10s 78ms/step - loss: 0.0924 - accuracy: 0.9792\nEpoch 30/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0485 - accuracy: 0.9893\nEpoch 31/200\n129/129 [==============================] - 11s 87ms/step - loss: 0.0265 - accuracy: 0.9976\nEpoch 32/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0170 - accuracy: 0.9981\nEpoch 33/200\n129/129 [==============================] - 10s 78ms/step - loss: 0.0309 - accuracy: 0.9927\nEpoch 34/200\n129/129 [==============================] - 11s 82ms/step - loss: 0.0214 - accuracy: 0.9956\nEpoch 35/200\n129/129 [==============================] - 11s 82ms/step - loss: 0.0105 - accuracy: 0.9990\nEpoch 36/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0048 - accuracy: 1.0000\nEpoch 37/200\n129/129 [==============================] - 10s 81ms/step - loss: 0.0057 - accuracy: 1.0000\nEpoch 38/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0042 - accuracy: 1.0000\nEpoch 39/200\n129/129 [==============================] - 11s 82ms/step - loss: 0.0028 - accuracy: 1.0000\nEpoch 40/200\n129/129 [==============================] - 11s 84ms/step - loss: 0.0023 - accuracy: 1.0000\nEpoch 41/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0019 - accuracy: 1.0000\nEpoch 42/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0019 - accuracy: 1.0000\nEpoch 43/200\n129/129 [==============================] - 11s 86ms/step - loss: 0.0048 - accuracy: 0.9990\nEpoch 44/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0131 - accuracy: 0.9966\nEpoch 45/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0799 - accuracy: 0.9772\nEpoch 46/200\n129/129 [==============================] - 11s 85ms/step - loss: 0.2646 - accuracy: 0.9176\nEpoch 47/200\n129/129 [==============================] - 11s 82ms/step - loss: 0.1859 - accuracy: 0.9404\nEpoch 48/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0986 - accuracy: 0.9680\nEpoch 49/200\n129/129 [==============================] - 11s 85ms/step - loss: 0.0292 - accuracy: 0.9947\nEpoch 50/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0221 - accuracy: 0.9966\nEpoch 51/200\n129/129 [==============================] - 11s 87ms/step - loss: 0.0152 - accuracy: 0.9971\nEpoch 52/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0073 - accuracy: 0.9995\nEpoch 53/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0040 - accuracy: 1.0000\nEpoch 54/200\n129/129 [==============================] - 11s 85ms/step - loss: 0.0036 - accuracy: 1.0000\nEpoch 55/200\n129/129 [==============================] - 11s 83ms/step - loss: 0.0028 - accuracy: 1.0000\nEpoch 56/200\n129/129 [==============================] - 10s 81ms/step - loss: 0.0023 - accuracy: 1.0000\nEpoch 57/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0025 - accuracy: 1.0000\nEpoch 58/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0025 - accuracy: 1.0000\nEpoch 59/200\n129/129 [==============================] - 11s 87ms/step - loss: 0.0029 - accuracy: 1.0000\nEpoch 60/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0025 - accuracy: 0.9995\nEpoch 61/200\n129/129 [==============================] - 11s 85ms/step - loss: 0.0199 - accuracy: 0.9932\nEpoch 62/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0654 - accuracy: 0.9816\nEpoch 63/200\n129/129 [==============================] - 11s 87ms/step - loss: 0.0342 - accuracy: 0.9927\nEpoch 64/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0281 - accuracy: 0.9898\nEpoch 65/200\n129/129 [==============================] - 10s 78ms/step - loss: 0.0121 - accuracy: 0.9981\nEpoch 66/200\n129/129 [==============================] - 11s 84ms/step - loss: 0.0092 - accuracy: 0.9981\nEpoch 67/200\n129/129 [==============================] - 11s 81ms/step - loss: 0.0139 - accuracy: 0.9961\nEpoch 68/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0062 - accuracy: 0.9990\nEpoch 69/200\n129/129 [==============================] - 11s 84ms/step - loss: 0.0033 - accuracy: 0.9995\nEpoch 70/200\n129/129 [==============================] - 10s 81ms/step - loss: 0.0077 - accuracy: 0.9985\nEpoch 71/200\n129/129 [==============================] - 10s 81ms/step - loss: 0.0038 - accuracy: 1.0000\nEpoch 72/200\n129/129 [==============================] - 11s 85ms/step - loss: 0.0021 - accuracy: 1.0000\nEpoch 73/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0028 - accuracy: 0.9995\nEpoch 74/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0012 - accuracy: 1.0000\nEpoch 75/200\n129/129 [==============================] - 11s 87ms/step - loss: 0.0011 - accuracy: 1.0000\nEpoch 76/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0010 - accuracy: 1.0000\nEpoch 77/200\n129/129 [==============================] - 10s 80ms/step - loss: 8.3328e-04 - accuracy: 1.0000\nEpoch 78/200\n129/129 [==============================] - 10s 80ms/step - loss: 6.9415e-04 - accuracy: 1.0000\nEpoch 79/200\n129/129 [==============================] - 11s 82ms/step - loss: 9.3545e-04 - accuracy: 1.0000\nEpoch 80/200\n129/129 [==============================] - 10s 80ms/step - loss: 6.6420e-04 - accuracy: 1.0000\nEpoch 81/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0016 - accuracy: 1.0000\nEpoch 82/200\n129/129 [==============================] - 11s 85ms/step - loss: 0.0140 - accuracy: 0.9956\nEpoch 83/200\n129/129 [==============================] - 11s 81ms/step - loss: 0.0233 - accuracy: 0.9937\nEpoch 84/200\n129/129 [==============================] - 11s 84ms/step - loss: 0.0112 - accuracy: 0.9985\nEpoch 85/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0051 - accuracy: 0.9990\nEpoch 86/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0025 - accuracy: 1.0000\nEpoch 87/200\n129/129 [==============================] - 11s 85ms/step - loss: 0.0068 - accuracy: 0.9981\nEpoch 88/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0116 - accuracy: 0.9976\nEpoch 89/200\n129/129 [==============================] - 11s 84ms/step - loss: 0.0042 - accuracy: 0.9995\nEpoch 90/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0019 - accuracy: 1.0000\nEpoch 91/200\n129/129 [==============================] - 11s 83ms/step - loss: 0.0015 - accuracy: 1.0000\nEpoch 92/200\n129/129 [==============================] - 11s 84ms/step - loss: 8.7797e-04 - accuracy: 1.0000\nEpoch 93/200\n129/129 [==============================] - 10s 80ms/step - loss: 7.4717e-04 - accuracy: 1.0000\nEpoch 94/200\n129/129 [==============================] - 10s 80ms/step - loss: 6.3452e-04 - accuracy: 1.0000\nEpoch 95/200\n129/129 [==============================] - 11s 86ms/step - loss: 5.1263e-04 - accuracy: 1.0000\nEpoch 96/200\n129/129 [==============================] - 10s 79ms/step - loss: 5.0989e-04 - accuracy: 1.0000\nEpoch 97/200\n129/129 [==============================] - 10s 79ms/step - loss: 5.9365e-04 - accuracy: 1.0000\nEpoch 98/200\n129/129 [==============================] - 10s 79ms/step - loss: 4.3286e-04 - accuracy: 1.0000\nEpoch 99/200\n129/129 [==============================] - 10s 81ms/step - loss: 4.5296e-04 - accuracy: 1.0000\nEpoch 100/200\n129/129 [==============================] - 11s 84ms/step - loss: 3.3902e-04 - accuracy: 1.0000\nEpoch 101/200\n129/129 [==============================] - 10s 79ms/step - loss: 5.7539e-04 - accuracy: 1.0000\nEpoch 102/200\n129/129 [==============================] - 10s 80ms/step - loss: 3.2159e-04 - accuracy: 1.0000\nEpoch 103/200\n129/129 [==============================] - 11s 86ms/step - loss: 3.1626e-04 - accuracy: 1.0000\nEpoch 104/200\n129/129 [==============================] - 10s 78ms/step - loss: 4.4789e-04 - accuracy: 1.0000\nEpoch 105/200\n129/129 [==============================] - 10s 78ms/step - loss: 0.0158 - accuracy: 0.9966\nEpoch 106/200\n129/129 [==============================] - 11s 83ms/step - loss: 0.0152 - accuracy: 0.9956\nEpoch 107/200\n129/129 [==============================] - 10s 81ms/step - loss: 0.0043 - accuracy: 1.0000\nEpoch 108/200\n129/129 [==============================] - 11s 83ms/step - loss: 0.0017 - accuracy: 1.0000\nEpoch 109/200\n129/129 [==============================] - 10s 78ms/step - loss: 9.1192e-04 - accuracy: 1.0000\nEpoch 110/200\n129/129 [==============================] - 10s 79ms/step - loss: 9.5712e-04 - accuracy: 1.0000\nEpoch 111/200\n129/129 [==============================] - 11s 85ms/step - loss: 0.0019 - accuracy: 0.9995\nEpoch 112/200\n129/129 [==============================] - 10s 78ms/step - loss: 4.6710e-04 - accuracy: 1.0000\nEpoch 113/200\n129/129 [==============================] - 10s 78ms/step - loss: 7.3392e-04 - accuracy: 1.0000\nEpoch 114/200\n129/129 [==============================] - 11s 83ms/step - loss: 5.0883e-04 - accuracy: 1.0000\nEpoch 115/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0016 - accuracy: 0.9995\nEpoch 116/200\n129/129 [==============================] - 10s 78ms/step - loss: 8.4065e-04 - accuracy: 1.0000\nEpoch 117/200\n129/129 [==============================] - 11s 84ms/step - loss: 5.4897e-04 - accuracy: 1.0000\nEpoch 118/200\n129/129 [==============================] - 10s 79ms/step - loss: 5.3473e-04 - accuracy: 1.0000\nEpoch 119/200\n129/129 [==============================] - 11s 85ms/step - loss: 7.5627e-04 - accuracy: 0.9995\nEpoch 120/200\n129/129 [==============================] - 10s 78ms/step - loss: 0.0059 - accuracy: 0.9990\nEpoch 121/200\n129/129 [==============================] - 10s 78ms/step - loss: 0.0091 - accuracy: 0.9966\nEpoch 122/200\n129/129 [==============================] - 11s 83ms/step - loss: 0.0064 - accuracy: 0.9990\nEpoch 123/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0046 - accuracy: 0.9985\nEpoch 124/200\n129/129 [==============================] - 10s 78ms/step - loss: 0.0017 - accuracy: 1.0000\nEpoch 125/200\n129/129 [==============================] - 11s 83ms/step - loss: 6.0675e-04 - accuracy: 1.0000\nEpoch 126/200\n129/129 [==============================] - 10s 79ms/step - loss: 4.6077e-04 - accuracy: 1.0000\nEpoch 127/200\n129/129 [==============================] - 10s 80ms/step - loss: 4.6218e-04 - accuracy: 1.0000\nEpoch 128/200\n129/129 [==============================] - 10s 78ms/step - loss: 5.2518e-04 - accuracy: 1.0000\nEpoch 129/200\n129/129 [==============================] - 10s 78ms/step - loss: 4.4346e-04 - accuracy: 1.0000\nEpoch 130/200\n129/129 [==============================] - 11s 84ms/step - loss: 4.8268e-04 - accuracy: 1.0000\nEpoch 131/200\n129/129 [==============================] - 10s 80ms/step - loss: 3.3040e-04 - accuracy: 1.0000\nEpoch 132/200\n129/129 [==============================] - 10s 79ms/step - loss: 9.6675e-04 - accuracy: 0.9995\nEpoch 133/200\n129/129 [==============================] - 11s 84ms/step - loss: 4.2396e-04 - accuracy: 1.0000\nEpoch 134/200\n129/129 [==============================] - 10s 79ms/step - loss: 4.4518e-04 - accuracy: 1.0000\nEpoch 135/200\n129/129 [==============================] - 11s 82ms/step - loss: 3.1731e-04 - accuracy: 1.0000\nEpoch 136/200\n129/129 [==============================] - 11s 83ms/step - loss: 3.5701e-04 - accuracy: 1.0000\nEpoch 137/200\n129/129 [==============================] - 10s 79ms/step - loss: 2.3190e-04 - accuracy: 1.0000\nEpoch 138/200\n129/129 [==============================] - 10s 79ms/step - loss: 2.2405e-04 - accuracy: 1.0000\nEpoch 139/200\n129/129 [==============================] - 11s 85ms/step - loss: 2.3646e-04 - accuracy: 1.0000\nEpoch 140/200\n129/129 [==============================] - 10s 78ms/step - loss: 1.6029e-04 - accuracy: 1.0000\nEpoch 141/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.6816e-04 - accuracy: 1.0000\nEpoch 142/200\n129/129 [==============================] - 11s 84ms/step - loss: 1.6948e-04 - accuracy: 1.0000\nEpoch 143/200\n129/129 [==============================] - 10s 81ms/step - loss: 1.5685e-04 - accuracy: 1.0000\nEpoch 144/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.5377e-04 - accuracy: 1.0000\nEpoch 145/200\n129/129 [==============================] - 10s 79ms/step - loss: 9.8698e-05 - accuracy: 1.0000\nEpoch 146/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.5789e-04 - accuracy: 1.0000\nEpoch 147/200\n129/129 [==============================] - 11s 84ms/step - loss: 1.1674e-04 - accuracy: 1.0000\nEpoch 148/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.3956e-04 - accuracy: 1.0000\nEpoch 149/200\n129/129 [==============================] - 10s 78ms/step - loss: 1.4665e-04 - accuracy: 1.0000\nEpoch 150/200\n129/129 [==============================] - 10s 81ms/step - loss: 1.1244e-04 - accuracy: 1.0000\nEpoch 151/200\n129/129 [==============================] - 11s 81ms/step - loss: 9.6211e-05 - accuracy: 1.0000\nEpoch 152/200\n129/129 [==============================] - 10s 80ms/step - loss: 9.3651e-05 - accuracy: 1.0000\nEpoch 153/200\n129/129 [==============================] - 11s 82ms/step - loss: 1.0298e-04 - accuracy: 1.0000\nEpoch 154/200\n129/129 [==============================] - 10s 79ms/step - loss: 8.3132e-05 - accuracy: 1.0000\nEpoch 155/200\n129/129 [==============================] - 10s 80ms/step - loss: 6.9135e-05 - accuracy: 1.0000\nEpoch 156/200\n129/129 [==============================] - 11s 84ms/step - loss: 1.3953e-04 - accuracy: 1.0000\nEpoch 157/200\n129/129 [==============================] - 10s 79ms/step - loss: 8.3709e-05 - accuracy: 1.0000\nEpoch 158/200\n129/129 [==============================] - 10s 78ms/step - loss: 7.6979e-05 - accuracy: 1.0000\nEpoch 159/200\n129/129 [==============================] - 11s 84ms/step - loss: 1.8212e-04 - accuracy: 1.0000\nEpoch 160/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0037 - accuracy: 0.9985\nEpoch 161/200\n129/129 [==============================] - 10s 78ms/step - loss: 9.7229e-04 - accuracy: 1.0000\nEpoch 162/200\n129/129 [==============================] - 11s 83ms/step - loss: 2.8654e-04 - accuracy: 1.0000\nEpoch 163/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0051 - accuracy: 0.9981\nEpoch 164/200\n129/129 [==============================] - 10s 78ms/step - loss: 8.2101e-04 - accuracy: 1.0000\nEpoch 165/200\n129/129 [==============================] - 11s 83ms/step - loss: 2.5760e-04 - accuracy: 1.0000\nEpoch 166/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.3639e-04 - accuracy: 1.0000\nEpoch 167/200\n129/129 [==============================] - 10s 80ms/step - loss: 1.8559e-04 - accuracy: 1.0000\nEpoch 168/200\n129/129 [==============================] - 11s 84ms/step - loss: 1.3451e-04 - accuracy: 1.0000\nEpoch 169/200\n129/129 [==============================] - 10s 78ms/step - loss: 1.3094e-04 - accuracy: 1.0000\nEpoch 170/200\n129/129 [==============================] - 10s 77ms/step - loss: 1.2725e-04 - accuracy: 1.0000\nEpoch 171/200\n129/129 [==============================] - 11s 85ms/step - loss: 1.8965e-04 - accuracy: 1.0000\nEpoch 172/200\n129/129 [==============================] - 10s 79ms/step - loss: 2.0460e-04 - accuracy: 1.0000\nEpoch 173/200\n129/129 [==============================] - 11s 84ms/step - loss: 1.1527e-04 - accuracy: 1.0000\nEpoch 174/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.3995e-04 - accuracy: 1.0000\nEpoch 175/200\n129/129 [==============================] - 10s 80ms/step - loss: 4.6349e-04 - accuracy: 1.0000\nEpoch 176/200\n129/129 [==============================] - 10s 78ms/step - loss: 3.3991e-04 - accuracy: 1.0000\nEpoch 177/200\n129/129 [==============================] - 10s 78ms/step - loss: 8.5286e-05 - accuracy: 1.0000\nEpoch 178/200\n129/129 [==============================] - 11s 83ms/step - loss: 2.0132e-04 - accuracy: 1.0000\nEpoch 179/200\n129/129 [==============================] - 10s 80ms/step - loss: 1.9701e-04 - accuracy: 1.0000\nEpoch 180/200\n129/129 [==============================] - 10s 77ms/step - loss: 1.2337e-04 - accuracy: 1.0000\nEpoch 181/200\n129/129 [==============================] - 11s 84ms/step - loss: 7.9860e-05 - accuracy: 1.0000\nEpoch 182/200\n129/129 [==============================] - 10s 79ms/step - loss: 9.2737e-05 - accuracy: 1.0000\nEpoch 183/200\n129/129 [==============================] - 10s 81ms/step - loss: 1.9232e-04 - accuracy: 1.0000\nEpoch 184/200\n129/129 [==============================] - 11s 84ms/step - loss: 1.6341e-04 - accuracy: 1.0000\nEpoch 185/200\n129/129 [==============================] - 10s 78ms/step - loss: 8.4847e-05 - accuracy: 1.0000\nEpoch 186/200\n129/129 [==============================] - 10s 78ms/step - loss: 9.5463e-05 - accuracy: 1.0000\nEpoch 187/200\n129/129 [==============================] - 11s 84ms/step - loss: 5.7110e-05 - accuracy: 1.0000\nEpoch 188/200\n129/129 [==============================] - 10s 78ms/step - loss: 7.9838e-05 - accuracy: 1.0000\nEpoch 189/200\n129/129 [==============================] - 10s 79ms/step - loss: 6.8357e-05 - accuracy: 1.0000\nEpoch 190/200\n129/129 [==============================] - 11s 83ms/step - loss: 5.3510e-05 - accuracy: 1.0000\nEpoch 191/200\n129/129 [==============================] - 10s 79ms/step - loss: 4.8145e-05 - accuracy: 1.0000\nEpoch 192/200\n129/129 [==============================] - 10s 78ms/step - loss: 4.3636e-05 - accuracy: 1.0000\nEpoch 193/200\n129/129 [==============================] - 11s 82ms/step - loss: 5.0044e-05 - accuracy: 1.0000\nEpoch 194/200\n129/129 [==============================] - 10s 78ms/step - loss: 5.6506e-05 - accuracy: 1.0000\nEpoch 195/200\n129/129 [==============================] - 10s 81ms/step - loss: 5.6957e-05 - accuracy: 1.0000\nEpoch 196/200\n129/129 [==============================] - 11s 83ms/step - loss: 3.7172e-05 - accuracy: 1.0000\nEpoch 197/200\n129/129 [==============================] - 10s 79ms/step - loss: 6.0754e-05 - accuracy: 1.0000\nEpoch 198/200\n129/129 [==============================] - 10s 79ms/step - loss: 5.1432e-05 - accuracy: 1.0000\nEpoch 199/200\n129/129 [==============================] - 11s 84ms/step - loss: 3.5704e-05 - accuracy: 1.0000\nEpoch 200/200\n129/129 [==============================] - 10s 79ms/step - loss: 3.4176e-05 - accuracy: 1.0000\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = []  # True labels for the test data\npredicted_labels = []  # Predicted labels for the test data\n\nfor batch_data, batch_labels in test_dataset:\n    batch_predictions = model.predict(batch_data)\n    batch_predicted_labels = np.argmax(batch_predictions, axis=1)\n    predicted_labels.extend(batch_predicted_labels)\n    true_labels.extend(np.argmax(batch_labels, axis=1))\n\ntrue_labels = np.array(true_labels)\npredicted_labels = np.array(predicted_labels)\n\naccuracy = np.mean(true_labels == predicted_labels)\nprint(true_labels)\nprint(predicted_labels)\nprint(accuracy)","metadata":{"papermill":{"duration":13.103915,"end_time":"2023-07-24T13:02:37.704268","exception":false,"start_time":"2023-07-24T13:02:24.600353","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-29T20:21:17.442642Z","iopub.execute_input":"2023-07-29T20:21:17.443428Z","iopub.status.idle":"2023-07-29T20:21:26.498376Z","shell.execute_reply.started":"2023-07-29T20:21:17.443392Z","shell.execute_reply":"2023-07-29T20:21:26.496782Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 34ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 35ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 28ms/step\n[29 11 26 ... 23 28 20]\n[29 11 26 ... 23 28 20]\n0.8134615384615385\n","output_type":"stream"}]}]}