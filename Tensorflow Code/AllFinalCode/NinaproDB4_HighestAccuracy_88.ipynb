{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.025217,"end_time":"2023-08-06T21:57:19.317113","exception":false,"start_time":"2023-08-06T21:57:19.291896","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T17:56:21.121850Z","iopub.execute_input":"2023-08-09T17:56:21.122250Z","iopub.status.idle":"2023-08-09T17:56:21.611402Z","shell.execute_reply.started":"2023-08-09T17:56:21.122218Z","shell.execute_reply":"2023-08-09T17:56:21.610471Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/newninaprodb4/ninaprodb44test.pkl\n/kaggle/input/newninaprodb4/ninaprodb44train.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport os\nfrom tensorflow.keras.layers import TimeDistributed, Conv1D\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import Activation\n\n# ... (other code) ...\n\n\nimport random\n#from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport math","metadata":{"papermill":{"duration":9.307141,"end_time":"2023-08-06T21:57:28.628582","exception":false,"start_time":"2023-08-06T21:57:19.321441","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T17:56:21.613314Z","iopub.execute_input":"2023-08-09T17:56:21.613720Z","iopub.status.idle":"2023-08-09T17:57:01.503813Z","shell.execute_reply.started":"2023-08-09T17:56:21.613690Z","shell.execute_reply":"2023-08-09T17:57:01.502918Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"D0809 17:56:53.587567727      15 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\nD0809 17:56:53.587590078      15 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\nD0809 17:56:53.587593462      15 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\nD0809 17:56:53.587596332      15 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\nD0809 17:56:53.587599108      15 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\nD0809 17:56:53.587602085      15 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\nD0809 17:56:53.587604781      15 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\nD0809 17:56:53.587608014      15 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\nD0809 17:56:53.587610617      15 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\nD0809 17:56:53.587613214      15 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\nD0809 17:56:53.587615877      15 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\nD0809 17:56:53.587618422      15 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\nD0809 17:56:53.587621063      15 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\nD0809 17:56:53.587623558      15 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\nI0809 17:56:53.587800861      15 ev_epoll1_linux.cc:122]               grpc epoll fd: 66\nD0809 17:56:53.587814554      15 ev_posix.cc:144]                      Using polling engine: epoll1\nD0809 17:56:53.587833189      15 dns_resolver_ares.cc:822]             Using ares dns resolver\nD0809 17:56:53.588283583      15 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD0809 17:56:53.588294390      15 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD0809 17:56:53.588298264      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD0809 17:56:53.588301881      15 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD0809 17:56:53.588305288      15 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD0809 17:56:53.588308538      15 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\nD0809 17:56:53.588315673      15 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD0809 17:56:53.588333354      15 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD0809 17:56:53.588359713      15 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD0809 17:56:53.588373667      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD0809 17:56:53.588377194      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD0809 17:56:53.588380596      15 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD0809 17:56:53.588387018      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\nD0809 17:56:53.588391186      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD0809 17:56:53.588406152      15 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD0809 17:56:53.588410167      15 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\nI0809 17:56:53.591552347      15 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\nI0809 17:56:53.607406813     369 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE0809 17:56:53.614429154     369 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2023-08-09T17:56:53.614412355+00:00\"}\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"papermill":{"duration":1.004777,"end_time":"2023-08-06T21:57:29.646226","exception":false,"start_time":"2023-08-06T21:57:28.641449","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T17:57:01.504912Z","iopub.execute_input":"2023-08-09T17:57:01.505460Z","iopub.status.idle":"2023-08-09T17:57:02.087559Z","shell.execute_reply.started":"2023-08-09T17:57:01.505427Z","shell.execute_reply":"2023-08-09T17:57:02.086519Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers, Sequential, optimizers, Input, Model","metadata":{"papermill":{"duration":0.012163,"end_time":"2023-08-06T21:57:29.663303","exception":false,"start_time":"2023-08-06T21:57:29.651140","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T17:57:02.088712Z","iopub.execute_input":"2023-08-09T17:57:02.089188Z","iopub.status.idle":"2023-08-09T17:57:02.093465Z","shell.execute_reply.started":"2023-08-09T17:57:02.089160Z","shell.execute_reply":"2023-08-09T17:57:02.092679Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**WORKED CODE HERE**","metadata":{"papermill":{"duration":0.004019,"end_time":"2023-08-06T21:57:29.671448","exception":false,"start_time":"2023-08-06T21:57:29.667429","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n#import tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\nimport gc\nimport random\nimport os\n\nimport matplotlib.pyplot as plt\nimport json","metadata":{"papermill":{"duration":0.014276,"end_time":"2023-08-06T21:57:29.689823","exception":false,"start_time":"2023-08-06T21:57:29.675547","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T17:57:02.095841Z","iopub.execute_input":"2023-08-09T17:57:02.096258Z","iopub.status.idle":"2023-08-09T17:57:03.241129Z","shell.execute_reply.started":"2023-08-09T17:57:02.096229Z","shell.execute_reply":"2023-08-09T17:57:03.239983Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"epochs = 64\nlearning_rate = 1e-3\nbatch_size = 16\nmethod = \"default\"\ndataset_type = 1","metadata":{"papermill":{"duration":0.012297,"end_time":"2023-08-06T21:57:29.708339","exception":false,"start_time":"2023-08-06T21:57:29.696042","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T17:57:03.242452Z","iopub.execute_input":"2023-08-09T17:57:03.242750Z","iopub.status.idle":"2023-08-09T17:57:03.247560Z","shell.execute_reply.started":"2023-08-09T17:57:03.242720Z","shell.execute_reply":"2023-08-09T17:57:03.246668Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"pip install PyWavelets\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:57:03.248576Z","iopub.execute_input":"2023-08-09T17:57:03.248833Z","iopub.status.idle":"2023-08-09T17:57:09.557770Z","shell.execute_reply.started":"2023-08-09T17:57:03.248809Z","shell.execute_reply":"2023-08-09T17:57:09.556650Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Collecting PyWavelets\n  Downloading PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/site-packages (from PyWavelets) (1.23.5)\nInstalling collected packages: PyWavelets\nSuccessfully installed PyWavelets-1.4.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pywt  # Added import for wavelet denoising\nimport scipy.signal as signal  # Added import for power line noise and low pass filtering\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nimport pywt\nimport scipy.signal as signal\nimport tensorflow as tf\nclass Nina1Dataset(tf.keras.utils.Sequence):\n    def __init__(self, dataframe, batch_size):\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.scaler = StandardScaler()\n        self.scaler.fit(np.concatenate(self.dataframe['emg'].tolist()))\n        \n        \n    def __len__(self):\n        return int(np.ceil(len(self.dataframe) / self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_data = self.dataframe[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        batch_input_data = []\n        batch_labels = []\n\n        for i, target_row in batch_data.iterrows():\n            data = target_row['emg'][:10000]\n\n            # Zero-Padding\n            if len(data) < 10000:\n                data = np.concatenate((data, np.zeros((10000 - len(data), 12))), axis=0)\n\n            \n            if(target_row['stimulus'] != 52):\n                fs = 2000  # Assuming sampling frequency of 2000 Hz\n                f0 = 50  # Power line frequency\n                Q = 30  # Quality factor\n                w0 = f0 / (fs / 2)\n                b, a = signal.iirnotch(w0, Q)\n                data = signal.lfilter(b, a, data, axis=0)\n\n                    # Low Pass Filtering with cutoff frequency fc = 500 Hz\n                fc = 500  # Cutoff frequency\n                b, a = signal.butter(4, fc / (fs / 2), 'low')\n                data = signal.lfilter(b, a, data, axis=0)\n\n\n                coeffs = pywt.wavedec(data, 'sym8')  \n                coeffs[1:] = (pywt.threshold(c, value=0.5) for c in coeffs[1:])\n                data = pywt.waverec(coeffs, 'sym8')\n\n\n                # Wavelet Denoising\n\n\n\n\n                    # Standardize the data\n                data = self.scaler.transform(data)\n                #data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n\n                    # Division data by time-segment (channel-wise)\n                input_data = data.reshape((25, 400, 12))\n                label = target_row['stimulus']\n                batch_input_data.append(input_data)\n                batch_labels.append(label)\n\n        # Check if the batch size is smaller than the desired batch_size\n        if len(batch_data) < self.batch_size:\n            # Create a dummy batch with all elements set to zero\n            dummy_input_data = np.zeros((self.batch_size,) + input_data.shape, dtype=np.float32)\n            dummy_labels = np.zeros((self.batch_size,), dtype=np.int32)\n            dummy_input_data[:len(batch_input_data)] = np.array(batch_input_data)\n            dummy_labels[:len(batch_labels)] = np.array(batch_labels)\n            dummy_labels = to_categorical(dummy_labels, num_classes=52)  # Convert labels to one-hot encoding\n\n            return dummy_input_data, dummy_labels\n\n        batch_labels = to_categorical(batch_labels, num_classes=52)  # Convert labels to one-hot encoding\n\n        return np.array(batch_input_data), np.array(batch_labels)\n\n# Parameters\n\nbatch_size = 16\ntrain_dir = '/kaggle/input/newninaprodb4/ninaprodb44train.pkl'\ntest_dir = '/kaggle/input/newninaprodb4/ninaprodb44test.pkl'\n\n# Set up dataset\ntrain = pd.read_pickle(train_dir)\neval_data = pd.read_pickle(test_dir)\n\n# Load train data\ntrain_data = pd.read_pickle(train_dir)\n\nimport pandas as pd\n\n# Assuming you have your DataFrame 'train_data' already defined\n\nselected_categories = [51, 28, 11]\n\nadditional_data = []  # Initialize an empty list to store the additional data\n\nfor category in selected_categories:\n    category_data = train_data[train_data['stimulus'] == category]\n    existing_count = len(category_data)\n    \n    if existing_count < 40:\n        additional_count = 40 - existing_count\n        if additional_count > 0:\n            sampled_data = category_data.sample(min(additional_count, len(category_data)), replace=True)\n            additional_data.append(sampled_data)\n\n# Concatenate the additional data into a new DataFrame\nadditional_data_df = pd.concat(additional_data, ignore_index=True)\n\n# Concatenate the original train_data and the additional_data_df\ntrain_data = pd.concat([train_data, additional_data_df], ignore_index=True)\n\nprint(train_data['stimulus'].value_counts())  # Display updated category counts\n\n\n# Shuffle data\ntrain_data = train_data.sample(frac=1).reset_index(drop=True)\neval_data = eval_data.sample(frac=1).reset_index(drop=True)\n#train_data, val_data = train_test_split(train_data, test_size=0.3, random_state=21)\n# Create train and test datasets\ntrain_dataset = Nina1Dataset(train_data, batch_size=batch_size)\n#val_dataset = Nina1Dataset(val_data, batch_size=batch_size)\ntest_dataset = Nina1Dataset(eval_data, batch_size=batch_size)\n\nprint(train_dataset)\n","metadata":{"papermill":{"duration":20.88191,"end_time":"2023-08-06T21:57:50.594783","exception":false,"start_time":"2023-08-06T21:57:29.712873","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T17:57:09.559304Z","iopub.execute_input":"2023-08-09T17:57:09.559611Z","iopub.status.idle":"2023-08-09T17:57:32.903578Z","shell.execute_reply.started":"2023-08-09T17:57:09.559582Z","shell.execute_reply":"2023-08-09T17:57:32.902652Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"stimulus\n0     40\n1     40\n28    40\n29    40\n30    40\n31    40\n32    40\n33    40\n34    40\n35    40\n36    40\n37    40\n38    40\n39    40\n40    40\n41    40\n42    40\n43    40\n44    40\n45    40\n46    40\n47    40\n48    40\n49    40\n50    40\n27    40\n26    40\n25    40\n12    40\n2     40\n3     40\n4     40\n5     40\n6     40\n7     40\n8     40\n9     40\n10    40\n11    40\n13    40\n24    40\n14    40\n15    40\n16    40\n17    40\n18    40\n19    40\n20    40\n21    40\n22    40\n23    40\n51    40\nName: count, dtype: int64\n<__main__.Nina1Dataset object at 0x79173753bf70>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_data['stimulus'])","metadata":{"papermill":{"duration":0.017775,"end_time":"2023-08-06T21:57:50.617152","exception":false,"start_time":"2023-08-06T21:57:50.599377","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T17:57:32.904745Z","iopub.execute_input":"2023-08-09T17:57:32.905042Z","iopub.status.idle":"2023-08-09T17:57:32.911350Z","shell.execute_reply.started":"2023-08-09T17:57:32.905016Z","shell.execute_reply":"2023-08-09T17:57:32.910614Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"0       33\n1        7\n2       12\n3       38\n4        1\n        ..\n2075     6\n2076    17\n2077    44\n2078    44\n2079    27\nName: stimulus, Length: 2080, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"print(eval_data['stimulus'])","metadata":{"execution":{"iopub.status.busy":"2023-08-09T17:57:32.912331Z","iopub.execute_input":"2023-08-09T17:57:32.912611Z","iopub.status.idle":"2023-08-09T17:57:32.927746Z","shell.execute_reply.started":"2023-08-09T17:57:32.912587Z","shell.execute_reply":"2023-08-09T17:57:32.926901Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"0       51\n1       30\n2       39\n3        0\n4        6\n        ..\n1035    33\n1036     9\n1037    11\n1038     9\n1039    13\nName: stimulus, Length: 1040, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras import regularizers, initializers\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\n# Define the CNN-BiLSTM model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.layers import TimeDistributed\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\n# Define the CNN-BiLSTM model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras import regularizers, initializers\ndef cnn(x):\n    #print(x.shape)\n    #x = Reshape((25, 20,10))(x)  \n    print(x.shape)\n    x = TimeDistributed((Conv1D(64, kernel_size=9, strides=2, padding='same', activation=tanh)))(x)\n    print(x.shape)\n    #(Batch, 10, 64)\n    #x = TimeDistributed(Dropout(0.2093))(x)\n    #x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(MaxPooling1D(pool_size=8, strides=2))(x)\n    print(x.shape)\n    #(Batch, 2, 64)\n    \n    #x = TimeDistributed(Dropout(0.2093))(x)\n    \n    \n    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n    #(Batch, 1, 64)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(Dropout(0.2093))(x)\n    \n    \n    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n    print(x.shape)\n    #(Batch, 1, 64)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(Dropout(0.2093))(x)\n   \n    \n    x = TimeDistributed((Conv1D(64, kernel_size=3, strides=2, padding='same', activation=tanh)))(x)\n    #print(x.shape)\n    #(Batch, 1, 64)\n    x = TimeDistributed(Dropout(0.2093))(x)\n    x = TimeDistributed(Dropout(0.5))(x)\n    #x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(Flatten())(x)\n    print(x.shape)\n    # (Batch, 64)\n\n    return x\n\ndef Bi_LSTMModel(input_shape,x):\n    #model = Sequential()\n    # Hidden dimensions\n    hidden_dim = 200\n    print(x.shape)\n    print(11)\n    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.3), input_shape=input_shape)(x)\n    #x = Dropout(0.2093)(x)\n    print(x.shape)\n    print(11)\n    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.3))(x)\n\n    print(x.shape)\n    print(11)\n    #x = Dropout(0.2093)(x)\n    x = Flatten()(x)\n    # ( ,10000)\n\n    return x\ndef EMGHandNet(input_shape, num_classes):\n    # Define the input layer\n    x = Input(shape=input_shape)\n    inputs = x\n    #print(x.shape)\n    #(batch, 25, 20, 10)\n    #temp = [cnn(x[:, t, :, :]) for t in range(x.shape[1])]\n    #x = tf.stack(temp, axis=1)\n    #print(x.shape)\n    x = cnn(x)\n   \n    #print(x.shape)\n    x = Bi_LSTMModel(x.shape[1:],x)\n    #print(x.shape)\n    #x = Dropout(0.2093)(x)\n    \n    x = Dense(512, activation='tanh')(x)\n    #print(x.shape)\n    x = Dropout(0.2093)(x)\n    #x = BatchNormalization(epsilon=1e-6, momentum=0.95)(x)\n\n    # Add the output layer\n    output_layer = Dense(52, activation='softmax')(x)\n    # Create the model\n    model = Model(inputs=inputs, outputs=output_layer)\n\n    return model\n\nnum_classes = 52\nmodel = EMGHandNet((25, 400, 12), num_classes)\n\n\ninitial_learning_rate = 0.001\ndecay_steps = 1000\ndecay_rate = 0.9\nbatch_size = 16\n# Define your model and its optimizer\n# Define learning rate schedule\nlr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n\n    # Compile the model with learning rate schedule\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999),\n                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n                metrics=['accuracy'])\n\n\n# Define your model and its optimizer\n# Define learning rate schedule\n\n\nhistory = model.fit(train_dataset,\n                    epochs=200,\n                    batch_size=16,\n                    validation_data=test_dataset)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T18:32:20.908480Z","iopub.execute_input":"2023-08-09T18:32:20.909132Z","iopub.status.idle":"2023-08-09T19:49:00.731375Z","shell.execute_reply.started":"2023-08-09T18:32:20.909096Z","shell.execute_reply":"2023-08-09T19:49:00.729934Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"(None, 25, 400, 12)\n(None, 25, 200, 64)\n(None, 25, 97, 64)\n(None, 25, 25, 64)\n(None, 25, 832)\n(None, 25, 832)\n11\n(None, 25, 400)\n11\n(None, 25, 400)\n11\nEpoch 1/200\n130/130 [==============================] - 34s 191ms/step - loss: 3.7703 - accuracy: 0.0976 - val_loss: 3.8930 - val_accuracy: 0.0894\nEpoch 2/200\n130/130 [==============================] - 23s 174ms/step - loss: 2.8087 - accuracy: 0.2337 - val_loss: 2.8706 - val_accuracy: 0.2154\nEpoch 3/200\n130/130 [==============================] - 22s 173ms/step - loss: 2.1425 - accuracy: 0.3529 - val_loss: 2.8565 - val_accuracy: 0.2308\nEpoch 4/200\n130/130 [==============================] - 23s 173ms/step - loss: 1.7165 - accuracy: 0.4822 - val_loss: 1.6690 - val_accuracy: 0.4798\nEpoch 5/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.4510 - accuracy: 0.5447 - val_loss: 2.8912 - val_accuracy: 0.2856\nEpoch 6/200\n130/130 [==============================] - 23s 174ms/step - loss: 1.2410 - accuracy: 0.6077 - val_loss: 1.3398 - val_accuracy: 0.5606\nEpoch 7/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.0088 - accuracy: 0.6678 - val_loss: 1.8779 - val_accuracy: 0.4433\nEpoch 8/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.7507 - accuracy: 0.7611 - val_loss: 2.0434 - val_accuracy: 0.4394\nEpoch 9/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.6607 - accuracy: 0.7793 - val_loss: 1.4530 - val_accuracy: 0.5683\nEpoch 10/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.5437 - accuracy: 0.8250 - val_loss: 0.9978 - val_accuracy: 0.7038\nEpoch 11/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.4177 - accuracy: 0.8611 - val_loss: 1.1091 - val_accuracy: 0.6529\nEpoch 12/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.4120 - accuracy: 0.8659 - val_loss: 0.8607 - val_accuracy: 0.7327\nEpoch 13/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.2899 - accuracy: 0.9149 - val_loss: 0.7441 - val_accuracy: 0.7663\nEpoch 14/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.2417 - accuracy: 0.9308 - val_loss: 1.0714 - val_accuracy: 0.6808\nEpoch 15/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.1888 - accuracy: 0.9466 - val_loss: 0.6692 - val_accuracy: 0.7923\nEpoch 16/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.1449 - accuracy: 0.9572 - val_loss: 0.7064 - val_accuracy: 0.7856\nEpoch 17/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.1110 - accuracy: 0.9764 - val_loss: 0.6446 - val_accuracy: 0.8106\nEpoch 18/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0861 - accuracy: 0.9837 - val_loss: 0.7243 - val_accuracy: 0.7913\nEpoch 19/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0669 - accuracy: 0.9880 - val_loss: 0.6299 - val_accuracy: 0.8115\nEpoch 20/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0802 - accuracy: 0.9798 - val_loss: 0.5882 - val_accuracy: 0.8260\nEpoch 21/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0841 - accuracy: 0.9784 - val_loss: 0.9371 - val_accuracy: 0.7529\nEpoch 22/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.1117 - accuracy: 0.9688 - val_loss: 0.8784 - val_accuracy: 0.7423\nEpoch 23/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0997 - accuracy: 0.9750 - val_loss: 0.7636 - val_accuracy: 0.7923\nEpoch 24/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0951 - accuracy: 0.9769 - val_loss: 0.8531 - val_accuracy: 0.7654\nEpoch 25/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.1117 - accuracy: 0.9683 - val_loss: 0.8907 - val_accuracy: 0.7510\nEpoch 26/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.1245 - accuracy: 0.9659 - val_loss: 0.6517 - val_accuracy: 0.8087\nEpoch 27/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0680 - accuracy: 0.9822 - val_loss: 0.9860 - val_accuracy: 0.7288\nEpoch 28/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0690 - accuracy: 0.9851 - val_loss: 0.7462 - val_accuracy: 0.8038\nEpoch 29/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0863 - accuracy: 0.9788 - val_loss: 0.7641 - val_accuracy: 0.7779\nEpoch 30/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0766 - accuracy: 0.9808 - val_loss: 0.6253 - val_accuracy: 0.8269\nEpoch 31/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0652 - accuracy: 0.9812 - val_loss: 1.3761 - val_accuracy: 0.6394\nEpoch 32/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0753 - accuracy: 0.9793 - val_loss: 0.8236 - val_accuracy: 0.7779\nEpoch 33/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0885 - accuracy: 0.9745 - val_loss: 1.2088 - val_accuracy: 0.7048\nEpoch 34/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.1036 - accuracy: 0.9697 - val_loss: 0.9573 - val_accuracy: 0.7413\nEpoch 35/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0711 - accuracy: 0.9803 - val_loss: 0.7294 - val_accuracy: 0.8048\nEpoch 36/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0304 - accuracy: 0.9957 - val_loss: 0.5367 - val_accuracy: 0.8433\nEpoch 37/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0229 - accuracy: 0.9947 - val_loss: 0.5554 - val_accuracy: 0.8462\nEpoch 38/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0187 - accuracy: 0.9962 - val_loss: 0.6785 - val_accuracy: 0.8202\nEpoch 39/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0165 - accuracy: 0.9971 - val_loss: 0.6487 - val_accuracy: 0.8327\nEpoch 40/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0372 - accuracy: 0.9899 - val_loss: 0.8787 - val_accuracy: 0.7692\nEpoch 41/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0585 - accuracy: 0.9832 - val_loss: 0.8602 - val_accuracy: 0.7875\nEpoch 42/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.1022 - accuracy: 0.9668 - val_loss: 1.7884 - val_accuracy: 0.5894\nEpoch 43/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0860 - accuracy: 0.9750 - val_loss: 0.6995 - val_accuracy: 0.8038\nEpoch 44/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0369 - accuracy: 0.9928 - val_loss: 0.6864 - val_accuracy: 0.8221\nEpoch 45/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0272 - accuracy: 0.9937 - val_loss: 0.6782 - val_accuracy: 0.8183\nEpoch 46/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0816 - accuracy: 0.9716 - val_loss: 0.9734 - val_accuracy: 0.7606\nEpoch 47/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0509 - accuracy: 0.9861 - val_loss: 0.6968 - val_accuracy: 0.8192\nEpoch 48/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0309 - accuracy: 0.9909 - val_loss: 0.6594 - val_accuracy: 0.8192\nEpoch 49/200\n130/130 [==============================] - 23s 173ms/step - loss: 0.0244 - accuracy: 0.9933 - val_loss: 0.9050 - val_accuracy: 0.7567\nEpoch 50/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0215 - accuracy: 0.9947 - val_loss: 0.6740 - val_accuracy: 0.8183\nEpoch 51/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0145 - accuracy: 0.9981 - val_loss: 0.5645 - val_accuracy: 0.8500\nEpoch 52/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0132 - accuracy: 0.9976 - val_loss: 0.7694 - val_accuracy: 0.8144\nEpoch 53/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0099 - accuracy: 0.9986 - val_loss: 0.8948 - val_accuracy: 0.7846\nEpoch 54/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0073 - accuracy: 0.9990 - val_loss: 0.6338 - val_accuracy: 0.8317\nEpoch 55/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0072 - accuracy: 0.9990 - val_loss: 0.6351 - val_accuracy: 0.8231\nEpoch 56/200\n130/130 [==============================] - 22s 172ms/step - loss: 0.0404 - accuracy: 0.9899 - val_loss: 0.6919 - val_accuracy: 0.8115\nEpoch 57/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0465 - accuracy: 0.9841 - val_loss: 1.1913 - val_accuracy: 0.6971\nEpoch 58/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0937 - accuracy: 0.9683 - val_loss: 1.1096 - val_accuracy: 0.7202\nEpoch 59/200\n130/130 [==============================] - 22s 173ms/step - loss: 0.0548 - accuracy: 0.9822 - val_loss: 0.8714 - val_accuracy: 0.7827\nEpoch 60/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0253 - accuracy: 0.9937 - val_loss: 0.6627 - val_accuracy: 0.8221\nEpoch 61/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0190 - accuracy: 0.9962 - val_loss: 0.5869 - val_accuracy: 0.8356\nEpoch 62/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0118 - accuracy: 0.9962 - val_loss: 0.5803 - val_accuracy: 0.8404\nEpoch 63/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0084 - accuracy: 0.9990 - val_loss: 0.6061 - val_accuracy: 0.8356\nEpoch 64/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0106 - accuracy: 0.9981 - val_loss: 0.7575 - val_accuracy: 0.8163\nEpoch 65/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0112 - accuracy: 0.9971 - val_loss: 0.6021 - val_accuracy: 0.8519\nEpoch 66/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0103 - accuracy: 0.9976 - val_loss: 0.7837 - val_accuracy: 0.8135\nEpoch 67/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0084 - accuracy: 0.9986 - val_loss: 0.6589 - val_accuracy: 0.8471\nEpoch 68/200\n130/130 [==============================] - 23s 179ms/step - loss: 0.0236 - accuracy: 0.9942 - val_loss: 0.5358 - val_accuracy: 0.8673\nEpoch 69/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0153 - accuracy: 0.9957 - val_loss: 0.5581 - val_accuracy: 0.8615\nEpoch 70/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.5742 - val_accuracy: 0.8567\nEpoch 71/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0098 - accuracy: 0.9971 - val_loss: 0.8033 - val_accuracy: 0.8010\nEpoch 72/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.5986 - val_accuracy: 0.8510\nEpoch 73/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0036 - accuracy: 0.9995 - val_loss: 0.5261 - val_accuracy: 0.8760\nEpoch 74/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.5092 - val_accuracy: 0.8750\nEpoch 75/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.6264 - val_accuracy: 0.8471\nEpoch 76/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0073 - accuracy: 0.9990 - val_loss: 0.6195 - val_accuracy: 0.8365\nEpoch 77/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0108 - accuracy: 0.9971 - val_loss: 0.6089 - val_accuracy: 0.8385\nEpoch 78/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0107 - accuracy: 0.9981 - val_loss: 0.7509 - val_accuracy: 0.7933\nEpoch 79/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0305 - accuracy: 0.9899 - val_loss: 0.5983 - val_accuracy: 0.8529\nEpoch 80/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0312 - accuracy: 0.9923 - val_loss: 0.9530 - val_accuracy: 0.7856\nEpoch 81/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0278 - accuracy: 0.9913 - val_loss: 0.7045 - val_accuracy: 0.8250\nEpoch 82/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0183 - accuracy: 0.9947 - val_loss: 0.6552 - val_accuracy: 0.8317\nEpoch 83/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0128 - accuracy: 0.9966 - val_loss: 0.5947 - val_accuracy: 0.8500\nEpoch 84/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0131 - accuracy: 0.9976 - val_loss: 0.6335 - val_accuracy: 0.8279\nEpoch 85/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0142 - accuracy: 0.9952 - val_loss: 0.6130 - val_accuracy: 0.8490\nEpoch 86/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0072 - accuracy: 0.9986 - val_loss: 0.5710 - val_accuracy: 0.8567\nEpoch 87/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.5074 - val_accuracy: 0.8731\nEpoch 88/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.5510 - val_accuracy: 0.8596\nEpoch 89/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.4736 - val_accuracy: 0.8692\nEpoch 90/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.4688 - val_accuracy: 0.8827\nEpoch 91/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.4793 - val_accuracy: 0.8760\nEpoch 92/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4807 - val_accuracy: 0.8808\nEpoch 93/200\n130/130 [==============================] - 23s 175ms/step - loss: 9.9394e-04 - accuracy: 1.0000 - val_loss: 0.4793 - val_accuracy: 0.8875\nEpoch 94/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.4785 - val_accuracy: 0.8865\nEpoch 95/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4826 - val_accuracy: 0.8798\nEpoch 96/200\n130/130 [==============================] - 23s 176ms/step - loss: 8.8528e-04 - accuracy: 1.0000 - val_loss: 0.4810 - val_accuracy: 0.8837\nEpoch 97/200\n130/130 [==============================] - 23s 175ms/step - loss: 6.7642e-04 - accuracy: 1.0000 - val_loss: 0.4686 - val_accuracy: 0.8837\nEpoch 98/200\n130/130 [==============================] - 23s 174ms/step - loss: 6.0438e-04 - accuracy: 1.0000 - val_loss: 0.4638 - val_accuracy: 0.8808\nEpoch 99/200\n130/130 [==============================] - 23s 175ms/step - loss: 6.9342e-04 - accuracy: 1.0000 - val_loss: 0.4887 - val_accuracy: 0.8837\nEpoch 100/200\n130/130 [==============================] - 23s 176ms/step - loss: 6.0491e-04 - accuracy: 1.0000 - val_loss: 0.4845 - val_accuracy: 0.8837\nEpoch 101/200\n130/130 [==============================] - 23s 176ms/step - loss: 5.4256e-04 - accuracy: 1.0000 - val_loss: 0.4736 - val_accuracy: 0.8817\nEpoch 102/200\n130/130 [==============================] - 23s 174ms/step - loss: 4.7769e-04 - accuracy: 1.0000 - val_loss: 0.4820 - val_accuracy: 0.8827\nEpoch 103/200\n130/130 [==============================] - 23s 176ms/step - loss: 5.2666e-04 - accuracy: 1.0000 - val_loss: 0.4881 - val_accuracy: 0.8837\nEpoch 104/200\n130/130 [==============================] - 23s 177ms/step - loss: 6.0240e-04 - accuracy: 1.0000 - val_loss: 0.4833 - val_accuracy: 0.8875\nEpoch 105/200\n130/130 [==============================] - 23s 174ms/step - loss: 6.4835e-04 - accuracy: 1.0000 - val_loss: 0.4832 - val_accuracy: 0.8817\nEpoch 106/200\n130/130 [==============================] - 23s 175ms/step - loss: 4.1119e-04 - accuracy: 1.0000 - val_loss: 0.4991 - val_accuracy: 0.8837\nEpoch 107/200\n130/130 [==============================] - 23s 176ms/step - loss: 4.1432e-04 - accuracy: 1.0000 - val_loss: 0.4857 - val_accuracy: 0.8827\nEpoch 108/200\n130/130 [==============================] - 23s 174ms/step - loss: 7.0165e-04 - accuracy: 1.0000 - val_loss: 0.6493 - val_accuracy: 0.8510\nEpoch 109/200\n130/130 [==============================] - 23s 176ms/step - loss: 5.8807e-04 - accuracy: 1.0000 - val_loss: 0.4933 - val_accuracy: 0.8788\nEpoch 110/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.6853 - val_accuracy: 0.8365\nEpoch 111/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0087 - accuracy: 0.9976 - val_loss: 0.6993 - val_accuracy: 0.8365\nEpoch 112/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0236 - accuracy: 0.9928 - val_loss: 1.5131 - val_accuracy: 0.6663\nEpoch 113/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0299 - accuracy: 0.9899 - val_loss: 0.8439 - val_accuracy: 0.8096\nEpoch 114/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0126 - accuracy: 0.9962 - val_loss: 0.7307 - val_accuracy: 0.8385\nEpoch 115/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0063 - accuracy: 0.9990 - val_loss: 0.5982 - val_accuracy: 0.8548\nEpoch 116/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.5962 - val_accuracy: 0.8606\nEpoch 117/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0044 - accuracy: 0.9990 - val_loss: 0.5429 - val_accuracy: 0.8731\nEpoch 118/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.6832 - val_accuracy: 0.8423\nEpoch 119/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.5668 - val_accuracy: 0.8654\nEpoch 120/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.5440 - val_accuracy: 0.8769\nEpoch 121/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0022 - accuracy: 0.9990 - val_loss: 0.5260 - val_accuracy: 0.8721\nEpoch 122/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5294 - val_accuracy: 0.8740\nEpoch 123/200\n130/130 [==============================] - 23s 178ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.5073 - val_accuracy: 0.8731\nEpoch 124/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5138 - val_accuracy: 0.8760\nEpoch 125/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0014 - accuracy: 0.9995 - val_loss: 0.5220 - val_accuracy: 0.8673\nEpoch 126/200\n130/130 [==============================] - 23s 174ms/step - loss: 6.9320e-04 - accuracy: 1.0000 - val_loss: 0.5489 - val_accuracy: 0.8692\nEpoch 127/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5120 - val_accuracy: 0.8750\nEpoch 128/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.6263 - val_accuracy: 0.8490\nEpoch 129/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0041 - accuracy: 0.9981 - val_loss: 1.1129 - val_accuracy: 0.7654\nEpoch 130/200\n130/130 [==============================] - 23s 174ms/step - loss: 0.0084 - accuracy: 0.9986 - val_loss: 0.6268 - val_accuracy: 0.8471\nEpoch 131/200\n130/130 [==============================] - 23s 177ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.6562 - val_accuracy: 0.8442\nEpoch 132/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5839 - val_accuracy: 0.8606\nEpoch 133/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.5689 - val_accuracy: 0.8606\nEpoch 134/200\n130/130 [==============================] - 23s 178ms/step - loss: 0.0046 - accuracy: 0.9990 - val_loss: 0.6713 - val_accuracy: 0.8385\nEpoch 135/200\n130/130 [==============================] - 23s 178ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.5611 - val_accuracy: 0.8683\nEpoch 136/200\n130/130 [==============================] - 23s 177ms/step - loss: 6.9971e-04 - accuracy: 1.0000 - val_loss: 0.5546 - val_accuracy: 0.8654\nEpoch 137/200\n130/130 [==============================] - 23s 176ms/step - loss: 9.5878e-04 - accuracy: 1.0000 - val_loss: 0.5615 - val_accuracy: 0.8731\nEpoch 138/200\n130/130 [==============================] - 23s 176ms/step - loss: 7.6894e-04 - accuracy: 1.0000 - val_loss: 0.5581 - val_accuracy: 0.8673\nEpoch 139/200\n130/130 [==============================] - 23s 175ms/step - loss: 7.7296e-04 - accuracy: 1.0000 - val_loss: 0.5603 - val_accuracy: 0.8721\nEpoch 140/200\n130/130 [==============================] - 23s 174ms/step - loss: 8.0298e-04 - accuracy: 1.0000 - val_loss: 0.5661 - val_accuracy: 0.8702\nEpoch 141/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0053 - accuracy: 0.9986 - val_loss: 0.5923 - val_accuracy: 0.8635\nEpoch 142/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0048 - accuracy: 0.9981 - val_loss: 0.6470 - val_accuracy: 0.8596\nEpoch 143/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.5875 - val_accuracy: 0.8577\nEpoch 144/200\n130/130 [==============================] - 23s 175ms/step - loss: 0.0013 - accuracy: 0.9995 - val_loss: 0.5538 - val_accuracy: 0.8683\nEpoch 145/200\n130/130 [==============================] - 23s 176ms/step - loss: 4.4664e-04 - accuracy: 1.0000 - val_loss: 0.5490 - val_accuracy: 0.8644\nEpoch 146/200\n130/130 [==============================] - 23s 175ms/step - loss: 7.3346e-04 - accuracy: 1.0000 - val_loss: 0.5628 - val_accuracy: 0.8663\nEpoch 147/200\n130/130 [==============================] - 23s 175ms/step - loss: 6.3902e-04 - accuracy: 1.0000 - val_loss: 0.5544 - val_accuracy: 0.8692\nEpoch 148/200\n130/130 [==============================] - 23s 175ms/step - loss: 5.0180e-04 - accuracy: 1.0000 - val_loss: 0.5381 - val_accuracy: 0.8683\nEpoch 149/200\n130/130 [==============================] - 23s 174ms/step - loss: 4.6700e-04 - accuracy: 1.0000 - val_loss: 0.5424 - val_accuracy: 0.8712\nEpoch 150/200\n130/130 [==============================] - 23s 175ms/step - loss: 3.6651e-04 - accuracy: 1.0000 - val_loss: 0.5293 - val_accuracy: 0.8702\nEpoch 151/200\n130/130 [==============================] - 23s 175ms/step - loss: 2.9651e-04 - accuracy: 1.0000 - val_loss: 0.5402 - val_accuracy: 0.8779\nEpoch 152/200\n130/130 [==============================] - 23s 177ms/step - loss: 4.6980e-04 - accuracy: 1.0000 - val_loss: 0.5335 - val_accuracy: 0.8731\nEpoch 153/200\n130/130 [==============================] - 23s 175ms/step - loss: 3.2728e-04 - accuracy: 1.0000 - val_loss: 0.5325 - val_accuracy: 0.8673\nEpoch 154/200\n130/130 [==============================] - 23s 175ms/step - loss: 3.9752e-04 - accuracy: 1.0000 - val_loss: 0.5339 - val_accuracy: 0.8721\nEpoch 155/200\n130/130 [==============================] - 23s 175ms/step - loss: 4.6711e-04 - accuracy: 1.0000 - val_loss: 0.5226 - val_accuracy: 0.8750\nEpoch 156/200\n130/130 [==============================] - 23s 175ms/step - loss: 3.5823e-04 - accuracy: 1.0000 - val_loss: 0.6211 - val_accuracy: 0.8644\nEpoch 157/200\n130/130 [==============================] - 23s 174ms/step - loss: 2.9820e-04 - accuracy: 1.0000 - val_loss: 0.5601 - val_accuracy: 0.8740\nEpoch 158/200\n130/130 [==============================] - 23s 174ms/step - loss: 3.0736e-04 - accuracy: 1.0000 - val_loss: 0.5566 - val_accuracy: 0.8702\nEpoch 159/200\n130/130 [==============================] - 23s 176ms/step - loss: 1.9477e-04 - accuracy: 1.0000 - val_loss: 0.5389 - val_accuracy: 0.8740\nEpoch 160/200\n130/130 [==============================] - 23s 177ms/step - loss: 3.4147e-04 - accuracy: 1.0000 - val_loss: 0.5389 - val_accuracy: 0.8788\nEpoch 161/200\n130/130 [==============================] - 23s 177ms/step - loss: 2.5072e-04 - accuracy: 1.0000 - val_loss: 0.5605 - val_accuracy: 0.8769\nEpoch 162/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.6779e-04 - accuracy: 1.0000 - val_loss: 0.5503 - val_accuracy: 0.8769\nEpoch 163/200\n130/130 [==============================] - 23s 178ms/step - loss: 1.8389e-04 - accuracy: 1.0000 - val_loss: 0.5540 - val_accuracy: 0.8740\nEpoch 164/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.8327e-04 - accuracy: 1.0000 - val_loss: 0.5371 - val_accuracy: 0.8788\nEpoch 165/200\n130/130 [==============================] - 23s 174ms/step - loss: 2.0052e-04 - accuracy: 1.0000 - val_loss: 0.5354 - val_accuracy: 0.8779\nEpoch 166/200\n130/130 [==============================] - 23s 175ms/step - loss: 2.0032e-04 - accuracy: 1.0000 - val_loss: 0.5377 - val_accuracy: 0.8788\nEpoch 167/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.8091e-04 - accuracy: 1.0000 - val_loss: 0.5365 - val_accuracy: 0.8817\nEpoch 168/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.4596e-04 - accuracy: 1.0000 - val_loss: 0.5421 - val_accuracy: 0.8817\nEpoch 169/200\n130/130 [==============================] - 23s 173ms/step - loss: 1.3767e-04 - accuracy: 1.0000 - val_loss: 0.5516 - val_accuracy: 0.8827\nEpoch 170/200\n130/130 [==============================] - 23s 177ms/step - loss: 1.7793e-04 - accuracy: 1.0000 - val_loss: 0.5555 - val_accuracy: 0.8798\nEpoch 171/200\n130/130 [==============================] - 23s 176ms/step - loss: 4.3967e-04 - accuracy: 1.0000 - val_loss: 0.5783 - val_accuracy: 0.8827\nEpoch 172/200\n130/130 [==============================] - 23s 173ms/step - loss: 2.1820e-04 - accuracy: 1.0000 - val_loss: 0.5747 - val_accuracy: 0.8808\nEpoch 173/200\n130/130 [==============================] - 23s 177ms/step - loss: 1.7284e-04 - accuracy: 1.0000 - val_loss: 0.5505 - val_accuracy: 0.8865\nEpoch 174/200\n130/130 [==============================] - 23s 177ms/step - loss: 1.7623e-04 - accuracy: 1.0000 - val_loss: 0.5528 - val_accuracy: 0.8827\nEpoch 175/200\n130/130 [==============================] - 23s 176ms/step - loss: 1.6691e-04 - accuracy: 1.0000 - val_loss: 0.5522 - val_accuracy: 0.8827\nEpoch 176/200\n130/130 [==============================] - 23s 173ms/step - loss: 1.7588e-04 - accuracy: 1.0000 - val_loss: 0.5283 - val_accuracy: 0.8856\nEpoch 177/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.7933e-04 - accuracy: 1.0000 - val_loss: 0.5577 - val_accuracy: 0.8817\nEpoch 178/200\n130/130 [==============================] - 23s 174ms/step - loss: 1.3477e-04 - accuracy: 1.0000 - val_loss: 0.5568 - val_accuracy: 0.8788\nEpoch 179/200\n130/130 [==============================] - 23s 175ms/step - loss: 9.9688e-05 - accuracy: 1.0000 - val_loss: 0.5555 - val_accuracy: 0.8827\nEpoch 180/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.2388e-04 - accuracy: 1.0000 - val_loss: 0.5556 - val_accuracy: 0.8846\nEpoch 181/200\n130/130 [==============================] - 23s 178ms/step - loss: 9.8544e-05 - accuracy: 1.0000 - val_loss: 0.5405 - val_accuracy: 0.8808\nEpoch 182/200\n130/130 [==============================] - 23s 176ms/step - loss: 7.5794e-05 - accuracy: 1.0000 - val_loss: 0.5365 - val_accuracy: 0.8904\nEpoch 183/200\n130/130 [==============================] - 23s 176ms/step - loss: 1.7278e-04 - accuracy: 1.0000 - val_loss: 0.5550 - val_accuracy: 0.8808\nEpoch 184/200\n130/130 [==============================] - 23s 174ms/step - loss: 9.4548e-05 - accuracy: 1.0000 - val_loss: 0.5530 - val_accuracy: 0.8798\nEpoch 185/200\n130/130 [==============================] - 23s 174ms/step - loss: 8.3943e-05 - accuracy: 1.0000 - val_loss: 0.5610 - val_accuracy: 0.8760\nEpoch 186/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.0842e-04 - accuracy: 1.0000 - val_loss: 0.5668 - val_accuracy: 0.8779\nEpoch 187/200\n130/130 [==============================] - 23s 176ms/step - loss: 8.0599e-04 - accuracy: 0.9995 - val_loss: 0.5955 - val_accuracy: 0.8769\nEpoch 188/200\n130/130 [==============================] - 23s 176ms/step - loss: 0.0040 - accuracy: 0.9995 - val_loss: 0.5334 - val_accuracy: 0.8817\nEpoch 189/200\n130/130 [==============================] - 23s 176ms/step - loss: 4.3923e-04 - accuracy: 1.0000 - val_loss: 0.5552 - val_accuracy: 0.8731\nEpoch 190/200\n130/130 [==============================] - 23s 175ms/step - loss: 2.1747e-04 - accuracy: 1.0000 - val_loss: 0.5387 - val_accuracy: 0.8817\nEpoch 191/200\n130/130 [==============================] - 23s 178ms/step - loss: 3.3101e-04 - accuracy: 1.0000 - val_loss: 0.5320 - val_accuracy: 0.8856\nEpoch 192/200\n130/130 [==============================] - 23s 175ms/step - loss: 1.5068e-04 - accuracy: 1.0000 - val_loss: 0.5351 - val_accuracy: 0.8865\nEpoch 193/200\n130/130 [==============================] - 23s 176ms/step - loss: 1.5128e-04 - accuracy: 1.0000 - val_loss: 0.5301 - val_accuracy: 0.8856\nEpoch 194/200\n130/130 [==============================] - 23s 176ms/step - loss: 1.3944e-04 - accuracy: 1.0000 - val_loss: 0.5362 - val_accuracy: 0.8827\nEpoch 195/200\n130/130 [==============================] - 23s 176ms/step - loss: 2.9568e-04 - accuracy: 1.0000 - val_loss: 0.5462 - val_accuracy: 0.8779\nEpoch 196/200\n130/130 [==============================] - 23s 175ms/step - loss: 2.2336e-04 - accuracy: 1.0000 - val_loss: 0.5659 - val_accuracy: 0.8798\nEpoch 197/200\n130/130 [==============================] - 22s 173ms/step - loss: 3.0261e-04 - accuracy: 1.0000 - val_loss: 0.5359 - val_accuracy: 0.8865\nEpoch 198/200\n130/130 [==============================] - 23s 174ms/step - loss: 1.0319e-04 - accuracy: 1.0000 - val_loss: 0.5274 - val_accuracy: 0.8846\nEpoch 199/200\n130/130 [==============================] - 23s 175ms/step - loss: 8.7841e-05 - accuracy: 1.0000 - val_loss: 0.5446 - val_accuracy: 0.8846\nEpoch 200/200\n130/130 [==============================] - 23s 176ms/step - loss: 8.9962e-05 - accuracy: 1.0000 - val_loss: 0.5457 - val_accuracy: 0.8808\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = []  # True labels for the test data\npredicted_labels = []  # Predicted labels for the test data\n\nfor batch_data, batch_labels in test_dataset:\n    batch_predictions = model.predict(batch_data)\n    batch_predicted_labels = np.argmax(batch_predictions, axis=1)\n    predicted_labels.extend(batch_predicted_labels)\n    true_labels.extend(np.argmax(batch_labels, axis=1))\n\ntrue_labels = np.array(true_labels)\npredicted_labels = np.array(predicted_labels)\n\naccuracy = np.mean(true_labels == predicted_labels)\nprint(true_labels)\nprint(predicted_labels)\nprint(accuracy)","metadata":{"papermill":{"duration":12.813259,"end_time":"2023-08-06T22:53:46.633707","exception":false,"start_time":"2023-08-06T22:53:33.820448","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-09T19:49:01.741072Z","iopub.execute_input":"2023-08-09T19:49:01.741443Z","iopub.status.idle":"2023-08-09T19:49:16.367349Z","shell.execute_reply.started":"2023-08-09T19:49:01.741414Z","shell.execute_reply":"2023-08-09T19:49:16.366085Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 71ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 105ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 70ms/step\n1/1 [==============================] - 0s 69ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 60ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 72ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 69ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 73ms/step\n1/1 [==============================] - 0s 65ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 73ms/step\n1/1 [==============================] - 0s 157ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 67ms/step\n1/1 [==============================] - 0s 63ms/step\n1/1 [==============================] - 0s 64ms/step\n1/1 [==============================] - 0s 66ms/step\n1/1 [==============================] - 0s 72ms/step\n1/1 [==============================] - 0s 68ms/step\n1/1 [==============================] - 0s 63ms/step\n[51 30 39 ... 11  9 13]\n[51 29 38 ... 11  9 13]\n0.8807692307692307\n","output_type":"stream"}]}]}