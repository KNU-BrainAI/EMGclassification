{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-24T11:26:37.001006Z","iopub.execute_input":"2023-07-24T11:26:37.001411Z","iopub.status.idle":"2023-07-24T11:26:37.026364Z","shell.execute_reply.started":"2023-07-24T11:26:37.001377Z","shell.execute_reply":"2023-07-24T11:26:37.025346Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/newninaprodb4/ninaprodb44test.pkl\n/kaggle/input/newninaprodb4/ninaprodb44train.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\n#from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport math","metadata":{"execution":{"iopub.status.busy":"2023-07-24T11:26:37.029111Z","iopub.execute_input":"2023-07-24T11:26:37.029535Z","iopub.status.idle":"2023-07-24T11:26:53.006946Z","shell.execute_reply.started":"2023-07-24T11:26:37.029503Z","shell.execute_reply":"2023-07-24T11:26:53.005963Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-07-24T11:26:53.008314Z","iopub.execute_input":"2023-07-24T11:26:53.009061Z","iopub.status.idle":"2023-07-24T11:26:53.713164Z","shell.execute_reply.started":"2023-07-24T11:26:53.009025Z","shell.execute_reply":"2023-07-24T11:26:53.712101Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers, Sequential, optimizers, Input, Model","metadata":{"execution":{"iopub.status.busy":"2023-07-24T11:26:53.716173Z","iopub.execute_input":"2023-07-24T11:26:53.716643Z","iopub.status.idle":"2023-07-24T11:26:53.721801Z","shell.execute_reply.started":"2023-07-24T11:26:53.716607Z","shell.execute_reply":"2023-07-24T11:26:53.720672Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"**WORKED CODE HERE**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\nimport gc\nimport random\nimport os\n\nimport matplotlib.pyplot as plt\nimport json","metadata":{"execution":{"iopub.status.busy":"2023-07-24T11:26:53.723473Z","iopub.execute_input":"2023-07-24T11:26:53.724121Z","iopub.status.idle":"2023-07-24T11:26:53.736182Z","shell.execute_reply.started":"2023-07-24T11:26:53.724085Z","shell.execute_reply":"2023-07-24T11:26:53.735234Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"epochs = 64\nlearning_rate = 1e-3\nbatch_size = 16\nmethod = \"default\"\ndataset_type = 1","metadata":{"execution":{"iopub.status.busy":"2023-07-24T11:26:53.737583Z","iopub.execute_input":"2023-07-24T11:26:53.738168Z","iopub.status.idle":"2023-07-24T11:26:53.747079Z","shell.execute_reply.started":"2023-07-24T11:26:53.738135Z","shell.execute_reply":"2023-07-24T11:26:53.745885Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pywt  # Added import for wavelet denoising\nimport scipy.signal as signal  # Added import for power line noise and low pass filtering\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nimport pywt\nimport scipy.signal as signal\nimport tensorflow as tf\nclass Nina1Dataset(tf.keras.utils.Sequence):\n    def __init__(self, dataframe, batch_size):\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.scaler = StandardScaler()\n        self.scaler.fit(np.concatenate(self.dataframe['emg'].tolist()))\n        \n        \n    def __len__(self):\n        return int(np.ceil(len(self.dataframe) / self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_data = self.dataframe[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        batch_input_data = []\n        batch_labels = []\n\n        for i, target_row in batch_data.iterrows():\n            data = target_row['emg'][:10000]\n\n            # Zero-Padding\n            if len(data) < 10000:\n                data = np.concatenate((data, np.zeros((10000 - len(data), 12))), axis=0)\n\n            \n            \n            \n            fs = 2000  # Assuming sampling frequency of 2000 Hz\n            f0 = 50  # Power line frequency\n            Q = 30  # Quality factor\n            w0 = f0 / (fs / 2)\n            b, a = signal.iirnotch(w0, Q)\n            data = signal.lfilter(b, a, data, axis=0)\n\n            # Low Pass Filtering with cutoff frequency fc = 500 Hz\n            fc = 500  # Cutoff frequency\n            b, a = signal.butter(4, fc / (fs / 2), 'low')\n            data = signal.lfilter(b, a, data, axis=0)\n            \n            \n            coeffs = pywt.wavedec(data, 'sym8')  # Using 3 decomposition levels as an example\n            coeffs[1:] = (pywt.threshold(c, value=0.5, mode='soft') for c in coeffs[1:])\n            data = pywt.waverec(coeffs, 'sym8')\n            \n\n            # Wavelet Denoising\n            \n            \n            \n\n            # Standardize the data\n            data = self.scaler.transform(data)\n\n            # Division data by time-segment (channel-wise)\n            input_data = data.reshape((25, 400, 12))\n            label = target_row['stimulus']\n            batch_input_data.append(input_data)\n            batch_labels.append(label)\n\n        # Check if the batch size is smaller than the desired batch_size\n        if len(batch_data) < self.batch_size:\n            # Create a dummy batch with all elements set to zero\n            dummy_input_data = np.zeros((self.batch_size,) + input_data.shape, dtype=np.float32)\n            dummy_labels = np.zeros((self.batch_size,), dtype=np.int32)\n            dummy_input_data[:len(batch_input_data)] = np.array(batch_input_data)\n            dummy_labels[:len(batch_labels)] = np.array(batch_labels)\n            dummy_labels = to_categorical(dummy_labels, num_classes=52)  # Convert labels to one-hot encoding\n\n            return dummy_input_data, dummy_labels\n\n        batch_labels = to_categorical(batch_labels, num_classes=52)  # Convert labels to one-hot encoding\n\n        return np.array(batch_input_data), np.array(batch_labels)\n\n# Parameters\n\nbatch_size = 16\ntrain_dir = '/kaggle/input/newninaprodb4/ninaprodb44train.pkl'\ntest_dir = '/kaggle/input/newninaprodb4/ninaprodb44test.pkl'\n\n# Set up dataset\ntrain = pd.read_pickle(train_dir)\neval_data = pd.read_pickle(test_dir)\n\n# Load train data\ntrain_data = pd.read_pickle(train_dir)\n\n# Shuffle data\ntrain_data = train_data.sample(frac=1).reset_index(drop=True)\neval_data = eval_data.sample(frac=1).reset_index(drop=True)\n#train_data, val_data = train_test_split(train_data, test_size=0.3, random_state=21)\n# Create train and test datasets\ntrain_dataset = Nina1Dataset(train_data, batch_size=batch_size)\n#val_dataset = Nina1Dataset(val_data, batch_size=batch_size)\ntest_dataset = Nina1Dataset(eval_data, batch_size=batch_size)\n\nprint(train_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-24T11:26:53.748457Z","iopub.execute_input":"2023-07-24T11:26:53.748818Z","iopub.status.idle":"2023-07-24T11:27:19.736053Z","shell.execute_reply.started":"2023-07-24T11:26:53.748784Z","shell.execute_reply":"2023-07-24T11:27:19.734936Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"<__main__.Nina1Dataset object at 0x7da9b556c4c0>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_data['stimulus'])","metadata":{"execution":{"iopub.status.busy":"2023-07-24T11:27:19.737657Z","iopub.execute_input":"2023-07-24T11:27:19.738033Z","iopub.status.idle":"2023-07-24T11:27:19.748376Z","shell.execute_reply.started":"2023-07-24T11:27:19.738000Z","shell.execute_reply":"2023-07-24T11:27:19.746170Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"0       46\n1       21\n2       29\n3       27\n4       40\n        ..\n2045    47\n2046     3\n2047     2\n2048    36\n2049    51\nName: stimulus, Length: 2050, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras import regularizers, initializers\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\n# Define the CNN-BiLSTM model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.layers import TimeDistributed\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\n# Define the CNN-BiLSTM model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import CosineDecayRestarts\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras import regularizers, initializers\ndef cnn(x):\n    #print(x.shape)\n    #x = Reshape((25, 20,10))(x)  \n    print(x.shape)\n    x = TimeDistributed((Conv1D(64, kernel_size=9, strides=2, padding='same', activation=tanh)))(x)\n    print(x.shape)\n    #(Batch, 10, 64)\n\n    x = TimeDistributed(MaxPooling1D(pool_size=8, strides=2))(x)\n    print(x.shape)\n    #(Batch, 2, 64)\n    \n    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n    #(Batch, 1, 64)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    \n    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n    print(x.shape)\n    #(Batch, 1, 64)\n    #x = TimeDistributed(Dropout(0.2093))(x)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    \n    #x = TimeDistributed(ReLU())(Conv1D(64, kernel_size=3, strides=2, padding='same')(x))\n    x = TimeDistributed((Conv1D(64, kernel_size=3, strides=2, padding='same', activation=tanh)))(x)\n    #print(x.shape)\n    #(Batch, 1, 64)\n    #x = TimeDistributed(Dropout(0.2093))(x)\n    #x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(Flatten())(x)\n    print(x.shape)\n    # (Batch, 64)\n\n    return x\n\ndef Bi_LSTMModel(input_shape,x):\n    #model = Sequential()\n    # Hidden dimensions\n    hidden_dim = 200\n    print(x.shape)\n    print(11)\n    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.2093), input_shape=input_shape)(x)\n    #x = Dropout(0.2093)(x)\n    print(x.shape)\n    print(11)\n    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.2093))(x)\n\n    print(x.shape)\n    print(11)\n    #x = Dropout(0.2093)(x)\n    x = Flatten()(x)\n    # ( ,10000)\n\n    return x\ndef EMGHandNet(input_shape, num_classes):\n    # Define the input layer\n    x = Input(shape=input_shape)\n    inputs = x\n    #print(x.shape)\n    #(batch, 25, 20, 10)\n    #temp = [cnn(x[:, t, :, :]) for t in range(x.shape[1])]\n    #x = tf.stack(temp, axis=1)\n    #print(x.shape)\n    x = cnn(x)\n   \n    #print(x.shape)\n    x = Bi_LSTMModel(x.shape[1:],x)\n    #print(x.shape)\n    #x = Dropout(0.2093)(x)\n    \n    x = Dense(512, activation='tanh')(x)\n    #print(x.shape)\n    x = Dropout(0.2093)(x)\n\n    # Add the output layer\n    output_layer = Dense(52, activation='softmax')(x)\n    # Create the model\n    model = Model(inputs=inputs, outputs=output_layer)\n\n    return model\n\nnum_classes = 52\nmodel = EMGHandNet((25, 400, 12), num_classes)\n\n\ninitial_learning_rate = 0.001\ndecay_steps = 1000\ndecay_rate = 0.9\nbatch_size = 16\n# Define your model and its optimizer\n# Define learning rate schedule\nlr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n\n    # Compile the model with learning rate schedule\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999),\n                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n                metrics=['accuracy'])\n\n\n# Define your model and its optimizer\n# Define learning rate schedule\n\n\nhistory = model.fit(train_dataset,\n                    epochs=200,\n                    batch_size=16)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-24T11:27:19.749783Z","iopub.execute_input":"2023-07-24T11:27:19.750084Z","iopub.status.idle":"2023-07-24T12:06:35.986269Z","shell.execute_reply.started":"2023-07-24T11:27:19.750048Z","shell.execute_reply":"2023-07-24T12:06:35.985181Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"(None, 25, 400, 12)\n(None, 25, 200, 64)\n(None, 25, 97, 64)\n(None, 25, 25, 64)\n(None, 25, 832)\n(None, 25, 832)\n11\n(None, 25, 400)\n11\n(None, 25, 400)\n11\nEpoch 1/200\n129/129 [==============================] - 37s 86ms/step - loss: 3.8337 - accuracy: 0.1037\nEpoch 2/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.8065 - accuracy: 0.2548\nEpoch 3/200\n129/129 [==============================] - 11s 86ms/step - loss: 2.3410 - accuracy: 0.3304\nEpoch 4/200\n129/129 [==============================] - 11s 81ms/step - loss: 1.8616 - accuracy: 0.4414\nEpoch 5/200\n129/129 [==============================] - 11s 83ms/step - loss: 1.5232 - accuracy: 0.5237\nEpoch 6/200\n129/129 [==============================] - 11s 86ms/step - loss: 1.2364 - accuracy: 0.6080\nEpoch 7/200\n129/129 [==============================] - 11s 81ms/step - loss: 0.8197 - accuracy: 0.7403\nEpoch 8/200\n129/129 [==============================] - 10s 81ms/step - loss: 0.5157 - accuracy: 0.8343\nEpoch 9/200\n129/129 [==============================] - 12s 89ms/step - loss: 0.3312 - accuracy: 0.9041\nEpoch 10/200\n129/129 [==============================] - 10s 81ms/step - loss: 0.1512 - accuracy: 0.9622\nEpoch 11/200\n129/129 [==============================] - 10s 81ms/step - loss: 0.0852 - accuracy: 0.9777\nEpoch 12/200\n129/129 [==============================] - 11s 86ms/step - loss: 0.0293 - accuracy: 0.9976\nEpoch 13/200\n129/129 [==============================] - 11s 82ms/step - loss: 0.0111 - accuracy: 1.0000\nEpoch 14/200\n129/129 [==============================] - 11s 85ms/step - loss: 0.0066 - accuracy: 1.0000\nEpoch 15/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0051 - accuracy: 1.0000\nEpoch 16/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0039 - accuracy: 1.0000\nEpoch 17/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0032 - accuracy: 1.0000\nEpoch 18/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0029 - accuracy: 1.0000\nEpoch 19/200\n129/129 [==============================] - 10s 81ms/step - loss: 0.0026 - accuracy: 1.0000\nEpoch 20/200\n129/129 [==============================] - 10s 79ms/step - loss: 0.0022 - accuracy: 1.0000\nEpoch 21/200\n129/129 [==============================] - 11s 82ms/step - loss: 0.0019 - accuracy: 1.0000\nEpoch 22/200\n129/129 [==============================] - 11s 81ms/step - loss: 0.0017 - accuracy: 1.0000\nEpoch 23/200\n129/129 [==============================] - 11s 86ms/step - loss: 0.0016 - accuracy: 1.0000\nEpoch 24/200\n129/129 [==============================] - 10s 81ms/step - loss: 0.0014 - accuracy: 1.0000\nEpoch 25/200\n129/129 [==============================] - 10s 81ms/step - loss: 0.0013 - accuracy: 1.0000\nEpoch 26/200\n129/129 [==============================] - 11s 85ms/step - loss: 0.0012 - accuracy: 1.0000\nEpoch 27/200\n129/129 [==============================] - 10s 80ms/step - loss: 0.0011 - accuracy: 1.0000\nEpoch 28/200\n129/129 [==============================] - 10s 81ms/step - loss: 9.8470e-04 - accuracy: 1.0000\nEpoch 29/200\n129/129 [==============================] - 10s 80ms/step - loss: 9.2316e-04 - accuracy: 1.0000\nEpoch 30/200\n129/129 [==============================] - 10s 80ms/step - loss: 8.2494e-04 - accuracy: 1.0000\nEpoch 31/200\n129/129 [==============================] - 11s 85ms/step - loss: 8.1108e-04 - accuracy: 1.0000\nEpoch 32/200\n129/129 [==============================] - 10s 81ms/step - loss: 7.5687e-04 - accuracy: 1.0000\nEpoch 33/200\n129/129 [==============================] - 11s 82ms/step - loss: 6.6196e-04 - accuracy: 1.0000\nEpoch 34/200\n129/129 [==============================] - 11s 85ms/step - loss: 6.2679e-04 - accuracy: 1.0000\nEpoch 35/200\n129/129 [==============================] - 10s 78ms/step - loss: 5.6866e-04 - accuracy: 1.0000\nEpoch 36/200\n129/129 [==============================] - 11s 85ms/step - loss: 5.5025e-04 - accuracy: 1.0000\nEpoch 37/200\n129/129 [==============================] - 10s 79ms/step - loss: 5.0400e-04 - accuracy: 1.0000\nEpoch 38/200\n129/129 [==============================] - 10s 79ms/step - loss: 4.6602e-04 - accuracy: 1.0000\nEpoch 39/200\n129/129 [==============================] - 11s 84ms/step - loss: 4.3280e-04 - accuracy: 1.0000\nEpoch 40/200\n129/129 [==============================] - 10s 79ms/step - loss: 4.3326e-04 - accuracy: 1.0000\nEpoch 41/200\n129/129 [==============================] - 10s 79ms/step - loss: 3.9151e-04 - accuracy: 1.0000\nEpoch 42/200\n129/129 [==============================] - 11s 84ms/step - loss: 3.7532e-04 - accuracy: 1.0000\nEpoch 43/200\n129/129 [==============================] - 10s 80ms/step - loss: 3.4675e-04 - accuracy: 1.0000\nEpoch 44/200\n129/129 [==============================] - 11s 84ms/step - loss: 3.3491e-04 - accuracy: 1.0000\nEpoch 45/200\n129/129 [==============================] - 10s 80ms/step - loss: 3.0139e-04 - accuracy: 1.0000\nEpoch 46/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.8219e-04 - accuracy: 1.0000\nEpoch 47/200\n129/129 [==============================] - 11s 84ms/step - loss: 2.5410e-04 - accuracy: 1.0000\nEpoch 48/200\n129/129 [==============================] - 11s 81ms/step - loss: 2.5276e-04 - accuracy: 1.0000\nEpoch 49/200\n129/129 [==============================] - 10s 79ms/step - loss: 2.3317e-04 - accuracy: 1.0000\nEpoch 50/200\n129/129 [==============================] - 11s 84ms/step - loss: 2.1895e-04 - accuracy: 1.0000\nEpoch 51/200\n129/129 [==============================] - 10s 79ms/step - loss: 2.1661e-04 - accuracy: 1.0000\nEpoch 52/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.9251e-04 - accuracy: 1.0000\nEpoch 53/200\n129/129 [==============================] - 11s 83ms/step - loss: 1.7855e-04 - accuracy: 1.0000\nEpoch 54/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.7812e-04 - accuracy: 1.0000\nEpoch 55/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.5936e-04 - accuracy: 1.0000\nEpoch 56/200\n129/129 [==============================] - 11s 82ms/step - loss: 1.5187e-04 - accuracy: 1.0000\nEpoch 57/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.4668e-04 - accuracy: 1.0000\nEpoch 58/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.3128e-04 - accuracy: 1.0000\nEpoch 59/200\n129/129 [==============================] - 11s 83ms/step - loss: 1.2348e-04 - accuracy: 1.0000\nEpoch 60/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.1784e-04 - accuracy: 1.0000\nEpoch 61/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.1465e-04 - accuracy: 1.0000\nEpoch 62/200\n129/129 [==============================] - 11s 84ms/step - loss: 1.0451e-04 - accuracy: 1.0000\nEpoch 63/200\n129/129 [==============================] - 10s 80ms/step - loss: 9.9321e-05 - accuracy: 1.0000\nEpoch 64/200\n129/129 [==============================] - 10s 79ms/step - loss: 9.3224e-05 - accuracy: 1.0000\nEpoch 65/200\n129/129 [==============================] - 11s 86ms/step - loss: 8.7881e-05 - accuracy: 1.0000\nEpoch 66/200\n129/129 [==============================] - 10s 79ms/step - loss: 8.5490e-05 - accuracy: 1.0000\nEpoch 67/200\n129/129 [==============================] - 10s 80ms/step - loss: 7.7633e-05 - accuracy: 1.0000\nEpoch 68/200\n129/129 [==============================] - 11s 85ms/step - loss: 7.2928e-05 - accuracy: 1.0000\nEpoch 69/200\n129/129 [==============================] - 10s 80ms/step - loss: 7.1775e-05 - accuracy: 1.0000\nEpoch 70/200\n129/129 [==============================] - 11s 84ms/step - loss: 6.8675e-05 - accuracy: 1.0000\nEpoch 71/200\n129/129 [==============================] - 10s 80ms/step - loss: 6.2805e-05 - accuracy: 1.0000\nEpoch 72/200\n129/129 [==============================] - 10s 80ms/step - loss: 5.8591e-05 - accuracy: 1.0000\nEpoch 73/200\n129/129 [==============================] - 11s 86ms/step - loss: 5.8642e-05 - accuracy: 1.0000\nEpoch 74/200\n129/129 [==============================] - 10s 81ms/step - loss: 5.4802e-05 - accuracy: 1.0000\nEpoch 75/200\n129/129 [==============================] - 10s 81ms/step - loss: 5.2148e-05 - accuracy: 1.0000\nEpoch 76/200\n129/129 [==============================] - 11s 85ms/step - loss: 5.0943e-05 - accuracy: 1.0000\nEpoch 77/200\n129/129 [==============================] - 10s 80ms/step - loss: 4.2196e-05 - accuracy: 1.0000\nEpoch 78/200\n129/129 [==============================] - 10s 80ms/step - loss: 4.3881e-05 - accuracy: 1.0000\nEpoch 79/200\n129/129 [==============================] - 11s 86ms/step - loss: 4.0918e-05 - accuracy: 1.0000\nEpoch 80/200\n129/129 [==============================] - 10s 80ms/step - loss: 3.9804e-05 - accuracy: 1.0000\nEpoch 81/200\n129/129 [==============================] - 10s 80ms/step - loss: 3.6342e-05 - accuracy: 1.0000\nEpoch 82/200\n129/129 [==============================] - 11s 85ms/step - loss: 3.3984e-05 - accuracy: 1.0000\nEpoch 83/200\n129/129 [==============================] - 10s 81ms/step - loss: 3.2417e-05 - accuracy: 1.0000\nEpoch 84/200\n129/129 [==============================] - 10s 80ms/step - loss: 3.2700e-05 - accuracy: 1.0000\nEpoch 85/200\n129/129 [==============================] - 11s 85ms/step - loss: 2.8499e-05 - accuracy: 1.0000\nEpoch 86/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.8055e-05 - accuracy: 1.0000\nEpoch 87/200\n129/129 [==============================] - 11s 83ms/step - loss: 2.6376e-05 - accuracy: 1.0000\nEpoch 88/200\n129/129 [==============================] - 11s 85ms/step - loss: 2.3785e-05 - accuracy: 1.0000\nEpoch 89/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.2526e-05 - accuracy: 1.0000\nEpoch 90/200\n129/129 [==============================] - 10s 81ms/step - loss: 2.1517e-05 - accuracy: 1.0000\nEpoch 91/200\n129/129 [==============================] - 11s 84ms/step - loss: 2.1507e-05 - accuracy: 1.0000\nEpoch 92/200\n129/129 [==============================] - 10s 81ms/step - loss: 1.9492e-05 - accuracy: 1.0000\nEpoch 93/200\n129/129 [==============================] - 11s 85ms/step - loss: 1.9247e-05 - accuracy: 1.0000\nEpoch 94/200\n129/129 [==============================] - 10s 80ms/step - loss: 1.7480e-05 - accuracy: 1.0000\nEpoch 95/200\n129/129 [==============================] - 10s 81ms/step - loss: 1.6332e-05 - accuracy: 1.0000\nEpoch 96/200\n129/129 [==============================] - 11s 85ms/step - loss: 1.5330e-05 - accuracy: 1.0000\nEpoch 97/200\n129/129 [==============================] - 10s 80ms/step - loss: 1.5383e-05 - accuracy: 1.0000\nEpoch 98/200\n129/129 [==============================] - 10s 81ms/step - loss: 1.3840e-05 - accuracy: 1.0000\nEpoch 99/200\n129/129 [==============================] - 11s 85ms/step - loss: 1.3564e-05 - accuracy: 1.0000\nEpoch 100/200\n129/129 [==============================] - 10s 80ms/step - loss: 1.3567e-05 - accuracy: 1.0000\nEpoch 101/200\n129/129 [==============================] - 10s 80ms/step - loss: 1.2619e-05 - accuracy: 1.0000\nEpoch 102/200\n129/129 [==============================] - 11s 85ms/step - loss: 1.1986e-05 - accuracy: 1.0000\nEpoch 103/200\n129/129 [==============================] - 11s 82ms/step - loss: 1.0815e-05 - accuracy: 1.0000\nEpoch 104/200\n129/129 [==============================] - 11s 86ms/step - loss: 1.0425e-05 - accuracy: 1.0000\nEpoch 105/200\n129/129 [==============================] - 10s 81ms/step - loss: 9.7094e-06 - accuracy: 1.0000\nEpoch 106/200\n129/129 [==============================] - 10s 81ms/step - loss: 9.1651e-06 - accuracy: 1.0000\nEpoch 107/200\n129/129 [==============================] - 11s 85ms/step - loss: 8.5703e-06 - accuracy: 1.0000\nEpoch 108/200\n129/129 [==============================] - 10s 81ms/step - loss: 8.0936e-06 - accuracy: 1.0000\nEpoch 109/200\n129/129 [==============================] - 11s 86ms/step - loss: 8.1863e-06 - accuracy: 1.0000\nEpoch 110/200\n129/129 [==============================] - 10s 81ms/step - loss: 7.2710e-06 - accuracy: 1.0000\nEpoch 111/200\n129/129 [==============================] - 10s 80ms/step - loss: 7.4706e-06 - accuracy: 1.0000\nEpoch 112/200\n129/129 [==============================] - 11s 85ms/step - loss: 6.6110e-06 - accuracy: 1.0000\nEpoch 113/200\n129/129 [==============================] - 10s 80ms/step - loss: 6.4377e-06 - accuracy: 1.0000\nEpoch 114/200\n129/129 [==============================] - 10s 80ms/step - loss: 5.9396e-06 - accuracy: 1.0000\nEpoch 115/200\n129/129 [==============================] - 10s 80ms/step - loss: 5.8387e-06 - accuracy: 1.0000\nEpoch 116/200\n129/129 [==============================] - 11s 84ms/step - loss: 5.3099e-06 - accuracy: 1.0000\nEpoch 117/200\n129/129 [==============================] - 10s 79ms/step - loss: 5.6168e-06 - accuracy: 1.0000\nEpoch 118/200\n129/129 [==============================] - 10s 79ms/step - loss: 5.0793e-06 - accuracy: 1.0000\nEpoch 119/200\n129/129 [==============================] - 11s 85ms/step - loss: 4.6276e-06 - accuracy: 1.0000\nEpoch 120/200\n129/129 [==============================] - 10s 80ms/step - loss: 4.3387e-06 - accuracy: 1.0000\nEpoch 121/200\n129/129 [==============================] - 10s 80ms/step - loss: 4.2253e-06 - accuracy: 1.0000\nEpoch 122/200\n129/129 [==============================] - 11s 84ms/step - loss: 3.8914e-06 - accuracy: 1.0000\nEpoch 123/200\n129/129 [==============================] - 10s 81ms/step - loss: 3.8372e-06 - accuracy: 1.0000\nEpoch 124/200\n129/129 [==============================] - 10s 80ms/step - loss: 3.7642e-06 - accuracy: 1.0000\nEpoch 125/200\n129/129 [==============================] - 11s 85ms/step - loss: 3.4720e-06 - accuracy: 1.0000\nEpoch 126/200\n129/129 [==============================] - 10s 80ms/step - loss: 3.3206e-06 - accuracy: 1.0000\nEpoch 127/200\n129/129 [==============================] - 10s 81ms/step - loss: 3.4708e-06 - accuracy: 1.0000\nEpoch 128/200\n129/129 [==============================] - 11s 86ms/step - loss: 2.7995e-06 - accuracy: 1.0000\nEpoch 129/200\n129/129 [==============================] - 10s 81ms/step - loss: 2.7301e-06 - accuracy: 1.0000\nEpoch 130/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.5370e-06 - accuracy: 1.0000\nEpoch 131/200\n129/129 [==============================] - 11s 85ms/step - loss: 2.4454e-06 - accuracy: 1.0000\nEpoch 132/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.3797e-06 - accuracy: 1.0000\nEpoch 133/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.1871e-06 - accuracy: 1.0000\nEpoch 134/200\n129/129 [==============================] - 11s 84ms/step - loss: 2.2105e-06 - accuracy: 1.0000\nEpoch 135/200\n129/129 [==============================] - 10s 81ms/step - loss: 2.0383e-06 - accuracy: 1.0000\nEpoch 136/200\n129/129 [==============================] - 10s 80ms/step - loss: 1.8649e-06 - accuracy: 1.0000\nEpoch 137/200\n129/129 [==============================] - 11s 84ms/step - loss: 2.0059e-06 - accuracy: 1.0000\nEpoch 138/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.8886e-06 - accuracy: 1.0000\nEpoch 139/200\n129/129 [==============================] - 11s 84ms/step - loss: 1.6823e-06 - accuracy: 1.0000\nEpoch 140/200\n129/129 [==============================] - 10s 80ms/step - loss: 1.7394e-06 - accuracy: 1.0000\nEpoch 141/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.5147e-06 - accuracy: 1.0000\nEpoch 142/200\n129/129 [==============================] - 11s 83ms/step - loss: 1.5407e-06 - accuracy: 1.0000\nEpoch 143/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.3633e-06 - accuracy: 1.0000\nEpoch 144/200\n129/129 [==============================] - 11s 81ms/step - loss: 1.3134e-06 - accuracy: 1.0000\nEpoch 145/200\n129/129 [==============================] - 11s 85ms/step - loss: 1.2819e-06 - accuracy: 1.0000\nEpoch 146/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.2503e-06 - accuracy: 1.0000\nEpoch 147/200\n129/129 [==============================] - 11s 84ms/step - loss: 1.2038e-06 - accuracy: 1.0000\nEpoch 148/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.1351e-06 - accuracy: 1.0000\nEpoch 149/200\n129/129 [==============================] - 10s 80ms/step - loss: 1.1100e-06 - accuracy: 1.0000\nEpoch 150/200\n129/129 [==============================] - 11s 85ms/step - loss: 9.6736e-07 - accuracy: 1.0000\nEpoch 151/200\n129/129 [==============================] - 10s 79ms/step - loss: 9.5246e-07 - accuracy: 1.0000\nEpoch 152/200\n129/129 [==============================] - 10s 81ms/step - loss: 9.4045e-07 - accuracy: 1.0000\nEpoch 153/200\n129/129 [==============================] - 10s 80ms/step - loss: 9.2202e-07 - accuracy: 1.0000\nEpoch 154/200\n129/129 [==============================] - 10s 80ms/step - loss: 9.1301e-07 - accuracy: 1.0000\nEpoch 155/200\n129/129 [==============================] - 11s 84ms/step - loss: 8.5064e-07 - accuracy: 1.0000\nEpoch 156/200\n129/129 [==============================] - 10s 80ms/step - loss: 7.8635e-07 - accuracy: 1.0000\nEpoch 157/200\n129/129 [==============================] - 11s 85ms/step - loss: 8.2205e-07 - accuracy: 1.0000\nEpoch 158/200\n129/129 [==============================] - 10s 79ms/step - loss: 7.3166e-07 - accuracy: 1.0000\nEpoch 159/200\n129/129 [==============================] - 10s 79ms/step - loss: 6.8493e-07 - accuracy: 1.0000\nEpoch 160/200\n129/129 [==============================] - 11s 86ms/step - loss: 6.4179e-07 - accuracy: 1.0000\nEpoch 161/200\n129/129 [==============================] - 10s 79ms/step - loss: 6.0662e-07 - accuracy: 1.0000\nEpoch 162/200\n129/129 [==============================] - 10s 80ms/step - loss: 6.0714e-07 - accuracy: 1.0000\nEpoch 163/200\n129/129 [==============================] - 11s 85ms/step - loss: 6.3648e-07 - accuracy: 1.0000\nEpoch 164/200\n129/129 [==============================] - 10s 80ms/step - loss: 5.7208e-07 - accuracy: 1.0000\nEpoch 165/200\n129/129 [==============================] - 10s 81ms/step - loss: 5.2304e-07 - accuracy: 1.0000\nEpoch 166/200\n129/129 [==============================] - 11s 84ms/step - loss: 5.4672e-07 - accuracy: 1.0000\nEpoch 167/200\n129/129 [==============================] - 10s 80ms/step - loss: 4.7828e-07 - accuracy: 1.0000\nEpoch 168/200\n129/129 [==============================] - 10s 81ms/step - loss: 5.4822e-07 - accuracy: 1.0000\nEpoch 169/200\n129/129 [==============================] - 11s 85ms/step - loss: 4.6736e-07 - accuracy: 1.0000\nEpoch 170/200\n129/129 [==============================] - 11s 83ms/step - loss: 4.4571e-07 - accuracy: 1.0000\nEpoch 171/200\n129/129 [==============================] - 10s 81ms/step - loss: 4.4068e-07 - accuracy: 1.0000\nEpoch 172/200\n129/129 [==============================] - 10s 80ms/step - loss: 4.1683e-07 - accuracy: 1.0000\nEpoch 173/200\n129/129 [==============================] - 10s 81ms/step - loss: 3.9991e-07 - accuracy: 1.0000\nEpoch 174/200\n129/129 [==============================] - 11s 84ms/step - loss: 3.8316e-07 - accuracy: 1.0000\nEpoch 175/200\n129/129 [==============================] - 10s 80ms/step - loss: 3.5445e-07 - accuracy: 1.0000\nEpoch 176/200\n129/129 [==============================] - 11s 85ms/step - loss: 4.3034e-07 - accuracy: 1.0000\nEpoch 177/200\n129/129 [==============================] - 11s 82ms/step - loss: 3.6843e-07 - accuracy: 1.0000\nEpoch 178/200\n129/129 [==============================] - 10s 81ms/step - loss: 3.2754e-07 - accuracy: 1.0000\nEpoch 179/200\n129/129 [==============================] - 11s 86ms/step - loss: 3.3331e-07 - accuracy: 1.0000\nEpoch 180/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.9791e-07 - accuracy: 1.0000\nEpoch 181/200\n129/129 [==============================] - 10s 80ms/step - loss: 3.1783e-07 - accuracy: 1.0000\nEpoch 182/200\n129/129 [==============================] - 11s 84ms/step - loss: 2.8243e-07 - accuracy: 1.0000\nEpoch 183/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.9155e-07 - accuracy: 1.0000\nEpoch 184/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.7013e-07 - accuracy: 1.0000\nEpoch 185/200\n129/129 [==============================] - 11s 84ms/step - loss: 2.6747e-07 - accuracy: 1.0000\nEpoch 186/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.3079e-07 - accuracy: 1.0000\nEpoch 187/200\n129/129 [==============================] - 10s 79ms/step - loss: 2.3952e-07 - accuracy: 1.0000\nEpoch 188/200\n129/129 [==============================] - 11s 84ms/step - loss: 2.2958e-07 - accuracy: 1.0000\nEpoch 189/200\n129/129 [==============================] - 10s 80ms/step - loss: 2.1214e-07 - accuracy: 1.0000\nEpoch 190/200\n129/129 [==============================] - 10s 79ms/step - loss: 2.1341e-07 - accuracy: 1.0000\nEpoch 191/200\n129/129 [==============================] - 11s 84ms/step - loss: 2.1041e-07 - accuracy: 1.0000\nEpoch 192/200\n129/129 [==============================] - 10s 80ms/step - loss: 1.8886e-07 - accuracy: 1.0000\nEpoch 193/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.9019e-07 - accuracy: 1.0000\nEpoch 194/200\n129/129 [==============================] - 11s 84ms/step - loss: 2.0781e-07 - accuracy: 1.0000\nEpoch 195/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.7789e-07 - accuracy: 1.0000\nEpoch 196/200\n129/129 [==============================] - 10s 78ms/step - loss: 1.7298e-07 - accuracy: 1.0000\nEpoch 197/200\n129/129 [==============================] - 11s 84ms/step - loss: 1.7177e-07 - accuracy: 1.0000\nEpoch 198/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.6212e-07 - accuracy: 1.0000\nEpoch 199/200\n129/129 [==============================] - 10s 79ms/step - loss: 1.7437e-07 - accuracy: 1.0000\nEpoch 200/200\n129/129 [==============================] - 11s 84ms/step - loss: 1.6536e-07 - accuracy: 1.0000\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = []  # True labels for the test data\npredicted_labels = []  # Predicted labels for the test data\n\nfor batch_data, batch_labels in test_dataset:\n    batch_predictions = model.predict(batch_data)\n    batch_predicted_labels = np.argmax(batch_predictions, axis=1)\n    predicted_labels.extend(batch_predicted_labels)\n    true_labels.extend(np.argmax(batch_labels, axis=1))\n\ntrue_labels = np.array(true_labels)\npredicted_labels = np.array(predicted_labels)\n\naccuracy = np.mean(true_labels == predicted_labels)\nprint(true_labels)\nprint(predicted_labels)\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T12:06:35.989701Z","iopub.execute_input":"2023-07-24T12:06:35.990047Z","iopub.status.idle":"2023-07-24T12:06:46.625156Z","shell.execute_reply.started":"2023-07-24T12:06:35.990018Z","shell.execute_reply":"2023-07-24T12:06:46.624067Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 1s 1s/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 33ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 62ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 31ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 30ms/step\n[ 1 20 50 ... 51 49 26]\n[ 1 20 50 ... 50 49 26]\n0.8\n","output_type":"stream"}]}]}