{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c561e55",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-08-12T08:31:16.534849Z",
     "iopub.status.busy": "2023-08-12T08:31:16.534336Z",
     "iopub.status.idle": "2023-08-12T08:31:16.558944Z",
     "shell.execute_reply": "2023-08-12T08:31:16.557459Z"
    },
    "papermill": {
     "duration": 0.033817,
     "end_time": "2023-08-12T08:31:16.561182",
     "exception": false,
     "start_time": "2023-08-12T08:31:16.527365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/datasetfornina/ninaprodb1train.pkl\n",
      "/kaggle/input/datasetfornina/ninaprodb1test.pkl\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4660c0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T08:31:16.571016Z",
     "iopub.status.busy": "2023-08-12T08:31:16.570728Z",
     "iopub.status.idle": "2023-08-12T08:31:24.751143Z",
     "shell.execute_reply": "2023-08-12T08:31:24.750040Z"
    },
    "papermill": {
     "duration": 8.188215,
     "end_time": "2023-08-12T08:31:24.753838",
     "exception": false,
     "start_time": "2023-08-12T08:31:16.565623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e743f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T08:31:24.765978Z",
     "iopub.status.busy": "2023-08-12T08:31:24.764276Z",
     "iopub.status.idle": "2023-08-12T08:31:25.704203Z",
     "shell.execute_reply": "2023-08-12T08:31:25.702962Z"
    },
    "papermill": {
     "duration": 0.948534,
     "end_time": "2023-08-12T08:31:25.707355",
     "exception": false,
     "start_time": "2023-08-12T08:31:24.758821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c037d605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T08:31:25.719569Z",
     "iopub.status.busy": "2023-08-12T08:31:25.718510Z",
     "iopub.status.idle": "2023-08-12T08:31:25.724053Z",
     "shell.execute_reply": "2023-08-12T08:31:25.723055Z"
    },
    "papermill": {
     "duration": 0.014202,
     "end_time": "2023-08-12T08:31:25.726500",
     "exception": false,
     "start_time": "2023-08-12T08:31:25.712298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential, optimizers, Input, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411aad23",
   "metadata": {
    "papermill": {
     "duration": 0.00428,
     "end_time": "2023-08-12T08:31:25.735176",
     "exception": false,
     "start_time": "2023-08-12T08:31:25.730896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**WORKED CODE HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37bbba02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T08:31:25.745401Z",
     "iopub.status.busy": "2023-08-12T08:31:25.745082Z",
     "iopub.status.idle": "2023-08-12T08:31:25.752917Z",
     "shell.execute_reply": "2023-08-12T08:31:25.751911Z"
    },
    "papermill": {
     "duration": 0.015557,
     "end_time": "2023-08-12T08:31:25.755272",
     "exception": false,
     "start_time": "2023-08-12T08:31:25.739715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7457ea3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T08:31:25.765584Z",
     "iopub.status.busy": "2023-08-12T08:31:25.765167Z",
     "iopub.status.idle": "2023-08-12T08:31:25.770454Z",
     "shell.execute_reply": "2023-08-12T08:31:25.769360Z"
    },
    "papermill": {
     "duration": 0.012764,
     "end_time": "2023-08-12T08:31:25.772560",
     "exception": false,
     "start_time": "2023-08-12T08:31:25.759796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 64\n",
    "learning_rate = 1e-3\n",
    "batch_size = 16\n",
    "method = \"default\"\n",
    "dataset_type = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e60938",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T08:31:25.783184Z",
     "iopub.status.busy": "2023-08-12T08:31:25.782308Z",
     "iopub.status.idle": "2023-08-12T08:31:39.657783Z",
     "shell.execute_reply": "2023-08-12T08:31:39.655911Z"
    },
    "papermill": {
     "duration": 13.883275,
     "end_time": "2023-08-12T08:31:39.660276",
     "exception": false,
     "start_time": "2023-08-12T08:31:25.777001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyWavelets in /opt/conda/lib/python3.10/site-packages (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from PyWavelets) (1.23.5)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyWavelets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00232055",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T08:31:39.671738Z",
     "iopub.status.busy": "2023-08-12T08:31:39.671382Z",
     "iopub.status.idle": "2023-08-12T08:31:47.453577Z",
     "shell.execute_reply": "2023-08-12T08:31:47.451064Z"
    },
    "papermill": {
     "duration": 7.791407,
     "end_time": "2023-08-12T08:31:47.456442",
     "exception": false,
     "start_time": "2023-08-12T08:31:39.665035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Nina1Dataset object at 0x7875f6a67580>\n"
     ]
    }
   ],
   "source": [
    "### import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import pywt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "class Nina1Dataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, dataframe, batch_size):\n",
    "        self.dataframe = dataframe\n",
    "        self.batch_size = batch_size\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(np.concatenate(self.dataframe['emg'].tolist()))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataframe) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_data = self.dataframe[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        batch_input_data = []\n",
    "        batch_labels = []\n",
    "\n",
    "        for i, target_row in batch_data.iterrows():\n",
    "            data = target_row['emg'][:500]\n",
    "\n",
    "            # Zero-Padding\n",
    "            if len(data) < 500:\n",
    "                data = np.concatenate((data, np.zeros((500 - len(data), 10))), axis=0)\n",
    "            coeffs = pywt.wavedec(data, 'sym8')  # Using 3 decomposition levels as an example\n",
    "            coeffs[1:] = (pywt.threshold(c, value=0.5) for c in coeffs[1:])\n",
    "            data = pywt.waverec(coeffs, 'sym8')\n",
    "            data = self.scaler.transform(data)\n",
    "            \n",
    "            # Division data by time-segment\n",
    "            #input_data = np.transpose(data.reshape((25, 20, 10)), (0, 2, 1))\n",
    "            input_data = data.reshape((25, 20, 10))\n",
    "            label = target_row['stimulus']\n",
    "            #label = to_categorical(label)\n",
    "            batch_input_data.append(input_data)\n",
    "            batch_labels.append(label)\n",
    "\n",
    "        # Check if the batch size is smaller than the desired batch_size\n",
    "        if len(batch_data) < self.batch_size:\n",
    "            # Create a dummy batch with all elements set to zero\n",
    "            dummy_input_data = np.zeros((self.batch_size,) + input_data.shape, dtype=np.float32)\n",
    "            dummy_labels = np.zeros((self.batch_size,), dtype=np.int32)\n",
    "            dummy_input_data[:len(batch_input_data)] = np.array(batch_input_data)\n",
    "            dummy_labels[:len(batch_labels)] = np.array(batch_labels)\n",
    "            dummy_labels = to_categorical(dummy_labels,num_classes=52)  # Convert labels to one-hot encoding\n",
    "            \n",
    "            return dummy_input_data, dummy_labels\n",
    "        \n",
    "        batch_labels = to_categorical(batch_labels,num_classes=52)  # Convert labels to one-hot encoding\n",
    "        \n",
    "        return np.array(batch_input_data), np.array(batch_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "train_dir = '/kaggle/input/datasetfornina/ninaprodb1train.pkl'\n",
    "test_dir = '/kaggle/input/datasetfornina/ninaprodb1test.pkl'\n",
    "\n",
    "# Set up dataset\n",
    "\n",
    "train = pd.read_pickle(train_dir)\n",
    "eval_data = pd.read_pickle(test_dir)\n",
    "\n",
    "# Load train data\n",
    "train_data = pd.read_pickle(train_dir)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "#train_data, val_data = train_test_split(train_data, test_size=0.3, random_state=21)\n",
    "\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
    "#val_data = val_data.sample(frac=1).reset_index(drop=True)\n",
    "eval_data = eval_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Create train, test and validation datasets\n",
    "train_dataset = Nina1Dataset(train_data, batch_size=batch_size)\n",
    "#val_dataset= Nina1Dataset(val_data, batch_size=batch_size)\n",
    "test_dataset= Nina1Dataset(eval_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec60c465",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T08:31:47.469006Z",
     "iopub.status.busy": "2023-08-12T08:31:47.468686Z",
     "iopub.status.idle": "2023-08-12T09:17:47.090991Z",
     "shell.execute_reply": "2023-08-12T09:17:47.089775Z"
    },
    "papermill": {
     "duration": 2759.631946,
     "end_time": "2023-08-12T09:17:47.094062",
     "exception": false,
     "start_time": "2023-08-12T08:31:47.462116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 25, 20, 10)\n",
      "(None, 25, 10, 64)\n",
      "(None, 25, 2, 64)\n",
      "(None, 25, 1, 64)\n",
      "(None, 25, 64)\n",
      "(None, 25, 64)\n",
      "11\n",
      "(None, 25, 400)\n",
      "11\n",
      "(None, 25, 400)\n",
      "11\n",
      "Epoch 1/200\n",
      "308/308 [==============================] - 35s 46ms/step - loss: 3.1080 - accuracy: 0.2120 - val_loss: 2.2665 - val_accuracy: 0.3594\n",
      "Epoch 2/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 2.2475 - accuracy: 0.3718 - val_loss: 1.5509 - val_accuracy: 0.5331\n",
      "Epoch 3/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 1.7731 - accuracy: 0.4825 - val_loss: 1.2601 - val_accuracy: 0.6179\n",
      "Epoch 4/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 1.4272 - accuracy: 0.5658 - val_loss: 0.9688 - val_accuracy: 0.6989\n",
      "Epoch 5/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 1.1338 - accuracy: 0.6476 - val_loss: 0.8247 - val_accuracy: 0.7495\n",
      "Epoch 6/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.9171 - accuracy: 0.7126 - val_loss: 0.6292 - val_accuracy: 0.8134\n",
      "Epoch 7/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.7287 - accuracy: 0.7750 - val_loss: 0.5622 - val_accuracy: 0.8364\n",
      "Epoch 8/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.6110 - accuracy: 0.8106 - val_loss: 0.4932 - val_accuracy: 0.8580\n",
      "Epoch 9/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.4981 - accuracy: 0.8528 - val_loss: 0.4559 - val_accuracy: 0.8674\n",
      "Epoch 10/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.4162 - accuracy: 0.8794 - val_loss: 0.3943 - val_accuracy: 0.8854\n",
      "Epoch 11/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.3447 - accuracy: 0.9048 - val_loss: 0.3714 - val_accuracy: 0.8916\n",
      "Epoch 12/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.3102 - accuracy: 0.9112 - val_loss: 0.3524 - val_accuracy: 0.8951\n",
      "Epoch 13/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.2780 - accuracy: 0.9223 - val_loss: 0.3513 - val_accuracy: 0.8944\n",
      "Epoch 14/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.2411 - accuracy: 0.9351 - val_loss: 0.3394 - val_accuracy: 0.9032\n",
      "Epoch 15/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.2156 - accuracy: 0.9463 - val_loss: 0.3297 - val_accuracy: 0.9020\n",
      "Epoch 16/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.2019 - accuracy: 0.9493 - val_loss: 0.3143 - val_accuracy: 0.9112\n",
      "Epoch 17/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1789 - accuracy: 0.9567 - val_loss: 0.3026 - val_accuracy: 0.9117\n",
      "Epoch 18/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1734 - accuracy: 0.9589 - val_loss: 0.3054 - val_accuracy: 0.9110\n",
      "Epoch 19/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1598 - accuracy: 0.9624 - val_loss: 0.3010 - val_accuracy: 0.9136\n",
      "Epoch 20/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1540 - accuracy: 0.9644 - val_loss: 0.2960 - val_accuracy: 0.9136\n",
      "Epoch 21/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1478 - accuracy: 0.9684 - val_loss: 0.2956 - val_accuracy: 0.9152\n",
      "Epoch 22/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1456 - accuracy: 0.9669 - val_loss: 0.2921 - val_accuracy: 0.9162\n",
      "Epoch 23/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1370 - accuracy: 0.9699 - val_loss: 0.2909 - val_accuracy: 0.9148\n",
      "Epoch 24/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1287 - accuracy: 0.9729 - val_loss: 0.2853 - val_accuracy: 0.9164\n",
      "Epoch 25/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1303 - accuracy: 0.9715 - val_loss: 0.2875 - val_accuracy: 0.9157\n",
      "Epoch 26/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1260 - accuracy: 0.9739 - val_loss: 0.2892 - val_accuracy: 0.9160\n",
      "Epoch 27/200\n",
      "308/308 [==============================] - 15s 48ms/step - loss: 0.1216 - accuracy: 0.9764 - val_loss: 0.2783 - val_accuracy: 0.9197\n",
      "Epoch 28/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1212 - accuracy: 0.9752 - val_loss: 0.2878 - val_accuracy: 0.9171\n",
      "Epoch 29/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1194 - accuracy: 0.9768 - val_loss: 0.2815 - val_accuracy: 0.9209\n",
      "Epoch 30/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1175 - accuracy: 0.9755 - val_loss: 0.2823 - val_accuracy: 0.9190\n",
      "Epoch 31/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1177 - accuracy: 0.9753 - val_loss: 0.2823 - val_accuracy: 0.9186\n",
      "Epoch 32/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1119 - accuracy: 0.9764 - val_loss: 0.2842 - val_accuracy: 0.9190\n",
      "Epoch 33/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1123 - accuracy: 0.9784 - val_loss: 0.2831 - val_accuracy: 0.9179\n",
      "Epoch 34/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1114 - accuracy: 0.9778 - val_loss: 0.2833 - val_accuracy: 0.9181\n",
      "Epoch 35/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1107 - accuracy: 0.9770 - val_loss: 0.2805 - val_accuracy: 0.9188\n",
      "Epoch 36/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1096 - accuracy: 0.9781 - val_loss: 0.2820 - val_accuracy: 0.9188\n",
      "Epoch 37/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1092 - accuracy: 0.9785 - val_loss: 0.2782 - val_accuracy: 0.9200\n",
      "Epoch 38/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1139 - accuracy: 0.9759 - val_loss: 0.2807 - val_accuracy: 0.9202\n",
      "Epoch 39/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1066 - accuracy: 0.9798 - val_loss: 0.2809 - val_accuracy: 0.9183\n",
      "Epoch 40/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1068 - accuracy: 0.9787 - val_loss: 0.2814 - val_accuracy: 0.9176\n",
      "Epoch 41/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1101 - accuracy: 0.9780 - val_loss: 0.2799 - val_accuracy: 0.9183\n",
      "Epoch 42/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1144 - accuracy: 0.9746 - val_loss: 0.2852 - val_accuracy: 0.9188\n",
      "Epoch 43/200\n",
      "308/308 [==============================] - 13s 43ms/step - loss: 0.1037 - accuracy: 0.9796 - val_loss: 0.2804 - val_accuracy: 0.9176\n",
      "Epoch 44/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1089 - accuracy: 0.9778 - val_loss: 0.2749 - val_accuracy: 0.9188\n",
      "Epoch 45/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1088 - accuracy: 0.9791 - val_loss: 0.2824 - val_accuracy: 0.9183\n",
      "Epoch 46/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1079 - accuracy: 0.9776 - val_loss: 0.2833 - val_accuracy: 0.9190\n",
      "Epoch 47/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1055 - accuracy: 0.9802 - val_loss: 0.2812 - val_accuracy: 0.9193\n",
      "Epoch 48/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1124 - accuracy: 0.9763 - val_loss: 0.2774 - val_accuracy: 0.9202\n",
      "Epoch 49/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1094 - accuracy: 0.9788 - val_loss: 0.2810 - val_accuracy: 0.9186\n",
      "Epoch 50/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1095 - accuracy: 0.9793 - val_loss: 0.2790 - val_accuracy: 0.9193\n",
      "Epoch 51/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1109 - accuracy: 0.9759 - val_loss: 0.2765 - val_accuracy: 0.9190\n",
      "Epoch 52/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1133 - accuracy: 0.9764 - val_loss: 0.2847 - val_accuracy: 0.9181\n",
      "Epoch 53/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1087 - accuracy: 0.9780 - val_loss: 0.2786 - val_accuracy: 0.9197\n",
      "Epoch 54/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1080 - accuracy: 0.9790 - val_loss: 0.2793 - val_accuracy: 0.9195\n",
      "Epoch 55/200\n",
      "308/308 [==============================] - 13s 43ms/step - loss: 0.1041 - accuracy: 0.9805 - val_loss: 0.2818 - val_accuracy: 0.9174\n",
      "Epoch 56/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1066 - accuracy: 0.9798 - val_loss: 0.2786 - val_accuracy: 0.9186\n",
      "Epoch 57/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1054 - accuracy: 0.9794 - val_loss: 0.2802 - val_accuracy: 0.9193\n",
      "Epoch 58/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1049 - accuracy: 0.9808 - val_loss: 0.2817 - val_accuracy: 0.9179\n",
      "Epoch 59/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1067 - accuracy: 0.9795 - val_loss: 0.2789 - val_accuracy: 0.9193\n",
      "Epoch 60/200\n",
      "308/308 [==============================] - 16s 51ms/step - loss: 0.1064 - accuracy: 0.9788 - val_loss: 0.2849 - val_accuracy: 0.9171\n",
      "Epoch 61/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1069 - accuracy: 0.9767 - val_loss: 0.2847 - val_accuracy: 0.9181\n",
      "Epoch 62/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1086 - accuracy: 0.9782 - val_loss: 0.2777 - val_accuracy: 0.9205\n",
      "Epoch 63/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1018 - accuracy: 0.9814 - val_loss: 0.2786 - val_accuracy: 0.9186\n",
      "Epoch 64/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1097 - accuracy: 0.9766 - val_loss: 0.2744 - val_accuracy: 0.9197\n",
      "Epoch 65/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1048 - accuracy: 0.9792 - val_loss: 0.2839 - val_accuracy: 0.9167\n",
      "Epoch 66/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1060 - accuracy: 0.9788 - val_loss: 0.2798 - val_accuracy: 0.9190\n",
      "Epoch 67/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1073 - accuracy: 0.9788 - val_loss: 0.2789 - val_accuracy: 0.9186\n",
      "Epoch 68/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1068 - accuracy: 0.9802 - val_loss: 0.2738 - val_accuracy: 0.9197\n",
      "Epoch 69/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1046 - accuracy: 0.9796 - val_loss: 0.2803 - val_accuracy: 0.9181\n",
      "Epoch 70/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1051 - accuracy: 0.9808 - val_loss: 0.2762 - val_accuracy: 0.9197\n",
      "Epoch 71/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1082 - accuracy: 0.9791 - val_loss: 0.2798 - val_accuracy: 0.9202\n",
      "Epoch 72/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1080 - accuracy: 0.9778 - val_loss: 0.2791 - val_accuracy: 0.9188\n",
      "Epoch 73/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1074 - accuracy: 0.9789 - val_loss: 0.2781 - val_accuracy: 0.9200\n",
      "Epoch 74/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1097 - accuracy: 0.9781 - val_loss: 0.2830 - val_accuracy: 0.9193\n",
      "Epoch 75/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1027 - accuracy: 0.9816 - val_loss: 0.2774 - val_accuracy: 0.9190\n",
      "Epoch 76/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1077 - accuracy: 0.9780 - val_loss: 0.2820 - val_accuracy: 0.9188\n",
      "Epoch 77/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1047 - accuracy: 0.9807 - val_loss: 0.2825 - val_accuracy: 0.9174\n",
      "Epoch 78/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1045 - accuracy: 0.9812 - val_loss: 0.2809 - val_accuracy: 0.9183\n",
      "Epoch 79/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1107 - accuracy: 0.9773 - val_loss: 0.2794 - val_accuracy: 0.9193\n",
      "Epoch 80/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1077 - accuracy: 0.9783 - val_loss: 0.2826 - val_accuracy: 0.9188\n",
      "Epoch 81/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1046 - accuracy: 0.9796 - val_loss: 0.2815 - val_accuracy: 0.9171\n",
      "Epoch 82/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1101 - accuracy: 0.9773 - val_loss: 0.2786 - val_accuracy: 0.9186\n",
      "Epoch 83/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1060 - accuracy: 0.9802 - val_loss: 0.2828 - val_accuracy: 0.9181\n",
      "Epoch 84/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1043 - accuracy: 0.9808 - val_loss: 0.2785 - val_accuracy: 0.9193\n",
      "Epoch 85/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1085 - accuracy: 0.9786 - val_loss: 0.2802 - val_accuracy: 0.9202\n",
      "Epoch 86/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1060 - accuracy: 0.9785 - val_loss: 0.2785 - val_accuracy: 0.9186\n",
      "Epoch 87/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1056 - accuracy: 0.9789 - val_loss: 0.2757 - val_accuracy: 0.9202\n",
      "Epoch 88/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1078 - accuracy: 0.9786 - val_loss: 0.2780 - val_accuracy: 0.9202\n",
      "Epoch 89/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1081 - accuracy: 0.9771 - val_loss: 0.2724 - val_accuracy: 0.9197\n",
      "Epoch 90/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1098 - accuracy: 0.9779 - val_loss: 0.2821 - val_accuracy: 0.9174\n",
      "Epoch 91/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1081 - accuracy: 0.9791 - val_loss: 0.2862 - val_accuracy: 0.9179\n",
      "Epoch 92/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1078 - accuracy: 0.9785 - val_loss: 0.2815 - val_accuracy: 0.9197\n",
      "Epoch 93/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1110 - accuracy: 0.9772 - val_loss: 0.2818 - val_accuracy: 0.9181\n",
      "Epoch 94/200\n",
      "308/308 [==============================] - 15s 49ms/step - loss: 0.1091 - accuracy: 0.9775 - val_loss: 0.2800 - val_accuracy: 0.9183\n",
      "Epoch 95/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1050 - accuracy: 0.9812 - val_loss: 0.2846 - val_accuracy: 0.9174\n",
      "Epoch 96/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1090 - accuracy: 0.9781 - val_loss: 0.2818 - val_accuracy: 0.9190\n",
      "Epoch 97/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1042 - accuracy: 0.9790 - val_loss: 0.2791 - val_accuracy: 0.9188\n",
      "Epoch 98/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1072 - accuracy: 0.9798 - val_loss: 0.2834 - val_accuracy: 0.9188\n",
      "Epoch 99/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1123 - accuracy: 0.9756 - val_loss: 0.2823 - val_accuracy: 0.9183\n",
      "Epoch 100/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1050 - accuracy: 0.9807 - val_loss: 0.2814 - val_accuracy: 0.9179\n",
      "Epoch 101/200\n",
      "308/308 [==============================] - 15s 48ms/step - loss: 0.1099 - accuracy: 0.9766 - val_loss: 0.2770 - val_accuracy: 0.9207\n",
      "Epoch 102/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1059 - accuracy: 0.9781 - val_loss: 0.2778 - val_accuracy: 0.9181\n",
      "Epoch 103/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1079 - accuracy: 0.9786 - val_loss: 0.2767 - val_accuracy: 0.9197\n",
      "Epoch 104/200\n",
      "308/308 [==============================] - 15s 50ms/step - loss: 0.1061 - accuracy: 0.9795 - val_loss: 0.2753 - val_accuracy: 0.9186\n",
      "Epoch 105/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1021 - accuracy: 0.9806 - val_loss: 0.2831 - val_accuracy: 0.9169\n",
      "Epoch 106/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1047 - accuracy: 0.9779 - val_loss: 0.2752 - val_accuracy: 0.9183\n",
      "Epoch 107/200\n",
      "308/308 [==============================] - 13s 43ms/step - loss: 0.1048 - accuracy: 0.9802 - val_loss: 0.2801 - val_accuracy: 0.9195\n",
      "Epoch 108/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1082 - accuracy: 0.9782 - val_loss: 0.2772 - val_accuracy: 0.9186\n",
      "Epoch 109/200\n",
      "308/308 [==============================] - 15s 49ms/step - loss: 0.1132 - accuracy: 0.9755 - val_loss: 0.2829 - val_accuracy: 0.9179\n",
      "Epoch 110/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1107 - accuracy: 0.9762 - val_loss: 0.2815 - val_accuracy: 0.9179\n",
      "Epoch 111/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1129 - accuracy: 0.9755 - val_loss: 0.2792 - val_accuracy: 0.9186\n",
      "Epoch 112/200\n",
      "308/308 [==============================] - 14s 44ms/step - loss: 0.1093 - accuracy: 0.9775 - val_loss: 0.2741 - val_accuracy: 0.9205\n",
      "Epoch 113/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1048 - accuracy: 0.9797 - val_loss: 0.2816 - val_accuracy: 0.9183\n",
      "Epoch 114/200\n",
      "308/308 [==============================] - 14s 44ms/step - loss: 0.1067 - accuracy: 0.9807 - val_loss: 0.2801 - val_accuracy: 0.9174\n",
      "Epoch 115/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1042 - accuracy: 0.9799 - val_loss: 0.2782 - val_accuracy: 0.9183\n",
      "Epoch 116/200\n",
      "308/308 [==============================] - 15s 47ms/step - loss: 0.1095 - accuracy: 0.9789 - val_loss: 0.2784 - val_accuracy: 0.9193\n",
      "Epoch 117/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1109 - accuracy: 0.9773 - val_loss: 0.2793 - val_accuracy: 0.9176\n",
      "Epoch 118/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1118 - accuracy: 0.9759 - val_loss: 0.2786 - val_accuracy: 0.9195\n",
      "Epoch 119/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1055 - accuracy: 0.9813 - val_loss: 0.2805 - val_accuracy: 0.9193\n",
      "Epoch 120/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1091 - accuracy: 0.9766 - val_loss: 0.2749 - val_accuracy: 0.9205\n",
      "Epoch 121/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1080 - accuracy: 0.9771 - val_loss: 0.2816 - val_accuracy: 0.9186\n",
      "Epoch 122/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1097 - accuracy: 0.9774 - val_loss: 0.2820 - val_accuracy: 0.9162\n",
      "Epoch 123/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1040 - accuracy: 0.9800 - val_loss: 0.2786 - val_accuracy: 0.9176\n",
      "Epoch 124/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1093 - accuracy: 0.9775 - val_loss: 0.2811 - val_accuracy: 0.9193\n",
      "Epoch 125/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1098 - accuracy: 0.9774 - val_loss: 0.2825 - val_accuracy: 0.9181\n",
      "Epoch 126/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1063 - accuracy: 0.9794 - val_loss: 0.2803 - val_accuracy: 0.9195\n",
      "Epoch 127/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1061 - accuracy: 0.9813 - val_loss: 0.2802 - val_accuracy: 0.9193\n",
      "Epoch 128/200\n",
      "308/308 [==============================] - 12s 38ms/step - loss: 0.1086 - accuracy: 0.9764 - val_loss: 0.2728 - val_accuracy: 0.9216\n",
      "Epoch 129/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1026 - accuracy: 0.9806 - val_loss: 0.2825 - val_accuracy: 0.9183\n",
      "Epoch 130/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1070 - accuracy: 0.9795 - val_loss: 0.2785 - val_accuracy: 0.9183\n",
      "Epoch 131/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1026 - accuracy: 0.9795 - val_loss: 0.2796 - val_accuracy: 0.9176\n",
      "Epoch 132/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1117 - accuracy: 0.9782 - val_loss: 0.2797 - val_accuracy: 0.9190\n",
      "Epoch 133/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1077 - accuracy: 0.9787 - val_loss: 0.2809 - val_accuracy: 0.9174\n",
      "Epoch 134/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1051 - accuracy: 0.9791 - val_loss: 0.2786 - val_accuracy: 0.9207\n",
      "Epoch 135/200\n",
      "308/308 [==============================] - 15s 48ms/step - loss: 0.1083 - accuracy: 0.9778 - val_loss: 0.2802 - val_accuracy: 0.9183\n",
      "Epoch 136/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1085 - accuracy: 0.9775 - val_loss: 0.2785 - val_accuracy: 0.9183\n",
      "Epoch 137/200\n",
      "308/308 [==============================] - 12s 38ms/step - loss: 0.1098 - accuracy: 0.9773 - val_loss: 0.2804 - val_accuracy: 0.9179\n",
      "Epoch 138/200\n",
      "308/308 [==============================] - 13s 43ms/step - loss: 0.1070 - accuracy: 0.9788 - val_loss: 0.2816 - val_accuracy: 0.9190\n",
      "Epoch 139/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1078 - accuracy: 0.9786 - val_loss: 0.2791 - val_accuracy: 0.9195\n",
      "Epoch 140/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1044 - accuracy: 0.9800 - val_loss: 0.2809 - val_accuracy: 0.9174\n",
      "Epoch 141/200\n",
      "308/308 [==============================] - 14s 45ms/step - loss: 0.1069 - accuracy: 0.9788 - val_loss: 0.2800 - val_accuracy: 0.9190\n",
      "Epoch 142/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1072 - accuracy: 0.9791 - val_loss: 0.2790 - val_accuracy: 0.9197\n",
      "Epoch 143/200\n",
      "308/308 [==============================] - 14s 46ms/step - loss: 0.1115 - accuracy: 0.9763 - val_loss: 0.2812 - val_accuracy: 0.9176\n",
      "Epoch 144/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1053 - accuracy: 0.9798 - val_loss: 0.2773 - val_accuracy: 0.9183\n",
      "Epoch 145/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1061 - accuracy: 0.9788 - val_loss: 0.2816 - val_accuracy: 0.9195\n",
      "Epoch 146/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1058 - accuracy: 0.9801 - val_loss: 0.2833 - val_accuracy: 0.9179\n",
      "Epoch 147/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1047 - accuracy: 0.9786 - val_loss: 0.2795 - val_accuracy: 0.9181\n",
      "Epoch 148/200\n",
      "308/308 [==============================] - 13s 43ms/step - loss: 0.1097 - accuracy: 0.9777 - val_loss: 0.2800 - val_accuracy: 0.9176\n",
      "Epoch 149/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1094 - accuracy: 0.9780 - val_loss: 0.2788 - val_accuracy: 0.9207\n",
      "Epoch 150/200\n",
      "308/308 [==============================] - 13s 43ms/step - loss: 0.1062 - accuracy: 0.9799 - val_loss: 0.2769 - val_accuracy: 0.9200\n",
      "Epoch 151/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1062 - accuracy: 0.9784 - val_loss: 0.2809 - val_accuracy: 0.9183\n",
      "Epoch 152/200\n",
      "308/308 [==============================] - 13s 44ms/step - loss: 0.1081 - accuracy: 0.9796 - val_loss: 0.2806 - val_accuracy: 0.9186\n",
      "Epoch 153/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1053 - accuracy: 0.9800 - val_loss: 0.2793 - val_accuracy: 0.9181\n",
      "Epoch 154/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1083 - accuracy: 0.9783 - val_loss: 0.2855 - val_accuracy: 0.9183\n",
      "Epoch 155/200\n",
      "308/308 [==============================] - 13s 43ms/step - loss: 0.1079 - accuracy: 0.9787 - val_loss: 0.2804 - val_accuracy: 0.9169\n",
      "Epoch 156/200\n",
      "308/308 [==============================] - 13s 42ms/step - loss: 0.1091 - accuracy: 0.9773 - val_loss: 0.2832 - val_accuracy: 0.9188\n",
      "Epoch 157/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1084 - accuracy: 0.9784 - val_loss: 0.2795 - val_accuracy: 0.9183\n",
      "Epoch 158/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1054 - accuracy: 0.9800 - val_loss: 0.2791 - val_accuracy: 0.9212\n",
      "Epoch 159/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1085 - accuracy: 0.9790 - val_loss: 0.2799 - val_accuracy: 0.9183\n",
      "Epoch 160/200\n",
      "308/308 [==============================] - 16s 52ms/step - loss: 0.1049 - accuracy: 0.9805 - val_loss: 0.2749 - val_accuracy: 0.9202\n",
      "Epoch 161/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1113 - accuracy: 0.9777 - val_loss: 0.2827 - val_accuracy: 0.9162\n",
      "Epoch 162/200\n",
      "308/308 [==============================] - 15s 49ms/step - loss: 0.1056 - accuracy: 0.9791 - val_loss: 0.2804 - val_accuracy: 0.9179\n",
      "Epoch 163/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1099 - accuracy: 0.9781 - val_loss: 0.2813 - val_accuracy: 0.9176\n",
      "Epoch 164/200\n",
      "308/308 [==============================] - 12s 38ms/step - loss: 0.1090 - accuracy: 0.9774 - val_loss: 0.2794 - val_accuracy: 0.9188\n",
      "Epoch 165/200\n",
      "308/308 [==============================] - 14s 46ms/step - loss: 0.1069 - accuracy: 0.9801 - val_loss: 0.2798 - val_accuracy: 0.9193\n",
      "Epoch 166/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1091 - accuracy: 0.9782 - val_loss: 0.2811 - val_accuracy: 0.9188\n",
      "Epoch 167/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1030 - accuracy: 0.9816 - val_loss: 0.2763 - val_accuracy: 0.9190\n",
      "Epoch 168/200\n",
      "308/308 [==============================] - 14s 46ms/step - loss: 0.1083 - accuracy: 0.9791 - val_loss: 0.2748 - val_accuracy: 0.9197\n",
      "Epoch 169/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1109 - accuracy: 0.9763 - val_loss: 0.2834 - val_accuracy: 0.9186\n",
      "Epoch 170/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1078 - accuracy: 0.9781 - val_loss: 0.2800 - val_accuracy: 0.9190\n",
      "Epoch 171/200\n",
      "308/308 [==============================] - 13s 43ms/step - loss: 0.1147 - accuracy: 0.9767 - val_loss: 0.2750 - val_accuracy: 0.9179\n",
      "Epoch 172/200\n",
      "308/308 [==============================] - 12s 40ms/step - loss: 0.1082 - accuracy: 0.9785 - val_loss: 0.2783 - val_accuracy: 0.9183\n",
      "Epoch 173/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1100 - accuracy: 0.9783 - val_loss: 0.2782 - val_accuracy: 0.9193\n",
      "Epoch 174/200\n",
      "308/308 [==============================] - 13s 43ms/step - loss: 0.1061 - accuracy: 0.9794 - val_loss: 0.2824 - val_accuracy: 0.9193\n",
      "Epoch 175/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1102 - accuracy: 0.9769 - val_loss: 0.2812 - val_accuracy: 0.9181\n",
      "Epoch 176/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1081 - accuracy: 0.9775 - val_loss: 0.2827 - val_accuracy: 0.9181\n",
      "Epoch 177/200\n",
      "308/308 [==============================] - 13s 43ms/step - loss: 0.1088 - accuracy: 0.9771 - val_loss: 0.2781 - val_accuracy: 0.9202\n",
      "Epoch 178/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1057 - accuracy: 0.9793 - val_loss: 0.2824 - val_accuracy: 0.9179\n",
      "Epoch 179/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1083 - accuracy: 0.9797 - val_loss: 0.2722 - val_accuracy: 0.9202\n",
      "Epoch 180/200\n",
      "308/308 [==============================] - 14s 46ms/step - loss: 0.1067 - accuracy: 0.9794 - val_loss: 0.2864 - val_accuracy: 0.9167\n",
      "Epoch 181/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1082 - accuracy: 0.9786 - val_loss: 0.2784 - val_accuracy: 0.9190\n",
      "Epoch 182/200\n",
      "308/308 [==============================] - 14s 47ms/step - loss: 0.1074 - accuracy: 0.9777 - val_loss: 0.2799 - val_accuracy: 0.9181\n",
      "Epoch 183/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1052 - accuracy: 0.9798 - val_loss: 0.2780 - val_accuracy: 0.9190\n",
      "Epoch 184/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1079 - accuracy: 0.9767 - val_loss: 0.2795 - val_accuracy: 0.9186\n",
      "Epoch 185/200\n",
      "308/308 [==============================] - 14s 46ms/step - loss: 0.1047 - accuracy: 0.9793 - val_loss: 0.2814 - val_accuracy: 0.9181\n",
      "Epoch 186/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1064 - accuracy: 0.9803 - val_loss: 0.2823 - val_accuracy: 0.9169\n",
      "Epoch 187/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1073 - accuracy: 0.9781 - val_loss: 0.2837 - val_accuracy: 0.9181\n",
      "Epoch 188/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1049 - accuracy: 0.9808 - val_loss: 0.2835 - val_accuracy: 0.9176\n",
      "Epoch 189/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1054 - accuracy: 0.9784 - val_loss: 0.2789 - val_accuracy: 0.9193\n",
      "Epoch 190/200\n",
      "308/308 [==============================] - 16s 53ms/step - loss: 0.1066 - accuracy: 0.9791 - val_loss: 0.2791 - val_accuracy: 0.9176\n",
      "Epoch 191/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1114 - accuracy: 0.9753 - val_loss: 0.2801 - val_accuracy: 0.9162\n",
      "Epoch 192/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1091 - accuracy: 0.9773 - val_loss: 0.2823 - val_accuracy: 0.9195\n",
      "Epoch 193/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1113 - accuracy: 0.9774 - val_loss: 0.2815 - val_accuracy: 0.9190\n",
      "Epoch 194/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1051 - accuracy: 0.9786 - val_loss: 0.2810 - val_accuracy: 0.9190\n",
      "Epoch 195/200\n",
      "308/308 [==============================] - 13s 44ms/step - loss: 0.1058 - accuracy: 0.9798 - val_loss: 0.2797 - val_accuracy: 0.9183\n",
      "Epoch 196/200\n",
      "308/308 [==============================] - 15s 48ms/step - loss: 0.1101 - accuracy: 0.9769 - val_loss: 0.2791 - val_accuracy: 0.9186\n",
      "Epoch 197/200\n",
      "308/308 [==============================] - 12s 38ms/step - loss: 0.1112 - accuracy: 0.9776 - val_loss: 0.2774 - val_accuracy: 0.9193\n",
      "Epoch 198/200\n",
      "308/308 [==============================] - 12s 39ms/step - loss: 0.1108 - accuracy: 0.9762 - val_loss: 0.2824 - val_accuracy: 0.9186\n",
      "Epoch 199/200\n",
      "308/308 [==============================] - 13s 41ms/step - loss: 0.1030 - accuracy: 0.9807 - val_loss: 0.2814 - val_accuracy: 0.9179\n",
      "Epoch 200/200\n",
      "308/308 [==============================] - 13s 43ms/step - loss: 0.1041 - accuracy: 0.9810 - val_loss: 0.2820 - val_accuracy: 0.9190\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.activations import tanh\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Reshape\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dropout\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Bidirectional, Dropout, Input, Conv1D, MaxPooling1D,\n",
    "    Flatten, Dense, ReLU, TimeDistributed, Reshape, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "from tensorflow.keras import regularizers, initializers\n",
    "def cnn(x):\n",
    "    #print(x.shape)\n",
    "    #x = Reshape((25, 20,10))(x)  \n",
    "    print(x.shape)\n",
    "    x = TimeDistributed((Conv1D(64, kernel_size=9, strides=2, padding='same', activation=tanh)))(x)\n",
    "    print(x.shape)\n",
    "    #(Batch, 10, 64)\n",
    "    #x = TimeDistributed(Dropout(0.2093))(x)\n",
    "    #x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n",
    "    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n",
    "    x = TimeDistributed(MaxPooling1D(pool_size=8, strides=2))(x)\n",
    "    print(x.shape)\n",
    "    #(Batch, 2, 64)\n",
    "    \n",
    "    #x = TimeDistributed(Dropout(0.2093))(x)\n",
    "    \n",
    "    \n",
    "    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n",
    "    #(Batch, 1, 64)\n",
    "    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n",
    "    x = TimeDistributed(Dropout(0.2093))(x)\n",
    "    \n",
    "    \n",
    "    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n",
    "    print(x.shape)\n",
    "    #(Batch, 1, 64)\n",
    "    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n",
    "    x = TimeDistributed(Dropout(0.2093))(x)\n",
    "   \n",
    "    \n",
    "    x = TimeDistributed((Conv1D(64, kernel_size=3, strides=2, padding='same', activation=tanh)))(x)\n",
    "    #print(x.shape)\n",
    "    #(Batch, 1, 64)\n",
    "    x = TimeDistributed(Dropout(0.2093))(x)\n",
    "    x = TimeDistributed(Dropout(0.5))(x)\n",
    "    #x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n",
    "    x = TimeDistributed(Flatten())(x)\n",
    "    print(x.shape)\n",
    "    # (Batch, 64)\n",
    "\n",
    "    return x\n",
    "\n",
    "def Bi_LSTMModel(input_shape,x):\n",
    "    #model = Sequential()\n",
    "    # Hidden dimensions\n",
    "    hidden_dim = 200\n",
    "    print(x.shape)\n",
    "    print(11)\n",
    "    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.3), input_shape=input_shape)(x)\n",
    "    #x = Dropout(0.2093)(x)\n",
    "    print(x.shape)\n",
    "    print(11)\n",
    "    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.3))(x)\n",
    "\n",
    "    print(x.shape)\n",
    "    print(11)\n",
    "    #x = Dropout(0.2093)(x)\n",
    "    x = Flatten()(x)\n",
    "    # ( ,10000)\n",
    "\n",
    "    return x\n",
    "def EMGHandNet(input_shape, num_classes):\n",
    "    # Define the input layer\n",
    "    x = Input(shape=input_shape)\n",
    "    inputs = x\n",
    "    #print(x.shape)\n",
    "    #(batch, 25, 20, 10)\n",
    "    #temp = [cnn(x[:, t, :, :]) for t in range(x.shape[1])]\n",
    "    #x = tf.stack(temp, axis=1)\n",
    "    #print(x.shape)\n",
    "    x = cnn(x)\n",
    "   \n",
    "    #print(x.shape)\n",
    "    x = Bi_LSTMModel(x.shape[1:],x)\n",
    "    #print(x.shape)\n",
    "    #x = Dropout(0.2093)(x)\n",
    "    \n",
    "    x = Dense(512, activation='tanh')(x)\n",
    "    #print(x.shape)\n",
    "    x = Dropout(0.2093)(x)\n",
    "    #x = BatchNormalization(epsilon=1e-6, momentum=0.95)(x)\n",
    "\n",
    "    # Add the output layer\n",
    "    output_layer = Dense(52, activation='softmax')(x)\n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=output_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "num_classes = 52  # Adjust based on the number of hand activity classes\n",
    "\n",
    "\n",
    "# Create an instance of the EMGHandNet model\n",
    "model = EMGHandNet((25,20,10), 52)\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "decay_steps = 200\n",
    "decay_rate = 0.9\n",
    "batch_size =32\n",
    "# Define your model and its optimizer\n",
    "# Define learning rate schedule\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n",
    "\n",
    "    # Compile the model with learning rate schedule\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999),\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "                metrics=['accuracy'])\n",
    "with tf.device('/GPU:0'):\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=200,\n",
    "                        batch_size=32,\n",
    "                        validation_data=test_dataset\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d4d991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-12T09:17:52.867619Z",
     "iopub.status.busy": "2023-08-12T09:17:52.867219Z",
     "iopub.status.idle": "2023-08-12T09:18:05.285627Z",
     "shell.execute_reply": "2023-08-12T09:18:05.284035Z"
    },
    "papermill": {
     "duration": 15.299347,
     "end_time": "2023-08-12T09:18:05.287853",
     "exception": false,
     "start_time": "2023-08-12T09:17:49.988506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "0.9190340909090909\n"
     ]
    }
   ],
   "source": [
    "true_labels = []  # True labels for the test data\n",
    "predicted_labels = []  # Predicted labels for the test data\n",
    "\n",
    "for batch_data, batch_labels in test_dataset:\n",
    "    batch_predictions = model.predict(batch_data)\n",
    "    batch_predicted_labels = np.argmax(batch_predictions, axis=1)\n",
    "    predicted_labels.extend(batch_predicted_labels)\n",
    "    true_labels.extend(np.argmax(batch_labels, axis=1))\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "accuracy = np.mean(true_labels == predicted_labels)\n",
    "\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2825.914206,
   "end_time": "2023-08-12T09:18:11.552481",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-12T08:31:05.638275",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
