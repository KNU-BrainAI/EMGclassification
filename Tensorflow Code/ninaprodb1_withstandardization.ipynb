{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":2051.05586,"end_time":"2023-05-21T13:12:12.053262","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-05-21T12:38:00.997402","version":"2.4.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.038029,"end_time":"2023-05-21T12:38:11.801112","exception":false,"start_time":"2023-05-21T12:38:11.763083","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T12:28:47.458812Z","iopub.execute_input":"2023-07-19T12:28:47.459285Z","iopub.status.idle":"2023-07-19T12:28:47.471736Z","shell.execute_reply.started":"2023-07-19T12:28:47.459243Z","shell.execute_reply":"2023-07-19T12:28:47.470649Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/datasetfornina/ninaprodb1train.pkl\n/kaggle/input/datasetfornina/ninaprodb1test.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\n#from sklearn.model_selection import train_test_split\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport math","metadata":{"papermill":{"duration":6.758124,"end_time":"2023-05-21T12:38:18.563919","exception":false,"start_time":"2023-05-21T12:38:11.805795","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T12:28:47.474692Z","iopub.execute_input":"2023-07-19T12:28:47.475018Z","iopub.status.idle":"2023-07-19T12:28:56.672751Z","shell.execute_reply.started":"2023-07-19T12:28:47.474987Z","shell.execute_reply":"2023-07-19T12:28:56.671667Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\n# Load the dataset from pickle file\nwith open('/kaggle/input/datasetfornina/ninaprodb1test.pkl', 'rb') as f:\n    test_data = pickle.load(f)\nwith open('/kaggle/input/datasetfornina/ninaprodb1train.pkl', 'rb') as f:\n    train_data = pickle.load(f)","metadata":{"papermill":{"duration":4.616483,"end_time":"2023-05-21T12:38:23.184515","exception":false,"start_time":"2023-05-21T12:38:18.568032","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T12:28:56.674080Z","iopub.execute_input":"2023-07-19T12:28:56.675451Z","iopub.status.idle":"2023-07-19T12:29:00.807101Z","shell.execute_reply.started":"2023-07-19T12:28:56.675419Z","shell.execute_reply":"2023-07-19T12:29:00.806106Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"papermill":{"duration":0.649738,"end_time":"2023-05-21T12:38:23.838778","exception":false,"start_time":"2023-05-21T12:38:23.189040","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T12:29:00.808467Z","iopub.execute_input":"2023-07-19T12:29:00.808846Z","iopub.status.idle":"2023-07-19T12:29:01.403073Z","shell.execute_reply.started":"2023-07-19T12:29:00.808812Z","shell.execute_reply":"2023-07-19T12:29:01.402017Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers, Sequential, optimizers, Input, Model","metadata":{"papermill":{"duration":0.0117,"end_time":"2023-05-21T12:38:23.854540","exception":false,"start_time":"2023-05-21T12:38:23.842840","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T12:29:01.406193Z","iopub.execute_input":"2023-07-19T12:29:01.406905Z","iopub.status.idle":"2023-07-19T12:29:01.412791Z","shell.execute_reply.started":"2023-07-19T12:29:01.406867Z","shell.execute_reply":"2023-07-19T12:29:01.411678Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**WORKED CODE HERE**","metadata":{"papermill":{"duration":0.003685,"end_time":"2023-05-21T12:38:23.862030","exception":false,"start_time":"2023-05-21T12:38:23.858345","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\nimport gc\nimport random\nimport os\n\nimport matplotlib.pyplot as plt\nimport json","metadata":{"papermill":{"duration":0.013837,"end_time":"2023-05-21T12:38:23.879571","exception":false,"start_time":"2023-05-21T12:38:23.865734","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T12:29:01.414733Z","iopub.execute_input":"2023-07-19T12:29:01.415493Z","iopub.status.idle":"2023-07-19T12:29:01.429415Z","shell.execute_reply.started":"2023-07-19T12:29:01.415452Z","shell.execute_reply":"2023-07-19T12:29:01.428446Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"epochs = 64\nlearning_rate = 1e-3\nbatch_size = 16\nmethod = \"default\"\ndataset_type = 1","metadata":{"papermill":{"duration":0.011418,"end_time":"2023-05-21T12:38:23.894788","exception":false,"start_time":"2023-05-21T12:38:23.883370","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T12:29:01.431127Z","iopub.execute_input":"2023-07-19T12:29:01.431454Z","iopub.status.idle":"2023-07-19T12:29:01.438257Z","shell.execute_reply.started":"2023-07-19T12:29:01.431423Z","shell.execute_reply":"2023-07-19T12:29:01.437291Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"### import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.utils import to_categorical\n\n\nclass Nina1Dataset(tf.keras.utils.Sequence):\n    def __init__(self, dataframe, batch_size):\n        self.dataframe = dataframe\n        self.batch_size = batch_size\n        self.scaler = StandardScaler()\n        self.scaler.fit(np.concatenate(self.dataframe['emg'].tolist()))\n        \n    def __len__(self):\n        return int(np.ceil(len(self.dataframe) / self.batch_size))\n\n    def __getitem__(self, idx):\n        batch_data = self.dataframe[idx * self.batch_size:(idx + 1) * self.batch_size]\n\n        batch_input_data = []\n        batch_labels = []\n\n        for i, target_row in batch_data.iterrows():\n            data = target_row['emg'][:500]\n\n            # Zero-Padding\n            if len(data) < 500:\n                data = np.concatenate((data, np.zeros((500 - len(data), 10))), axis=0)\n            data = self.scaler.transform(data)\n\n            # Division data by time-segment\n            #input_data = np.transpose(data.reshape((25, 20, 10)), (0, 2, 1))\n            input_data = data.reshape((25, 20, 10))\n            label = target_row['stimulus']\n            #label = to_categorical(label)\n            batch_input_data.append(input_data)\n            batch_labels.append(label)\n\n        # Check if the batch size is smaller than the desired batch_size\n        if len(batch_data) < self.batch_size:\n            # Create a dummy batch with all elements set to zero\n            dummy_input_data = np.zeros((self.batch_size,) + input_data.shape, dtype=np.float32)\n            dummy_labels = np.zeros((self.batch_size,), dtype=np.int32)\n            dummy_input_data[:len(batch_input_data)] = np.array(batch_input_data)\n            dummy_labels[:len(batch_labels)] = np.array(batch_labels)\n            dummy_labels = to_categorical(dummy_labels,num_classes=52)  # Convert labels to one-hot encoding\n            \n            return dummy_input_data, dummy_labels\n        \n        batch_labels = to_categorical(batch_labels,num_classes=52)  # Convert labels to one-hot encoding\n        \n        return np.array(batch_input_data), np.array(batch_labels)\n\n\n\n\n\n# Parameters\nbatch_size = 16\ntrain_dir = '/kaggle/input/datasetfornina/ninaprodb1train.pkl'\ntest_dir = '/kaggle/input/datasetfornina/ninaprodb1test.pkl'\n\n# Set up dataset\n\ntrain = pd.read_pickle(train_dir)\neval_data = pd.read_pickle(test_dir)\n\n# Load train data\ntrain_data = pd.read_pickle(train_dir)\n\n# Split data into train and validation sets\ntrain_data, val_data = train_test_split(train_data, test_size=0.3, random_state=21)\n\n# Create train, test and validation datasets\ntrain_dataset = Nina1Dataset(train_data, batch_size=batch_size)\nval_dataset= Nina1Dataset(val_data, batch_size=batch_size)\ntest_dataset= Nina1Dataset(eval_data, batch_size=batch_size)\n\n\nprint(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2023-07-19T12:29:01.439869Z","iopub.execute_input":"2023-07-19T12:29:01.440313Z","iopub.status.idle":"2023-07-19T12:29:03.984994Z","shell.execute_reply.started":"2023-07-19T12:29:01.440264Z","shell.execute_reply":"2023-07-19T12:29:03.983743Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"<__main__.Nina1Dataset object at 0x7f1155b37d30>\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\n# Define the CNN-BiLSTM model\nfrom tensorflow.keras.layers import LSTM, Bidirectional, Dropout\nfrom tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, ReLU\nfrom tensorflow.keras.activations import tanh\n\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.layers import TimeDistributed\n\ndef cnn(x):\n    #print(x.shape)\n    #x = Reshape((25, 20,10))(x)  \n    print(x.shape)\n    x = TimeDistributed((Conv1D(64, kernel_size=9, strides=2, padding='same', activation=tanh)))(x)\n    print(x.shape)\n    #(Batch, 10, 64)\n\n    x = TimeDistributed(MaxPooling1D(pool_size=8, strides=2))(x)\n    print(x.shape)\n    #(Batch, 2, 64)\n    \n    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n    #(Batch, 1, 64)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    \n    x = TimeDistributed((Conv1D(64, kernel_size=5, strides=2, padding='same', activation=tanh)))(x)\n    print(x.shape)\n    #(Batch, 1, 64)\n    #x = TimeDistributed(Dropout(0.2093))(x)\n    x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    \n    #x = TimeDistributed(ReLU())(Conv1D(64, kernel_size=3, strides=2, padding='same')(x))\n    x = TimeDistributed((Conv1D(64, kernel_size=3, strides=2, padding='same', activation=tanh)))(x)\n    #print(x.shape)\n    #(Batch, 1, 64)\n    #x = TimeDistributed(Dropout(0.2093))(x)\n    #x = TimeDistributed(BatchNormalization(epsilon=1e-6, momentum=0.95))(x)\n    x = TimeDistributed(Flatten())(x)\n    print(x.shape)\n    # (Batch, 64)\n\n    return x\n\ndef Bi_LSTMModel(input_shape,x):\n    #model = Sequential()\n    # Hidden dimensions\n    hidden_dim = 200\n    print(x.shape)\n    print(11)\n    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.2093), input_shape=input_shape)(x)\n    #x = Dropout(0.2093)(x)\n    print(x.shape)\n    print(11)\n    x = Bidirectional(LSTM(hidden_dim, return_sequences=True, dropout=0.2093))(x)\n\n    print(x.shape)\n    print(11)\n    #x = Dropout(0.2093)(x)\n    x = Flatten()(x)\n    # ( ,10000)\n\n    return x\ndef EMGHandNet(input_shape, num_classes):\n    # Define the input layer\n    x = Input(shape=input_shape)\n    inputs = x\n    #print(x.shape)\n    #(batch, 25, 20, 10)\n    #temp = [cnn(x[:, t, :, :]) for t in range(x.shape[1])]\n    #x = tf.stack(temp, axis=1)\n    #print(x.shape)\n    x = cnn(x)\n   \n    #print(x.shape)\n    x = Bi_LSTMModel(x.shape[1:],x)\n    #print(x.shape)\n    #x = Dropout(0.2093)(x)\n    \n    x = Dense(512, activation='tanh')(x)\n    #print(x.shape)\n    x = Dropout(0.2093)(x)\n\n    # Add the output layer\n    output_layer = Dense(52, activation='softmax')(x)\n    # Create the model\n    model = Model(inputs=inputs, outputs=output_layer)\n\n    return model\n\n\nnum_classes = 52  # Adjust based on the number of hand activity classes\n\n\n# Create an instance of the EMGHandNet model\nmodel = EMGHandNet((25,20,10), 52)\n\n\n# Compile the model\n\n#dtype = tf.float32\n\ninitial_learning_rate = 0.001\ndecay_steps = 1000\ndecay_rate = 0.9\nbatch_size = 16\n# Define your model and its optimizer\n# Define learning rate schedule\nlr_schedule = ExponentialDecay(initial_learning_rate, decay_steps, decay_rate)\n\n    # Compile the model with learning rate schedule\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999),\n                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n                metrics=['accuracy'])\nhistory = model.fit(train_dataset,\n                    epochs=200,\n                    validation_data=val_dataset,\n                    batch_size=16\n                    )","metadata":{"papermill":{"duration":1994.814887,"end_time":"2023-05-21T13:11:39.782204","exception":false,"start_time":"2023-05-21T12:38:24.967317","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T13:36:25.085187Z","iopub.execute_input":"2023-07-19T13:36:25.085548Z","iopub.status.idle":"2023-07-19T14:19:15.000260Z","shell.execute_reply.started":"2023-07-19T13:36:25.085516Z","shell.execute_reply":"2023-07-19T14:19:14.999250Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(None, 25, 20, 10)\n(None, 25, 10, 64)\n(None, 25, 2, 64)\n(None, 25, 1, 64)\n(None, 25, 64)\n(None, 25, 64)\n11\n(None, 25, 400)\n11\n(None, 25, 400)\n11\nEpoch 1/200\n430/430 [==============================] - 24s 32ms/step - loss: 3.0975 - accuracy: 0.2158 - val_loss: 2.5213 - val_accuracy: 0.3176\nEpoch 2/200\n430/430 [==============================] - 12s 28ms/step - loss: 2.2246 - accuracy: 0.3874 - val_loss: 2.0724 - val_accuracy: 0.4118\nEpoch 3/200\n430/430 [==============================] - 12s 27ms/step - loss: 1.7352 - accuracy: 0.4945 - val_loss: 1.6652 - val_accuracy: 0.5155\nEpoch 4/200\n430/430 [==============================] - 12s 29ms/step - loss: 1.3126 - accuracy: 0.6090 - val_loss: 1.5111 - val_accuracy: 0.5554\nEpoch 5/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.9040 - accuracy: 0.7219 - val_loss: 1.2231 - val_accuracy: 0.6304\nEpoch 6/200\n430/430 [==============================] - 13s 29ms/step - loss: 0.6104 - accuracy: 0.8064 - val_loss: 1.1462 - val_accuracy: 0.6608\nEpoch 7/200\n430/430 [==============================] - 12s 27ms/step - loss: 0.3990 - accuracy: 0.8741 - val_loss: 1.0360 - val_accuracy: 0.7071\nEpoch 8/200\n430/430 [==============================] - 11s 26ms/step - loss: 0.2296 - accuracy: 0.9323 - val_loss: 0.9707 - val_accuracy: 0.7324\nEpoch 9/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.1641 - accuracy: 0.9528 - val_loss: 1.0463 - val_accuracy: 0.7142\nEpoch 10/200\n430/430 [==============================] - 13s 29ms/step - loss: 0.1195 - accuracy: 0.9686 - val_loss: 1.0209 - val_accuracy: 0.7253\nEpoch 11/200\n430/430 [==============================] - 12s 29ms/step - loss: 0.0730 - accuracy: 0.9847 - val_loss: 0.9574 - val_accuracy: 0.7439\nEpoch 12/200\n430/430 [==============================] - 12s 27ms/step - loss: 0.0760 - accuracy: 0.9811 - val_loss: 0.9829 - val_accuracy: 0.7490\nEpoch 13/200\n430/430 [==============================] - 13s 29ms/step - loss: 0.0850 - accuracy: 0.9744 - val_loss: 1.0252 - val_accuracy: 0.7270\nEpoch 14/200\n430/430 [==============================] - 11s 26ms/step - loss: 0.0741 - accuracy: 0.9799 - val_loss: 1.0497 - val_accuracy: 0.7314\nEpoch 15/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0590 - accuracy: 0.9850 - val_loss: 1.0084 - val_accuracy: 0.7584\nEpoch 16/200\n430/430 [==============================] - 12s 27ms/step - loss: 0.0444 - accuracy: 0.9881 - val_loss: 1.0162 - val_accuracy: 0.7574\nEpoch 17/200\n430/430 [==============================] - 11s 26ms/step - loss: 0.0450 - accuracy: 0.9888 - val_loss: 1.1475 - val_accuracy: 0.7264\nEpoch 18/200\n430/430 [==============================] - 13s 29ms/step - loss: 0.0276 - accuracy: 0.9937 - val_loss: 1.0776 - val_accuracy: 0.7382\nEpoch 19/200\n430/430 [==============================] - 13s 30ms/step - loss: 0.0194 - accuracy: 0.9968 - val_loss: 0.9395 - val_accuracy: 0.7703\nEpoch 20/200\n430/430 [==============================] - 13s 29ms/step - loss: 0.0286 - accuracy: 0.9927 - val_loss: 1.1894 - val_accuracy: 0.7226\nEpoch 21/200\n430/430 [==============================] - 13s 30ms/step - loss: 0.0520 - accuracy: 0.9852 - val_loss: 1.0856 - val_accuracy: 0.7598\nEpoch 22/200\n430/430 [==============================] - 12s 29ms/step - loss: 0.0189 - accuracy: 0.9955 - val_loss: 1.0337 - val_accuracy: 0.7632\nEpoch 23/200\n430/430 [==============================] - 14s 33ms/step - loss: 0.0118 - accuracy: 0.9977 - val_loss: 1.0249 - val_accuracy: 0.7669\nEpoch 24/200\n430/430 [==============================] - 11s 26ms/step - loss: 0.0065 - accuracy: 0.9987 - val_loss: 0.9653 - val_accuracy: 0.7818\nEpoch 25/200\n430/430 [==============================] - 11s 27ms/step - loss: 0.0157 - accuracy: 0.9969 - val_loss: 1.0213 - val_accuracy: 0.7693\nEpoch 26/200\n430/430 [==============================] - 11s 26ms/step - loss: 0.0115 - accuracy: 0.9969 - val_loss: 1.1029 - val_accuracy: 0.7544\nEpoch 27/200\n430/430 [==============================] - 11s 26ms/step - loss: 0.0282 - accuracy: 0.9916 - val_loss: 1.0943 - val_accuracy: 0.7510\nEpoch 28/200\n430/430 [==============================] - 12s 29ms/step - loss: 0.0179 - accuracy: 0.9952 - val_loss: 1.0366 - val_accuracy: 0.7666\nEpoch 29/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0037 - accuracy: 0.9999 - val_loss: 0.9678 - val_accuracy: 0.7905\nEpoch 30/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.9726 - val_accuracy: 0.7899\nEpoch 31/200\n430/430 [==============================] - 11s 27ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.9566 - val_accuracy: 0.7936\nEpoch 32/200\n430/430 [==============================] - 12s 28ms/step - loss: 9.9245e-04 - accuracy: 1.0000 - val_loss: 0.9542 - val_accuracy: 0.7953\nEpoch 33/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0022 - accuracy: 0.9996 - val_loss: 1.0130 - val_accuracy: 0.7797\nEpoch 34/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0348 - accuracy: 0.9908 - val_loss: 1.0859 - val_accuracy: 0.7611\nEpoch 35/200\n430/430 [==============================] - 12s 28ms/step - loss: 0.0038 - accuracy: 0.9996 - val_loss: 1.0094 - val_accuracy: 0.7777\nEpoch 36/200\n430/430 [==============================] - 11s 26ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.9874 - val_accuracy: 0.7851\nEpoch 37/200\n430/430 [==============================] - 11s 26ms/step - loss: 9.8361e-04 - accuracy: 1.0000 - val_loss: 0.9917 - val_accuracy: 0.7875\nEpoch 38/200\n430/430 [==============================] - 12s 27ms/step - loss: 7.1493e-04 - accuracy: 1.0000 - val_loss: 0.9749 - val_accuracy: 0.7919\nEpoch 39/200\n430/430 [==============================] - 11s 26ms/step - loss: 5.4753e-04 - accuracy: 1.0000 - val_loss: 0.9769 - val_accuracy: 0.7902\nEpoch 40/200\n430/430 [==============================] - 11s 26ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 1.0863 - val_accuracy: 0.7709\nEpoch 41/200\n430/430 [==============================] - 12s 27ms/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 1.0607 - val_accuracy: 0.7804\nEpoch 42/200\n430/430 [==============================] - 11s 26ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 1.0097 - val_accuracy: 0.7909\nEpoch 43/200\n430/430 [==============================] - 15s 34ms/step - loss: 6.9619e-04 - accuracy: 1.0000 - val_loss: 0.9997 - val_accuracy: 0.7956\nEpoch 44/200\n430/430 [==============================] - 12s 27ms/step - loss: 4.0970e-04 - accuracy: 1.0000 - val_loss: 0.9953 - val_accuracy: 0.7963\nEpoch 45/200\n430/430 [==============================] - 12s 28ms/step - loss: 4.0412e-04 - accuracy: 1.0000 - val_loss: 0.9943 - val_accuracy: 0.7980\nEpoch 46/200\n430/430 [==============================] - 12s 28ms/step - loss: 3.6431e-04 - accuracy: 1.0000 - val_loss: 0.9785 - val_accuracy: 0.8000\nEpoch 47/200\n430/430 [==============================] - 12s 27ms/step - loss: 2.4401e-04 - accuracy: 1.0000 - val_loss: 0.9775 - val_accuracy: 0.8034\nEpoch 48/200\n430/430 [==============================] - 12s 29ms/step - loss: 2.2775e-04 - accuracy: 1.0000 - val_loss: 0.9815 - val_accuracy: 0.8003\nEpoch 49/200\n430/430 [==============================] - 12s 28ms/step - loss: 2.0364e-04 - accuracy: 1.0000 - val_loss: 0.9787 - val_accuracy: 0.8034\nEpoch 50/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.6880e-04 - accuracy: 1.0000 - val_loss: 0.9764 - val_accuracy: 0.8037\nEpoch 51/200\n430/430 [==============================] - 13s 30ms/step - loss: 1.6045e-04 - accuracy: 1.0000 - val_loss: 0.9704 - val_accuracy: 0.8020\nEpoch 52/200\n430/430 [==============================] - 12s 27ms/step - loss: 1.4278e-04 - accuracy: 1.0000 - val_loss: 0.9884 - val_accuracy: 0.8017\nEpoch 53/200\n430/430 [==============================] - 12s 29ms/step - loss: 0.0059 - accuracy: 0.9990 - val_loss: 1.1072 - val_accuracy: 0.7804\nEpoch 54/200\n430/430 [==============================] - 12s 29ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 1.0320 - val_accuracy: 0.7916\nEpoch 55/200\n430/430 [==============================] - 12s 27ms/step - loss: 2.8168e-04 - accuracy: 1.0000 - val_loss: 1.0183 - val_accuracy: 0.7932\nEpoch 56/200\n430/430 [==============================] - 15s 34ms/step - loss: 2.2174e-04 - accuracy: 1.0000 - val_loss: 1.0067 - val_accuracy: 0.7943\nEpoch 57/200\n430/430 [==============================] - 12s 28ms/step - loss: 2.2769e-04 - accuracy: 1.0000 - val_loss: 1.0035 - val_accuracy: 0.7959\nEpoch 58/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.7633e-04 - accuracy: 1.0000 - val_loss: 1.0032 - val_accuracy: 0.7963\nEpoch 59/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.5135e-04 - accuracy: 1.0000 - val_loss: 1.0012 - val_accuracy: 0.7963\nEpoch 60/200\n430/430 [==============================] - 11s 26ms/step - loss: 1.5121e-04 - accuracy: 1.0000 - val_loss: 1.0102 - val_accuracy: 0.7980\nEpoch 61/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.1124e-04 - accuracy: 1.0000 - val_loss: 0.9994 - val_accuracy: 0.7966\nEpoch 62/200\n430/430 [==============================] - 11s 26ms/step - loss: 1.1242e-04 - accuracy: 1.0000 - val_loss: 0.9950 - val_accuracy: 0.8000\nEpoch 63/200\n430/430 [==============================] - 12s 29ms/step - loss: 1.8017e-04 - accuracy: 1.0000 - val_loss: 1.0105 - val_accuracy: 0.7943\nEpoch 64/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.3295e-04 - accuracy: 1.0000 - val_loss: 1.0038 - val_accuracy: 0.7976\nEpoch 65/200\n430/430 [==============================] - 12s 28ms/step - loss: 8.4611e-05 - accuracy: 1.0000 - val_loss: 1.0003 - val_accuracy: 0.7976\nEpoch 66/200\n430/430 [==============================] - 13s 29ms/step - loss: 5.9076e-04 - accuracy: 0.9999 - val_loss: 1.0153 - val_accuracy: 0.7939\nEpoch 67/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.5696e-04 - accuracy: 1.0000 - val_loss: 1.0033 - val_accuracy: 0.8007\nEpoch 68/200\n430/430 [==============================] - 12s 28ms/step - loss: 7.1392e-05 - accuracy: 1.0000 - val_loss: 0.9972 - val_accuracy: 0.8000\nEpoch 69/200\n430/430 [==============================] - 12s 27ms/step - loss: 6.3976e-05 - accuracy: 1.0000 - val_loss: 0.9967 - val_accuracy: 0.8074\nEpoch 70/200\n430/430 [==============================] - 12s 29ms/step - loss: 5.4965e-05 - accuracy: 1.0000 - val_loss: 0.9974 - val_accuracy: 0.8014\nEpoch 71/200\n430/430 [==============================] - 12s 29ms/step - loss: 5.2853e-05 - accuracy: 1.0000 - val_loss: 1.0077 - val_accuracy: 0.8007\nEpoch 72/200\n430/430 [==============================] - 11s 26ms/step - loss: 4.6449e-05 - accuracy: 1.0000 - val_loss: 1.0023 - val_accuracy: 0.8030\nEpoch 73/200\n430/430 [==============================] - 12s 28ms/step - loss: 4.6789e-05 - accuracy: 1.0000 - val_loss: 0.9997 - val_accuracy: 0.8051\nEpoch 74/200\n430/430 [==============================] - 12s 27ms/step - loss: 4.2971e-05 - accuracy: 1.0000 - val_loss: 1.0050 - val_accuracy: 0.8000\nEpoch 75/200\n430/430 [==============================] - 11s 27ms/step - loss: 3.8698e-05 - accuracy: 1.0000 - val_loss: 1.0052 - val_accuracy: 0.8027\nEpoch 76/200\n430/430 [==============================] - 13s 30ms/step - loss: 3.5498e-05 - accuracy: 1.0000 - val_loss: 1.0124 - val_accuracy: 0.8024\nEpoch 77/200\n430/430 [==============================] - 12s 27ms/step - loss: 3.4069e-05 - accuracy: 1.0000 - val_loss: 1.0088 - val_accuracy: 0.8057\nEpoch 78/200\n430/430 [==============================] - 12s 28ms/step - loss: 3.3535e-05 - accuracy: 1.0000 - val_loss: 1.0148 - val_accuracy: 0.8061\nEpoch 79/200\n430/430 [==============================] - 13s 29ms/step - loss: 2.9262e-05 - accuracy: 1.0000 - val_loss: 1.0150 - val_accuracy: 0.8007\nEpoch 80/200\n430/430 [==============================] - 11s 27ms/step - loss: 3.0850e-05 - accuracy: 1.0000 - val_loss: 1.0222 - val_accuracy: 0.8020\nEpoch 81/200\n430/430 [==============================] - 12s 28ms/step - loss: 2.5640e-05 - accuracy: 1.0000 - val_loss: 1.0233 - val_accuracy: 0.8030\nEpoch 82/200\n430/430 [==============================] - 12s 27ms/step - loss: 2.4309e-05 - accuracy: 1.0000 - val_loss: 1.0279 - val_accuracy: 0.8000\nEpoch 83/200\n430/430 [==============================] - 12s 28ms/step - loss: 2.3494e-05 - accuracy: 1.0000 - val_loss: 1.0199 - val_accuracy: 0.8034\nEpoch 84/200\n430/430 [==============================] - 11s 26ms/step - loss: 1.9542e-05 - accuracy: 1.0000 - val_loss: 1.0298 - val_accuracy: 0.8057\nEpoch 85/200\n430/430 [==============================] - 12s 28ms/step - loss: 2.0142e-05 - accuracy: 1.0000 - val_loss: 1.0319 - val_accuracy: 0.8051\nEpoch 86/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.9607e-05 - accuracy: 1.0000 - val_loss: 1.0311 - val_accuracy: 0.8068\nEpoch 87/200\n430/430 [==============================] - 11s 26ms/step - loss: 1.8263e-05 - accuracy: 1.0000 - val_loss: 1.0400 - val_accuracy: 0.8054\nEpoch 88/200\n430/430 [==============================] - 11s 26ms/step - loss: 1.6730e-05 - accuracy: 1.0000 - val_loss: 1.0369 - val_accuracy: 0.8057\nEpoch 89/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.8145e-05 - accuracy: 1.0000 - val_loss: 1.0386 - val_accuracy: 0.8068\nEpoch 90/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.4374e-05 - accuracy: 1.0000 - val_loss: 1.0412 - val_accuracy: 0.8068\nEpoch 91/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.4541e-05 - accuracy: 1.0000 - val_loss: 1.0455 - val_accuracy: 0.8111\nEpoch 92/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.6260e-05 - accuracy: 1.0000 - val_loss: 1.0417 - val_accuracy: 0.8074\nEpoch 93/200\n430/430 [==============================] - 12s 27ms/step - loss: 1.5472e-05 - accuracy: 1.0000 - val_loss: 1.0497 - val_accuracy: 0.8057\nEpoch 94/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.2981e-05 - accuracy: 1.0000 - val_loss: 1.0393 - val_accuracy: 0.8105\nEpoch 95/200\n430/430 [==============================] - 11s 26ms/step - loss: 1.2754e-05 - accuracy: 1.0000 - val_loss: 1.0498 - val_accuracy: 0.8084\nEpoch 96/200\n430/430 [==============================] - 11s 26ms/step - loss: 1.0397e-05 - accuracy: 1.0000 - val_loss: 1.0401 - val_accuracy: 0.8095\nEpoch 97/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.1150e-05 - accuracy: 1.0000 - val_loss: 1.0463 - val_accuracy: 0.8091\nEpoch 98/200\n430/430 [==============================] - 12s 27ms/step - loss: 1.2261e-05 - accuracy: 1.0000 - val_loss: 1.0447 - val_accuracy: 0.8054\nEpoch 99/200\n430/430 [==============================] - 12s 28ms/step - loss: 9.8875e-06 - accuracy: 1.0000 - val_loss: 1.0499 - val_accuracy: 0.8061\nEpoch 100/200\n430/430 [==============================] - 12s 28ms/step - loss: 9.3521e-06 - accuracy: 1.0000 - val_loss: 1.0439 - val_accuracy: 0.8068\nEpoch 101/200\n430/430 [==============================] - 11s 26ms/step - loss: 9.9910e-06 - accuracy: 1.0000 - val_loss: 1.0512 - val_accuracy: 0.8081\nEpoch 102/200\n430/430 [==============================] - 12s 28ms/step - loss: 9.9910e-06 - accuracy: 1.0000 - val_loss: 1.0554 - val_accuracy: 0.8078\nEpoch 103/200\n430/430 [==============================] - 12s 28ms/step - loss: 1.0458e-05 - accuracy: 1.0000 - val_loss: 1.0496 - val_accuracy: 0.8084\nEpoch 104/200\n430/430 [==============================] - 12s 28ms/step - loss: 9.9040e-06 - accuracy: 1.0000 - val_loss: 1.0541 - val_accuracy: 0.8044\nEpoch 105/200\n430/430 [==============================] - 11s 27ms/step - loss: 9.1350e-06 - accuracy: 1.0000 - val_loss: 1.0584 - val_accuracy: 0.8071\nEpoch 106/200\n430/430 [==============================] - 12s 28ms/step - loss: 9.4589e-06 - accuracy: 1.0000 - val_loss: 1.0561 - val_accuracy: 0.8081\nEpoch 107/200\n430/430 [==============================] - 12s 28ms/step - loss: 8.1126e-06 - accuracy: 1.0000 - val_loss: 1.0548 - val_accuracy: 0.8074\nEpoch 108/200\n430/430 [==============================] - 12s 28ms/step - loss: 7.9910e-06 - accuracy: 1.0000 - val_loss: 1.0591 - val_accuracy: 0.8064\nEpoch 109/200\n430/430 [==============================] - 12s 28ms/step - loss: 8.0783e-06 - accuracy: 1.0000 - val_loss: 1.0578 - val_accuracy: 0.8041\nEpoch 110/200\n430/430 [==============================] - 11s 27ms/step - loss: 8.6177e-06 - accuracy: 1.0000 - val_loss: 1.0609 - val_accuracy: 0.8061\nEpoch 111/200\n430/430 [==============================] - 13s 29ms/step - loss: 8.4831e-06 - accuracy: 1.0000 - val_loss: 1.0662 - val_accuracy: 0.8061\nEpoch 112/200\n430/430 [==============================] - 12s 28ms/step - loss: 9.7916e-06 - accuracy: 1.0000 - val_loss: 1.0587 - val_accuracy: 0.8064\nEpoch 113/200\n430/430 [==============================] - 11s 27ms/step - loss: 7.8063e-06 - accuracy: 1.0000 - val_loss: 1.0631 - val_accuracy: 0.8068\nEpoch 114/200\n430/430 [==============================] - 13s 29ms/step - loss: 7.4673e-06 - accuracy: 1.0000 - val_loss: 1.0627 - val_accuracy: 0.8037\nEpoch 115/200\n430/430 [==============================] - 11s 26ms/step - loss: 6.8584e-06 - accuracy: 1.0000 - val_loss: 1.0615 - val_accuracy: 0.8091\nEpoch 116/200\n430/430 [==============================] - 12s 28ms/step - loss: 7.5355e-06 - accuracy: 1.0000 - val_loss: 1.0622 - val_accuracy: 0.8105\nEpoch 117/200\n430/430 [==============================] - 12s 27ms/step - loss: 6.8136e-06 - accuracy: 1.0000 - val_loss: 1.0581 - val_accuracy: 0.8057\nEpoch 118/200\n430/430 [==============================] - 11s 27ms/step - loss: 6.8130e-06 - accuracy: 1.0000 - val_loss: 1.0597 - val_accuracy: 0.8054\nEpoch 119/200\n430/430 [==============================] - 11s 27ms/step - loss: 7.2754e-06 - accuracy: 1.0000 - val_loss: 1.0594 - val_accuracy: 0.8057\nEpoch 120/200\n430/430 [==============================] - 11s 26ms/step - loss: 6.6641e-06 - accuracy: 1.0000 - val_loss: 1.0594 - val_accuracy: 0.8068\nEpoch 121/200\n430/430 [==============================] - 12s 28ms/step - loss: 6.3375e-06 - accuracy: 1.0000 - val_loss: 1.0585 - val_accuracy: 0.8095\nEpoch 122/200\n430/430 [==============================] - 12s 28ms/step - loss: 6.0780e-06 - accuracy: 1.0000 - val_loss: 1.0547 - val_accuracy: 0.8061\nEpoch 123/200\n430/430 [==============================] - 13s 29ms/step - loss: 6.6772e-06 - accuracy: 1.0000 - val_loss: 1.0562 - val_accuracy: 0.8054\nEpoch 124/200\n430/430 [==============================] - 11s 27ms/step - loss: 6.9177e-06 - accuracy: 1.0000 - val_loss: 1.0582 - val_accuracy: 0.8084\nEpoch 125/200\n430/430 [==============================] - 11s 26ms/step - loss: 6.0690e-06 - accuracy: 1.0000 - val_loss: 1.0534 - val_accuracy: 0.8088\nEpoch 126/200\n430/430 [==============================] - 13s 29ms/step - loss: 9.1496e-06 - accuracy: 1.0000 - val_loss: 1.0576 - val_accuracy: 0.8061\nEpoch 127/200\n430/430 [==============================] - 12s 28ms/step - loss: 7.2527e-06 - accuracy: 1.0000 - val_loss: 1.0590 - val_accuracy: 0.8098\nEpoch 128/200\n430/430 [==============================] - 12s 29ms/step - loss: 6.4790e-06 - accuracy: 1.0000 - val_loss: 1.0565 - val_accuracy: 0.8088\nEpoch 129/200\n430/430 [==============================] - 11s 26ms/step - loss: 6.5077e-06 - accuracy: 1.0000 - val_loss: 1.0603 - val_accuracy: 0.8061\nEpoch 130/200\n430/430 [==============================] - 11s 26ms/step - loss: 6.3082e-06 - accuracy: 1.0000 - val_loss: 1.0567 - val_accuracy: 0.8081\nEpoch 131/200\n430/430 [==============================] - 12s 28ms/step - loss: 6.2428e-06 - accuracy: 1.0000 - val_loss: 1.0608 - val_accuracy: 0.8084\nEpoch 132/200\n430/430 [==============================] - 11s 26ms/step - loss: 6.1066e-06 - accuracy: 1.0000 - val_loss: 1.0610 - val_accuracy: 0.8064\nEpoch 133/200\n430/430 [==============================] - 12s 28ms/step - loss: 6.5322e-06 - accuracy: 1.0000 - val_loss: 1.0676 - val_accuracy: 0.8115\nEpoch 134/200\n430/430 [==============================] - 12s 28ms/step - loss: 6.4422e-06 - accuracy: 1.0000 - val_loss: 1.0625 - val_accuracy: 0.8071\nEpoch 135/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.8473e-06 - accuracy: 1.0000 - val_loss: 1.0636 - val_accuracy: 0.8091\nEpoch 136/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.5747e-06 - accuracy: 1.0000 - val_loss: 1.0603 - val_accuracy: 0.8115\nEpoch 137/200\n430/430 [==============================] - 12s 28ms/step - loss: 6.0752e-06 - accuracy: 1.0000 - val_loss: 1.0599 - val_accuracy: 0.8061\nEpoch 138/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.3755e-06 - accuracy: 1.0000 - val_loss: 1.0618 - val_accuracy: 0.8071\nEpoch 139/200\n430/430 [==============================] - 11s 27ms/step - loss: 6.0737e-06 - accuracy: 1.0000 - val_loss: 1.0640 - val_accuracy: 0.8095\nEpoch 140/200\n430/430 [==============================] - 12s 27ms/step - loss: 6.2395e-06 - accuracy: 1.0000 - val_loss: 1.0641 - val_accuracy: 0.8074\nEpoch 141/200\n430/430 [==============================] - 13s 29ms/step - loss: 5.8188e-06 - accuracy: 1.0000 - val_loss: 1.0645 - val_accuracy: 0.8084\nEpoch 142/200\n430/430 [==============================] - 12s 27ms/step - loss: 6.1310e-06 - accuracy: 1.0000 - val_loss: 1.0581 - val_accuracy: 0.8074\nEpoch 143/200\n430/430 [==============================] - 11s 26ms/step - loss: 6.2427e-06 - accuracy: 1.0000 - val_loss: 1.0612 - val_accuracy: 0.8101\nEpoch 144/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.6207e-06 - accuracy: 1.0000 - val_loss: 1.0693 - val_accuracy: 0.8095\nEpoch 145/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.8135e-06 - accuracy: 1.0000 - val_loss: 1.0594 - val_accuracy: 0.8078\nEpoch 146/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.5747e-06 - accuracy: 1.0000 - val_loss: 1.0615 - val_accuracy: 0.8095\nEpoch 147/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.4406e-06 - accuracy: 1.0000 - val_loss: 1.0590 - val_accuracy: 0.8088\nEpoch 148/200\n430/430 [==============================] - 13s 29ms/step - loss: 6.1018e-06 - accuracy: 1.0000 - val_loss: 1.0630 - val_accuracy: 0.8098\nEpoch 149/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.6942e-06 - accuracy: 1.0000 - val_loss: 1.0674 - val_accuracy: 0.8084\nEpoch 150/200\n430/430 [==============================] - 12s 27ms/step - loss: 5.7617e-06 - accuracy: 1.0000 - val_loss: 1.0640 - val_accuracy: 0.8081\nEpoch 151/200\n430/430 [==============================] - 13s 29ms/step - loss: 5.6737e-06 - accuracy: 1.0000 - val_loss: 1.0634 - val_accuracy: 0.8101\nEpoch 152/200\n430/430 [==============================] - 12s 27ms/step - loss: 5.3562e-06 - accuracy: 1.0000 - val_loss: 1.0678 - val_accuracy: 0.8078\nEpoch 153/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.9666e-06 - accuracy: 1.0000 - val_loss: 1.0681 - val_accuracy: 0.8091\nEpoch 154/200\n430/430 [==============================] - 11s 27ms/step - loss: 6.3006e-06 - accuracy: 1.0000 - val_loss: 1.0695 - val_accuracy: 0.8081\nEpoch 155/200\n430/430 [==============================] - 11s 26ms/step - loss: 5.8628e-06 - accuracy: 1.0000 - val_loss: 1.0596 - val_accuracy: 0.8101\nEpoch 156/200\n430/430 [==============================] - 13s 30ms/step - loss: 6.3699e-06 - accuracy: 1.0000 - val_loss: 1.0622 - val_accuracy: 0.8078\nEpoch 157/200\n430/430 [==============================] - 11s 27ms/step - loss: 5.7783e-06 - accuracy: 1.0000 - val_loss: 1.0694 - val_accuracy: 0.8095\nEpoch 158/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.7417e-06 - accuracy: 1.0000 - val_loss: 1.0674 - val_accuracy: 0.8091\nEpoch 159/200\n430/430 [==============================] - 13s 29ms/step - loss: 5.3467e-06 - accuracy: 1.0000 - val_loss: 1.0709 - val_accuracy: 0.8111\nEpoch 160/200\n430/430 [==============================] - 11s 26ms/step - loss: 5.5814e-06 - accuracy: 1.0000 - val_loss: 1.0719 - val_accuracy: 0.8101\nEpoch 161/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.6533e-06 - accuracy: 1.0000 - val_loss: 1.0642 - val_accuracy: 0.8105\nEpoch 162/200\n430/430 [==============================] - 11s 27ms/step - loss: 4.8328e-06 - accuracy: 1.0000 - val_loss: 1.0674 - val_accuracy: 0.8081\nEpoch 163/200\n430/430 [==============================] - 11s 26ms/step - loss: 5.2318e-06 - accuracy: 1.0000 - val_loss: 1.0647 - val_accuracy: 0.8078\nEpoch 164/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.2291e-06 - accuracy: 1.0000 - val_loss: 1.0695 - val_accuracy: 0.8108\nEpoch 165/200\n430/430 [==============================] - 11s 27ms/step - loss: 4.8897e-06 - accuracy: 1.0000 - val_loss: 1.0676 - val_accuracy: 0.8115\nEpoch 166/200\n430/430 [==============================] - 12s 27ms/step - loss: 5.6821e-06 - accuracy: 1.0000 - val_loss: 1.0655 - val_accuracy: 0.8091\nEpoch 167/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.2160e-06 - accuracy: 1.0000 - val_loss: 1.0662 - val_accuracy: 0.8125\nEpoch 168/200\n430/430 [==============================] - 11s 26ms/step - loss: 5.4065e-06 - accuracy: 1.0000 - val_loss: 1.0675 - val_accuracy: 0.8101\nEpoch 169/200\n430/430 [==============================] - 11s 26ms/step - loss: 5.3375e-06 - accuracy: 1.0000 - val_loss: 1.0603 - val_accuracy: 0.8084\nEpoch 170/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.6419e-06 - accuracy: 1.0000 - val_loss: 1.0668 - val_accuracy: 0.8108\nEpoch 171/200\n430/430 [==============================] - 11s 26ms/step - loss: 5.5751e-06 - accuracy: 1.0000 - val_loss: 1.0631 - val_accuracy: 0.8081\nEpoch 172/200\n430/430 [==============================] - 12s 29ms/step - loss: 5.3147e-06 - accuracy: 1.0000 - val_loss: 1.0673 - val_accuracy: 0.8068\nEpoch 173/200\n430/430 [==============================] - 11s 27ms/step - loss: 5.2397e-06 - accuracy: 1.0000 - val_loss: 1.0610 - val_accuracy: 0.8098\nEpoch 174/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.4066e-06 - accuracy: 1.0000 - val_loss: 1.0663 - val_accuracy: 0.8098\nEpoch 175/200\n430/430 [==============================] - 13s 29ms/step - loss: 5.5447e-06 - accuracy: 1.0000 - val_loss: 1.0645 - val_accuracy: 0.8101\nEpoch 176/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.2465e-06 - accuracy: 1.0000 - val_loss: 1.0677 - val_accuracy: 0.8068\nEpoch 177/200\n430/430 [==============================] - 11s 27ms/step - loss: 5.1767e-06 - accuracy: 1.0000 - val_loss: 1.0656 - val_accuracy: 0.8088\nEpoch 178/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.2210e-06 - accuracy: 1.0000 - val_loss: 1.0712 - val_accuracy: 0.8091\nEpoch 179/200\n430/430 [==============================] - 11s 26ms/step - loss: 4.8684e-06 - accuracy: 1.0000 - val_loss: 1.0623 - val_accuracy: 0.8108\nEpoch 180/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.4576e-06 - accuracy: 1.0000 - val_loss: 1.0592 - val_accuracy: 0.8095\nEpoch 181/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.0397e-06 - accuracy: 1.0000 - val_loss: 1.0655 - val_accuracy: 0.8088\nEpoch 182/200\n430/430 [==============================] - 11s 27ms/step - loss: 5.4299e-06 - accuracy: 1.0000 - val_loss: 1.0728 - val_accuracy: 0.8074\nEpoch 183/200\n430/430 [==============================] - 13s 29ms/step - loss: 5.0542e-06 - accuracy: 1.0000 - val_loss: 1.0696 - val_accuracy: 0.8091\nEpoch 184/200\n430/430 [==============================] - 11s 26ms/step - loss: 4.8769e-06 - accuracy: 1.0000 - val_loss: 1.0724 - val_accuracy: 0.8084\nEpoch 185/200\n430/430 [==============================] - 11s 26ms/step - loss: 5.3137e-06 - accuracy: 1.0000 - val_loss: 1.0649 - val_accuracy: 0.8074\nEpoch 186/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.1520e-06 - accuracy: 1.0000 - val_loss: 1.0667 - val_accuracy: 0.8081\nEpoch 187/200\n430/430 [==============================] - 12s 27ms/step - loss: 4.8510e-06 - accuracy: 1.0000 - val_loss: 1.0670 - val_accuracy: 0.8105\nEpoch 188/200\n430/430 [==============================] - 12s 27ms/step - loss: 5.4754e-06 - accuracy: 1.0000 - val_loss: 1.0639 - val_accuracy: 0.8081\nEpoch 189/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.6408e-06 - accuracy: 1.0000 - val_loss: 1.0640 - val_accuracy: 0.8078\nEpoch 190/200\n430/430 [==============================] - 15s 34ms/step - loss: 5.2436e-06 - accuracy: 1.0000 - val_loss: 1.0672 - val_accuracy: 0.8091\nEpoch 191/200\n430/430 [==============================] - 12s 29ms/step - loss: 5.0626e-06 - accuracy: 1.0000 - val_loss: 1.0713 - val_accuracy: 0.8101\nEpoch 192/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.2906e-06 - accuracy: 1.0000 - val_loss: 1.0677 - val_accuracy: 0.8108\nEpoch 193/200\n430/430 [==============================] - 12s 28ms/step - loss: 4.6323e-06 - accuracy: 1.0000 - val_loss: 1.0683 - val_accuracy: 0.8098\nEpoch 194/200\n430/430 [==============================] - 12s 27ms/step - loss: 5.4246e-06 - accuracy: 1.0000 - val_loss: 1.0649 - val_accuracy: 0.8098\nEpoch 195/200\n430/430 [==============================] - 12s 28ms/step - loss: 4.7585e-06 - accuracy: 1.0000 - val_loss: 1.0595 - val_accuracy: 0.8095\nEpoch 196/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.5949e-06 - accuracy: 1.0000 - val_loss: 1.0711 - val_accuracy: 0.8088\nEpoch 197/200\n430/430 [==============================] - 12s 27ms/step - loss: 5.1787e-06 - accuracy: 1.0000 - val_loss: 1.0643 - val_accuracy: 0.8088\nEpoch 198/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.2021e-06 - accuracy: 1.0000 - val_loss: 1.0636 - val_accuracy: 0.8054\nEpoch 199/200\n430/430 [==============================] - 11s 27ms/step - loss: 5.0114e-06 - accuracy: 1.0000 - val_loss: 1.0651 - val_accuracy: 0.8091\nEpoch 200/200\n430/430 [==============================] - 12s 28ms/step - loss: 5.1732e-06 - accuracy: 1.0000 - val_loss: 1.0708 - val_accuracy: 0.8051\n","output_type":"stream"}]},{"cell_type":"code","source":"true_labels = []  # True labels for the test data\npredicted_labels = []  # Predicted labels for the test data\n\nfor batch_data, batch_labels in test_dataset:\n    batch_predictions = model.predict(batch_data)\n    batch_predicted_labels = np.argmax(batch_predictions, axis=1)\n    predicted_labels.extend(batch_predicted_labels)\n    true_labels.extend(np.argmax(batch_labels, axis=1))\n\ntrue_labels = np.array(true_labels)\npredicted_labels = np.array(predicted_labels)\n\naccuracy = np.mean(true_labels == predicted_labels)\n\nprint(accuracy)","metadata":{"papermill":{"duration":23.887179,"end_time":"2023-05-21T13:12:05.959505","exception":false,"start_time":"2023-05-21T13:11:42.072326","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-07-19T14:19:15.002536Z","iopub.execute_input":"2023-07-19T14:19:15.002882Z","iopub.status.idle":"2023-07-19T14:19:35.870846Z","shell.execute_reply.started":"2023-07-19T14:19:15.002852Z","shell.execute_reply":"2023-07-19T14:19:35.869842Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 2s 2s/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 37ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 32ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 46ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 30ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 61ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 58ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 29ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 23ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 26ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 28ms/step\n1/1 [==============================] - 0s 27ms/step\n1/1 [==============================] - 0s 25ms/step\n1/1 [==============================] - 0s 24ms/step\n0.8934659090909091\n","output_type":"stream"}]}]}